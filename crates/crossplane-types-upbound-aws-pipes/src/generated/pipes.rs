// WARNING: generated by kopium - manual changes will be overwritten
// kopium version: 0.21.2

#[allow(unused_imports)]
mod prelude {
    pub use kube::CustomResource;
    pub use schemars::JsonSchema;
    pub use serde::{Serialize, Deserialize};
    pub use std::collections::HashMap;
    pub use k8s_openapi::apimachinery::pkg::apis::meta::v1::Condition;
}
use self::prelude::*;

/// PipeSpec defines the desired state of Pipe
#[derive(CustomResource, Serialize, Deserialize, Clone, Debug, JsonSchema)]
#[kube(group = "pipes.aws.upbound.io", version = "v1beta1", kind = "Pipe", plural = "pipes")]
#[kube(status = "PipeStatus")]
pub struct PipeSpec {
    /// DeletionPolicy specifies what will happen to the underlying external
    /// when this managed resource is deleted - either "Delete" or "Orphan" the
    /// external resource.
    /// This field is planned to be deprecated in favor of the ManagementPolicies
    /// field in a future release. Currently, both could be set independently and
    /// non-default values would be honored if the feature flag is enabled.
    /// See the design doc for more information: https://github.com/crossplane/crossplane/blob/499895a25d1a1a0ba1604944ef98ac7a1a71f197/design/design-doc-observe-only-resources.md?plain=1#L223
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "deletionPolicy")]
    pub deletion_policy: Option<PipeDeletionPolicy>,
    #[serde(rename = "forProvider")]
    pub for_provider: PipeForProvider,
    /// THIS IS A BETA FIELD. It will be honored
    /// unless the Management Policies feature flag is disabled.
    /// InitProvider holds the same fields as ForProvider, with the exception
    /// of Identifier and other resource reference fields. The fields that are
    /// in InitProvider are merged into ForProvider when the resource is created.
    /// The same fields are also added to the terraform ignore_changes hook, to
    /// avoid updating them after creation. This is useful for fields that are
    /// required on creation, but we do not desire to update them after creation,
    /// for example because of an external controller is managing them, like an
    /// autoscaler.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initProvider")]
    pub init_provider: Option<PipeInitProvider>,
    /// THIS IS A BETA FIELD. It is on by default but can be opted out
    /// through a Crossplane feature flag.
    /// ManagementPolicies specify the array of actions Crossplane is allowed to
    /// take on the managed and external resources.
    /// This field is planned to replace the DeletionPolicy field in a future
    /// release. Currently, both could be set independently and non-default
    /// values would be honored if the feature flag is enabled. If both are
    /// custom, the DeletionPolicy field will be ignored.
    /// See the design doc for more information: https://github.com/crossplane/crossplane/blob/499895a25d1a1a0ba1604944ef98ac7a1a71f197/design/design-doc-observe-only-resources.md?plain=1#L223
    /// and this one: https://github.com/crossplane/crossplane/blob/444267e84783136daa93568b364a5f01228cacbe/design/one-pager-ignore-changes.md
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "managementPolicies")]
    pub management_policies: Option<Vec<String>>,
    /// ProviderConfigReference specifies how the provider that will be used to
    /// create, observe, update, and delete this managed resource should be
    /// configured.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "providerConfigRef")]
    pub provider_config_ref: Option<PipeProviderConfigRef>,
    /// PublishConnectionDetailsTo specifies the connection secret config which
    /// contains a name, metadata and a reference to secret store config to
    /// which any connection details for this managed resource should be written.
    /// Connection details frequently include the endpoint, username,
    /// and password required to connect to the managed resource.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "publishConnectionDetailsTo")]
    pub publish_connection_details_to: Option<PipePublishConnectionDetailsTo>,
    /// WriteConnectionSecretToReference specifies the namespace and name of a
    /// Secret to which any connection details for this managed resource should
    /// be written. Connection details frequently include the endpoint, username,
    /// and password required to connect to the managed resource.
    /// This field is planned to be replaced in a future release in favor of
    /// PublishConnectionDetailsTo. Currently, both could be set independently
    /// and connection details would be published to both without affecting
    /// each other.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "writeConnectionSecretToRef")]
    pub write_connection_secret_to_ref: Option<PipeWriteConnectionSecretToRef>,
}

/// PipeSpec defines the desired state of Pipe
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum PipeDeletionPolicy {
    Orphan,
    Delete,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProvider {
    /// A description of the pipe. At most 512 characters.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub description: Option<String>,
    /// The state the pipe should be in. One of: RUNNING, STOPPED.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "desiredState")]
    pub desired_state: Option<String>,
    /// Enrichment resource of the pipe (typically an ARN). Read more about enrichment in the User Guide.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub enrichment: Option<String>,
    /// Parameters to configure enrichment for your pipe. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enrichmentParameters")]
    pub enrichment_parameters: Option<PipeForProviderEnrichmentParameters>,
    /// Reference to a APIDestination in cloudwatchevents to populate enrichment.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enrichmentRef")]
    pub enrichment_ref: Option<PipeForProviderEnrichmentRef>,
    /// Selector for a APIDestination in cloudwatchevents to populate enrichment.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enrichmentSelector")]
    pub enrichment_selector: Option<PipeForProviderEnrichmentSelector>,
    /// Logging configuration settings for the pipe. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "logConfiguration")]
    pub log_configuration: Option<PipeForProviderLogConfiguration>,
    /// Region is the region you'd like your resource to be created in.
    pub region: String,
    /// ARN of the role that allows the pipe to send data to the target.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "roleArn")]
    pub role_arn: Option<String>,
    /// Reference to a Role in iam to populate roleArn.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "roleArnRef")]
    pub role_arn_ref: Option<PipeForProviderRoleArnRef>,
    /// Selector for a Role in iam to populate roleArn.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "roleArnSelector")]
    pub role_arn_selector: Option<PipeForProviderRoleArnSelector>,
    /// Source resource of the pipe. This field typically requires an ARN (Amazon Resource Name). However, when using a self-managed Kafka cluster, you should use a different format. Instead of an ARN, use 'smk://' followed by the bootstrap server's address.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub source: Option<String>,
    /// Parameters to configure a source for the pipe. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sourceParameters")]
    pub source_parameters: Option<PipeForProviderSourceParameters>,
    /// Reference to a Queue in sqs to populate source.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sourceRef")]
    pub source_ref: Option<PipeForProviderSourceRef>,
    /// Selector for a Queue in sqs to populate source.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sourceSelector")]
    pub source_selector: Option<PipeForProviderSourceSelector>,
    /// Key-value map of resource tags.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tags: Option<HashMap<String, String>>,
    /// Target resource of the pipe (typically an ARN).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub target: Option<String>,
    /// Parameters to configure a target for your pipe. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "targetParameters")]
    pub target_parameters: Option<PipeForProviderTargetParameters>,
    /// Reference to a Queue in sqs to populate target.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "targetRef")]
    pub target_ref: Option<PipeForProviderTargetRef>,
    /// Selector for a Queue in sqs to populate target.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "targetSelector")]
    pub target_selector: Option<PipeForProviderTargetSelector>,
}

/// Parameters to configure enrichment for your pipe. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderEnrichmentParameters {
    /// Contains the HTTP parameters to use when the target is a API Gateway REST endpoint or EventBridge ApiDestination. If you specify an API Gateway REST API or EventBridge ApiDestination as a target, you can use this parameter to specify headers, path parameters, and query string keys/values as part of your target invoking request. If you're using ApiDestinations, the corresponding Connection can also have these values configured. In case of any conflicting keys, values from the Connection take precedence. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpParameters")]
    pub http_parameters: Option<PipeForProviderEnrichmentParametersHttpParameters>,
    /// Valid JSON text passed to the target. In this case, nothing from the event itself is passed to the target. Maximum length of 8192 characters.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "inputTemplate")]
    pub input_template: Option<String>,
}

/// Contains the HTTP parameters to use when the target is a API Gateway REST endpoint or EventBridge ApiDestination. If you specify an API Gateway REST API or EventBridge ApiDestination as a target, you can use this parameter to specify headers, path parameters, and query string keys/values as part of your target invoking request. If you're using ApiDestinations, the corresponding Connection can also have these values configured. In case of any conflicting keys, values from the Connection take precedence. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderEnrichmentParametersHttpParameters {
    /// Key-value mapping of the headers that need to be sent as part of request invoking the API Gateway REST API or EventBridge ApiDestination.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "headerParameters")]
    pub header_parameters: Option<HashMap<String, String>>,
    /// The path parameter values to be used to populate API Gateway REST API or EventBridge ApiDestination path wildcards ("*").
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "pathParameterValues")]
    pub path_parameter_values: Option<Vec<String>>,
    /// Key-value mapping of the query strings that need to be sent as part of request invoking the API Gateway REST API or EventBridge ApiDestination.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryStringParameters")]
    pub query_string_parameters: Option<HashMap<String, String>>,
}

/// Reference to a APIDestination in cloudwatchevents to populate enrichment.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderEnrichmentRef {
    /// Name of the referenced object.
    pub name: String,
    /// Policies for referencing.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub policy: Option<PipeForProviderEnrichmentRefPolicy>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderEnrichmentRefPolicy {
    /// Resolution specifies whether resolution of this reference is required.
    /// The default is 'Required', which means the reconcile will fail if the
    /// reference cannot be resolved. 'Optional' means this reference will be
    /// a no-op if it cannot be resolved.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolution: Option<PipeForProviderEnrichmentRefPolicyResolution>,
    /// Resolve specifies when this reference should be resolved. The default
    /// is 'IfNotPresent', which will attempt to resolve the reference only when
    /// the corresponding field is not present. Use 'Always' to resolve the
    /// reference on every reconcile.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolve: Option<PipeForProviderEnrichmentRefPolicyResolve>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum PipeForProviderEnrichmentRefPolicyResolution {
    Required,
    Optional,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum PipeForProviderEnrichmentRefPolicyResolve {
    Always,
    IfNotPresent,
}

/// Selector for a APIDestination in cloudwatchevents to populate enrichment.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderEnrichmentSelector {
    /// MatchControllerRef ensures an object with the same controller reference
    /// as the selecting object is selected.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchControllerRef")]
    pub match_controller_ref: Option<bool>,
    /// MatchLabels ensures an object with matching labels is selected.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<HashMap<String, String>>,
    /// Policies for selection.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub policy: Option<PipeForProviderEnrichmentSelectorPolicy>,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderEnrichmentSelectorPolicy {
    /// Resolution specifies whether resolution of this reference is required.
    /// The default is 'Required', which means the reconcile will fail if the
    /// reference cannot be resolved. 'Optional' means this reference will be
    /// a no-op if it cannot be resolved.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolution: Option<PipeForProviderEnrichmentSelectorPolicyResolution>,
    /// Resolve specifies when this reference should be resolved. The default
    /// is 'IfNotPresent', which will attempt to resolve the reference only when
    /// the corresponding field is not present. Use 'Always' to resolve the
    /// reference on every reconcile.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolve: Option<PipeForProviderEnrichmentSelectorPolicyResolve>,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum PipeForProviderEnrichmentSelectorPolicyResolution {
    Required,
    Optional,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum PipeForProviderEnrichmentSelectorPolicyResolve {
    Always,
    IfNotPresent,
}

/// Logging configuration settings for the pipe. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderLogConfiguration {
    /// Amazon CloudWatch Logs logging configuration settings for the pipe. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "cloudwatchLogsLogDestination")]
    pub cloudwatch_logs_log_destination: Option<PipeForProviderLogConfigurationCloudwatchLogsLogDestination>,
    /// Amazon Kinesis Data Firehose logging configuration settings for the pipe. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "firehoseLogDestination")]
    pub firehose_log_destination: Option<PipeForProviderLogConfigurationFirehoseLogDestination>,
    /// String list that specifies whether the execution data (specifically, the payload, awsRequest, and awsResponse fields) is included in the log messages for this pipe. This applies to all log destinations for the pipe. Valid values ALL.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "includeExecutionData")]
    pub include_execution_data: Option<Vec<String>>,
    /// The level of logging detail to include. Valid values OFF, ERROR, INFO and TRACE.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub level: Option<String>,
    /// Amazon S3 logging configuration settings for the pipe. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "s3LogDestination")]
    pub s3_log_destination: Option<PipeForProviderLogConfigurationS3LogDestination>,
}

/// Amazon CloudWatch Logs logging configuration settings for the pipe. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderLogConfigurationCloudwatchLogsLogDestination {
    /// Amazon Web Services Resource Name (ARN) for the CloudWatch log group to which EventBridge sends the log records.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "logGroupArn")]
    pub log_group_arn: Option<String>,
    /// Reference to a Group in cloudwatchlogs to populate logGroupArn.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "logGroupArnRef")]
    pub log_group_arn_ref: Option<PipeForProviderLogConfigurationCloudwatchLogsLogDestinationLogGroupArnRef>,
    /// Selector for a Group in cloudwatchlogs to populate logGroupArn.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "logGroupArnSelector")]
    pub log_group_arn_selector: Option<PipeForProviderLogConfigurationCloudwatchLogsLogDestinationLogGroupArnSelector>,
}

/// Reference to a Group in cloudwatchlogs to populate logGroupArn.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderLogConfigurationCloudwatchLogsLogDestinationLogGroupArnRef {
    /// Name of the referenced object.
    pub name: String,
    /// Policies for referencing.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub policy: Option<PipeForProviderLogConfigurationCloudwatchLogsLogDestinationLogGroupArnRefPolicy>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderLogConfigurationCloudwatchLogsLogDestinationLogGroupArnRefPolicy {
    /// Resolution specifies whether resolution of this reference is required.
    /// The default is 'Required', which means the reconcile will fail if the
    /// reference cannot be resolved. 'Optional' means this reference will be
    /// a no-op if it cannot be resolved.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolution: Option<PipeForProviderLogConfigurationCloudwatchLogsLogDestinationLogGroupArnRefPolicyResolution>,
    /// Resolve specifies when this reference should be resolved. The default
    /// is 'IfNotPresent', which will attempt to resolve the reference only when
    /// the corresponding field is not present. Use 'Always' to resolve the
    /// reference on every reconcile.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolve: Option<PipeForProviderLogConfigurationCloudwatchLogsLogDestinationLogGroupArnRefPolicyResolve>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum PipeForProviderLogConfigurationCloudwatchLogsLogDestinationLogGroupArnRefPolicyResolution {
    Required,
    Optional,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum PipeForProviderLogConfigurationCloudwatchLogsLogDestinationLogGroupArnRefPolicyResolve {
    Always,
    IfNotPresent,
}

/// Selector for a Group in cloudwatchlogs to populate logGroupArn.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderLogConfigurationCloudwatchLogsLogDestinationLogGroupArnSelector {
    /// MatchControllerRef ensures an object with the same controller reference
    /// as the selecting object is selected.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchControllerRef")]
    pub match_controller_ref: Option<bool>,
    /// MatchLabels ensures an object with matching labels is selected.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<HashMap<String, String>>,
    /// Policies for selection.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub policy: Option<PipeForProviderLogConfigurationCloudwatchLogsLogDestinationLogGroupArnSelectorPolicy>,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderLogConfigurationCloudwatchLogsLogDestinationLogGroupArnSelectorPolicy {
    /// Resolution specifies whether resolution of this reference is required.
    /// The default is 'Required', which means the reconcile will fail if the
    /// reference cannot be resolved. 'Optional' means this reference will be
    /// a no-op if it cannot be resolved.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolution: Option<PipeForProviderLogConfigurationCloudwatchLogsLogDestinationLogGroupArnSelectorPolicyResolution>,
    /// Resolve specifies when this reference should be resolved. The default
    /// is 'IfNotPresent', which will attempt to resolve the reference only when
    /// the corresponding field is not present. Use 'Always' to resolve the
    /// reference on every reconcile.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolve: Option<PipeForProviderLogConfigurationCloudwatchLogsLogDestinationLogGroupArnSelectorPolicyResolve>,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum PipeForProviderLogConfigurationCloudwatchLogsLogDestinationLogGroupArnSelectorPolicyResolution {
    Required,
    Optional,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum PipeForProviderLogConfigurationCloudwatchLogsLogDestinationLogGroupArnSelectorPolicyResolve {
    Always,
    IfNotPresent,
}

/// Amazon Kinesis Data Firehose logging configuration settings for the pipe. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderLogConfigurationFirehoseLogDestination {
    /// Amazon Resource Name (ARN) of the Kinesis Data Firehose delivery stream to which EventBridge delivers the pipe log records.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "deliveryStreamArn")]
    pub delivery_stream_arn: Option<String>,
}

/// Amazon S3 logging configuration settings for the pipe. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderLogConfigurationS3LogDestination {
    /// Name of the Amazon S3 bucket to which EventBridge delivers the log records for the pipe.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketName")]
    pub bucket_name: Option<String>,
    /// Amazon Web Services account that owns the Amazon S3 bucket to which EventBridge delivers the log records for the pipe.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketOwner")]
    pub bucket_owner: Option<String>,
    /// EventBridge format for the log records. Valid values json, plain and w3c.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "outputFormat")]
    pub output_format: Option<String>,
    /// Prefix text with which to begin Amazon S3 log object names.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub prefix: Option<String>,
}

/// Reference to a Role in iam to populate roleArn.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderRoleArnRef {
    /// Name of the referenced object.
    pub name: String,
    /// Policies for referencing.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub policy: Option<PipeForProviderRoleArnRefPolicy>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderRoleArnRefPolicy {
    /// Resolution specifies whether resolution of this reference is required.
    /// The default is 'Required', which means the reconcile will fail if the
    /// reference cannot be resolved. 'Optional' means this reference will be
    /// a no-op if it cannot be resolved.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolution: Option<PipeForProviderRoleArnRefPolicyResolution>,
    /// Resolve specifies when this reference should be resolved. The default
    /// is 'IfNotPresent', which will attempt to resolve the reference only when
    /// the corresponding field is not present. Use 'Always' to resolve the
    /// reference on every reconcile.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolve: Option<PipeForProviderRoleArnRefPolicyResolve>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum PipeForProviderRoleArnRefPolicyResolution {
    Required,
    Optional,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum PipeForProviderRoleArnRefPolicyResolve {
    Always,
    IfNotPresent,
}

/// Selector for a Role in iam to populate roleArn.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderRoleArnSelector {
    /// MatchControllerRef ensures an object with the same controller reference
    /// as the selecting object is selected.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchControllerRef")]
    pub match_controller_ref: Option<bool>,
    /// MatchLabels ensures an object with matching labels is selected.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<HashMap<String, String>>,
    /// Policies for selection.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub policy: Option<PipeForProviderRoleArnSelectorPolicy>,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderRoleArnSelectorPolicy {
    /// Resolution specifies whether resolution of this reference is required.
    /// The default is 'Required', which means the reconcile will fail if the
    /// reference cannot be resolved. 'Optional' means this reference will be
    /// a no-op if it cannot be resolved.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolution: Option<PipeForProviderRoleArnSelectorPolicyResolution>,
    /// Resolve specifies when this reference should be resolved. The default
    /// is 'IfNotPresent', which will attempt to resolve the reference only when
    /// the corresponding field is not present. Use 'Always' to resolve the
    /// reference on every reconcile.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolve: Option<PipeForProviderRoleArnSelectorPolicyResolve>,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum PipeForProviderRoleArnSelectorPolicyResolution {
    Required,
    Optional,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum PipeForProviderRoleArnSelectorPolicyResolve {
    Always,
    IfNotPresent,
}

/// Parameters to configure a source for the pipe. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderSourceParameters {
    /// The parameters for using an Active MQ broker as a source. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "activemqBrokerParameters")]
    pub activemq_broker_parameters: Option<PipeForProviderSourceParametersActivemqBrokerParameters>,
    /// The parameters for using a DynamoDB stream as a source.  Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dynamodbStreamParameters")]
    pub dynamodb_stream_parameters: Option<PipeForProviderSourceParametersDynamodbStreamParameters>,
    /// The collection of event patterns used to filter events. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "filterCriteria")]
    pub filter_criteria: Option<PipeForProviderSourceParametersFilterCriteria>,
    /// The parameters for using a Kinesis stream as a source. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kinesisStreamParameters")]
    pub kinesis_stream_parameters: Option<PipeForProviderSourceParametersKinesisStreamParameters>,
    /// The parameters for using an MSK stream as a source. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "managedStreamingKafkaParameters")]
    pub managed_streaming_kafka_parameters: Option<PipeForProviderSourceParametersManagedStreamingKafkaParameters>,
    /// The parameters for using a Rabbit MQ broker as a source. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "rabbitmqBrokerParameters")]
    pub rabbitmq_broker_parameters: Option<PipeForProviderSourceParametersRabbitmqBrokerParameters>,
    /// The parameters for using a self-managed Apache Kafka stream as a source. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "selfManagedKafkaParameters")]
    pub self_managed_kafka_parameters: Option<PipeForProviderSourceParametersSelfManagedKafkaParameters>,
    /// The parameters for using a Amazon SQS stream as a source. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sqsQueueParameters")]
    pub sqs_queue_parameters: Option<PipeForProviderSourceParametersSqsQueueParameters>,
}

/// The parameters for using an Active MQ broker as a source. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderSourceParametersActivemqBrokerParameters {
    /// The maximum number of records to include in each batch. Maximum value of 10000.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "batchSize")]
    pub batch_size: Option<f64>,
    /// The credentials needed to access the resource. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub credentials: Option<PipeForProviderSourceParametersActivemqBrokerParametersCredentials>,
    /// The maximum length of a time to wait for events. Maximum value of 300.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maximumBatchingWindowInSeconds")]
    pub maximum_batching_window_in_seconds: Option<f64>,
    /// The name of the destination queue to consume. Maximum length of 1000.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queueName")]
    pub queue_name: Option<String>,
}

/// The credentials needed to access the resource. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderSourceParametersActivemqBrokerParametersCredentials {
    /// The ARN of the Secrets Manager secret containing the basic auth credentials.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "basicAuth")]
    pub basic_auth: Option<String>,
}

/// The parameters for using a DynamoDB stream as a source.  Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderSourceParametersDynamodbStreamParameters {
    /// The maximum number of records to include in each batch. Maximum value of 10000.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "batchSize")]
    pub batch_size: Option<f64>,
    /// Define the target queue to send dead-letter queue events to. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "deadLetterConfig")]
    pub dead_letter_config: Option<PipeForProviderSourceParametersDynamodbStreamParametersDeadLetterConfig>,
    /// The maximum length of a time to wait for events. Maximum value of 300.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maximumBatchingWindowInSeconds")]
    pub maximum_batching_window_in_seconds: Option<f64>,
    /// Discard records older than the specified age. The default value is -1, which sets the maximum age to infinite. When the value is set to infinite, EventBridge never discards old records. Maximum value of 604,800.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maximumRecordAgeInSeconds")]
    pub maximum_record_age_in_seconds: Option<f64>,
    /// Discard records after the specified number of retries. The default value is -1, which sets the maximum number of retries to infinite. When MaximumRetryAttempts is infinite, EventBridge retries failed records until the record expires in the event source. Maximum value of 10,000.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maximumRetryAttempts")]
    pub maximum_retry_attempts: Option<f64>,
    /// Define how to handle item process failures. AUTOMATIC_BISECT halves each batch and retry each half until all the records are processed or there is one failed message left in the batch. Valid values: AUTOMATIC_BISECT.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "onPartialBatchItemFailure")]
    pub on_partial_batch_item_failure: Option<String>,
    /// The number of batches to process concurrently from each shard. The default value is 1. Maximum value of 10.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "parallelizationFactor")]
    pub parallelization_factor: Option<f64>,
    /// The position in a stream from which to start reading. Valid values: TRIM_HORIZON, LATEST.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "startingPosition")]
    pub starting_position: Option<String>,
}

/// Define the target queue to send dead-letter queue events to. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderSourceParametersDynamodbStreamParametersDeadLetterConfig {
    /// The ARN of the Amazon SQS queue specified as the target for the dead-letter queue.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub arn: Option<String>,
}

/// The collection of event patterns used to filter events. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderSourceParametersFilterCriteria {
    /// An array of up to 5 event patterns. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub filter: Option<Vec<PipeForProviderSourceParametersFilterCriteriaFilter>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderSourceParametersFilterCriteriaFilter {
    /// The event pattern. At most 4096 characters.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub pattern: Option<String>,
}

/// The parameters for using a Kinesis stream as a source. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderSourceParametersKinesisStreamParameters {
    /// The maximum number of records to include in each batch. Maximum value of 10000.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "batchSize")]
    pub batch_size: Option<f64>,
    /// Define the target queue to send dead-letter queue events to. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "deadLetterConfig")]
    pub dead_letter_config: Option<PipeForProviderSourceParametersKinesisStreamParametersDeadLetterConfig>,
    /// The maximum length of a time to wait for events. Maximum value of 300.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maximumBatchingWindowInSeconds")]
    pub maximum_batching_window_in_seconds: Option<f64>,
    /// Discard records older than the specified age. The default value is -1, which sets the maximum age to infinite. When the value is set to infinite, EventBridge never discards old records. Maximum value of 604,800.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maximumRecordAgeInSeconds")]
    pub maximum_record_age_in_seconds: Option<f64>,
    /// Discard records after the specified number of retries. The default value is -1, which sets the maximum number of retries to infinite. When MaximumRetryAttempts is infinite, EventBridge retries failed records until the record expires in the event source. Maximum value of 10,000.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maximumRetryAttempts")]
    pub maximum_retry_attempts: Option<f64>,
    /// Define how to handle item process failures. AUTOMATIC_BISECT halves each batch and retry each half until all the records are processed or there is one failed message left in the batch. Valid values: AUTOMATIC_BISECT.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "onPartialBatchItemFailure")]
    pub on_partial_batch_item_failure: Option<String>,
    /// The number of batches to process concurrently from each shard. The default value is 1. Maximum value of 10.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "parallelizationFactor")]
    pub parallelization_factor: Option<f64>,
    /// The position in a stream from which to start reading. Valid values: TRIM_HORIZON, LATEST, AT_TIMESTAMP.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "startingPosition")]
    pub starting_position: Option<String>,
    /// With StartingPosition set to AT_TIMESTAMP, the time from which to start reading, in Unix time seconds.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "startingPositionTimestamp")]
    pub starting_position_timestamp: Option<String>,
}

/// Define the target queue to send dead-letter queue events to. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderSourceParametersKinesisStreamParametersDeadLetterConfig {
    /// The ARN of the Amazon SQS queue specified as the target for the dead-letter queue.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub arn: Option<String>,
}

/// The parameters for using an MSK stream as a source. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderSourceParametersManagedStreamingKafkaParameters {
    /// The maximum number of records to include in each batch. Maximum value of 10000.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "batchSize")]
    pub batch_size: Option<f64>,
    /// The name of the destination queue to consume. Maximum value of 200.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "consumerGroupId")]
    pub consumer_group_id: Option<String>,
    /// The credentials needed to access the resource. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub credentials: Option<PipeForProviderSourceParametersManagedStreamingKafkaParametersCredentials>,
    /// The maximum length of a time to wait for events. Maximum value of 300.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maximumBatchingWindowInSeconds")]
    pub maximum_batching_window_in_seconds: Option<f64>,
    /// The position in a stream from which to start reading. Valid values: TRIM_HORIZON, LATEST.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "startingPosition")]
    pub starting_position: Option<String>,
    /// The name of the topic that the pipe will read from. Maximum length of 249.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "topicName")]
    pub topic_name: Option<String>,
}

/// The credentials needed to access the resource. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderSourceParametersManagedStreamingKafkaParametersCredentials {
    /// The ARN of the Secrets Manager secret containing the credentials.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clientCertificateTlsAuth")]
    pub client_certificate_tls_auth: Option<String>,
    /// The ARN of the Secrets Manager secret containing the credentials.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "saslScram512Auth")]
    pub sasl_scram512_auth: Option<String>,
}

/// The parameters for using a Rabbit MQ broker as a source. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderSourceParametersRabbitmqBrokerParameters {
    /// The maximum number of records to include in each batch. Maximum value of 10000.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "batchSize")]
    pub batch_size: Option<f64>,
    /// The credentials needed to access the resource. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub credentials: Option<PipeForProviderSourceParametersRabbitmqBrokerParametersCredentials>,
    /// The maximum length of a time to wait for events. Maximum value of 300.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maximumBatchingWindowInSeconds")]
    pub maximum_batching_window_in_seconds: Option<f64>,
    /// The name of the destination queue to consume. Maximum length of 1000.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queueName")]
    pub queue_name: Option<String>,
    /// The name of the virtual host associated with the source broker. Maximum length of 200.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "virtualHost")]
    pub virtual_host: Option<String>,
}

/// The credentials needed to access the resource. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderSourceParametersRabbitmqBrokerParametersCredentials {
    /// The ARN of the Secrets Manager secret containing the credentials.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "basicAuth")]
    pub basic_auth: Option<String>,
}

/// The parameters for using a self-managed Apache Kafka stream as a source. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderSourceParametersSelfManagedKafkaParameters {
    /// An array of server URLs. Maximum number of 2 items, each of maximum length 300.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "additionalBootstrapServers")]
    pub additional_bootstrap_servers: Option<Vec<String>>,
    /// The maximum number of records to include in each batch. Maximum value of 10000.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "batchSize")]
    pub batch_size: Option<f64>,
    /// The name of the destination queue to consume. Maximum value of 200.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "consumerGroupId")]
    pub consumer_group_id: Option<String>,
    /// The credentials needed to access the resource. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub credentials: Option<PipeForProviderSourceParametersSelfManagedKafkaParametersCredentials>,
    /// The maximum length of a time to wait for events. Maximum value of 300.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maximumBatchingWindowInSeconds")]
    pub maximum_batching_window_in_seconds: Option<f64>,
    /// The ARN of the Secrets Manager secret used for certification.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serverRootCaCertificate")]
    pub server_root_ca_certificate: Option<String>,
    /// The position in a stream from which to start reading. Valid values: TRIM_HORIZON, LATEST.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "startingPosition")]
    pub starting_position: Option<String>,
    /// The name of the topic that the pipe will read from. Maximum length of 249.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "topicName")]
    pub topic_name: Option<String>,
    /// This structure specifies the VPC subnets and security groups for the stream, and whether a public IP address is to be used. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub vpc: Option<PipeForProviderSourceParametersSelfManagedKafkaParametersVpc>,
}

/// The credentials needed to access the resource. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderSourceParametersSelfManagedKafkaParametersCredentials {
    /// The ARN of the Secrets Manager secret containing the credentials.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "basicAuth")]
    pub basic_auth: Option<String>,
    /// The ARN of the Secrets Manager secret containing the credentials.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clientCertificateTlsAuth")]
    pub client_certificate_tls_auth: Option<String>,
    /// The ARN of the Secrets Manager secret containing the credentials.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "saslScram256Auth")]
    pub sasl_scram256_auth: Option<String>,
    /// The ARN of the Secrets Manager secret containing the credentials.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "saslScram512Auth")]
    pub sasl_scram512_auth: Option<String>,
}

/// This structure specifies the VPC subnets and security groups for the stream, and whether a public IP address is to be used. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderSourceParametersSelfManagedKafkaParametersVpc {
    /// List of security groups associated with the stream. These security groups must all be in the same VPC. You can specify as many as five security groups. If you do not specify a security group, the default security group for the VPC is used.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "securityGroups")]
    pub security_groups: Option<Vec<String>>,
    /// List of the subnets associated with the stream. These subnets must all be in the same VPC. You can specify as many as 16 subnets.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub subnets: Option<Vec<String>>,
}

/// The parameters for using a Amazon SQS stream as a source. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderSourceParametersSqsQueueParameters {
    /// The maximum number of records to include in each batch. Maximum value of 10000.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "batchSize")]
    pub batch_size: Option<f64>,
    /// The maximum length of a time to wait for events. Maximum value of 300.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maximumBatchingWindowInSeconds")]
    pub maximum_batching_window_in_seconds: Option<f64>,
}

/// Reference to a Queue in sqs to populate source.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderSourceRef {
    /// Name of the referenced object.
    pub name: String,
    /// Policies for referencing.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub policy: Option<PipeForProviderSourceRefPolicy>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderSourceRefPolicy {
    /// Resolution specifies whether resolution of this reference is required.
    /// The default is 'Required', which means the reconcile will fail if the
    /// reference cannot be resolved. 'Optional' means this reference will be
    /// a no-op if it cannot be resolved.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolution: Option<PipeForProviderSourceRefPolicyResolution>,
    /// Resolve specifies when this reference should be resolved. The default
    /// is 'IfNotPresent', which will attempt to resolve the reference only when
    /// the corresponding field is not present. Use 'Always' to resolve the
    /// reference on every reconcile.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolve: Option<PipeForProviderSourceRefPolicyResolve>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum PipeForProviderSourceRefPolicyResolution {
    Required,
    Optional,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum PipeForProviderSourceRefPolicyResolve {
    Always,
    IfNotPresent,
}

/// Selector for a Queue in sqs to populate source.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderSourceSelector {
    /// MatchControllerRef ensures an object with the same controller reference
    /// as the selecting object is selected.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchControllerRef")]
    pub match_controller_ref: Option<bool>,
    /// MatchLabels ensures an object with matching labels is selected.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<HashMap<String, String>>,
    /// Policies for selection.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub policy: Option<PipeForProviderSourceSelectorPolicy>,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderSourceSelectorPolicy {
    /// Resolution specifies whether resolution of this reference is required.
    /// The default is 'Required', which means the reconcile will fail if the
    /// reference cannot be resolved. 'Optional' means this reference will be
    /// a no-op if it cannot be resolved.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolution: Option<PipeForProviderSourceSelectorPolicyResolution>,
    /// Resolve specifies when this reference should be resolved. The default
    /// is 'IfNotPresent', which will attempt to resolve the reference only when
    /// the corresponding field is not present. Use 'Always' to resolve the
    /// reference on every reconcile.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolve: Option<PipeForProviderSourceSelectorPolicyResolve>,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum PipeForProviderSourceSelectorPolicyResolution {
    Required,
    Optional,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum PipeForProviderSourceSelectorPolicyResolve {
    Always,
    IfNotPresent,
}

/// Parameters to configure a target for your pipe. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderTargetParameters {
    /// The parameters for using an AWS Batch job as a target. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "batchJobParameters")]
    pub batch_job_parameters: Option<PipeForProviderTargetParametersBatchJobParameters>,
    /// The parameters for using an CloudWatch Logs log stream as a target. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "cloudwatchLogsParameters")]
    pub cloudwatch_logs_parameters: Option<PipeForProviderTargetParametersCloudwatchLogsParameters>,
    /// The parameters for using an Amazon ECS task as a target. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "ecsTaskParameters")]
    pub ecs_task_parameters: Option<PipeForProviderTargetParametersEcsTaskParameters>,
    /// The parameters for using an EventBridge event bus as a target. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "eventbridgeEventBusParameters")]
    pub eventbridge_event_bus_parameters: Option<PipeForProviderTargetParametersEventbridgeEventBusParameters>,
    /// These are custom parameter to be used when the target is an API Gateway REST APIs or EventBridge ApiDestinations. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpParameters")]
    pub http_parameters: Option<PipeForProviderTargetParametersHttpParameters>,
    /// Valid JSON text passed to the target. In this case, nothing from the event itself is passed to the target. Maximum length of 8192 characters.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "inputTemplate")]
    pub input_template: Option<String>,
    /// The parameters for using a Kinesis stream as a source. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kinesisStreamParameters")]
    pub kinesis_stream_parameters: Option<PipeForProviderTargetParametersKinesisStreamParameters>,
    /// The parameters for using a Lambda function as a target. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "lambdaFunctionParameters")]
    pub lambda_function_parameters: Option<PipeForProviderTargetParametersLambdaFunctionParameters>,
    /// These are custom parameters to be used when the target is a Amazon Redshift cluster to invoke the Amazon Redshift Data API BatchExecuteStatement. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "redshiftDataParameters")]
    pub redshift_data_parameters: Option<PipeForProviderTargetParametersRedshiftDataParameters>,
    /// The parameters for using a SageMaker pipeline as a target. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sagemakerPipelineParameters")]
    pub sagemaker_pipeline_parameters: Option<PipeForProviderTargetParametersSagemakerPipelineParameters>,
    /// The parameters for using a Amazon SQS stream as a target. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sqsQueueParameters")]
    pub sqs_queue_parameters: Option<PipeForProviderTargetParametersSqsQueueParameters>,
    /// The parameters for using a Step Functions state machine as a target. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "stepFunctionStateMachineParameters")]
    pub step_function_state_machine_parameters: Option<PipeForProviderTargetParametersStepFunctionStateMachineParameters>,
}

/// The parameters for using an AWS Batch job as a target. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderTargetParametersBatchJobParameters {
    /// The array properties for the submitted job, such as the size of the array. The array size can be between 2 and 10,000. If you specify array properties for a job, it becomes an array job. This parameter is used only if the target is an AWS Batch job. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "arrayProperties")]
    pub array_properties: Option<PipeForProviderTargetParametersBatchJobParametersArrayProperties>,
    /// The overrides that are sent to a container. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "containerOverrides")]
    pub container_overrides: Option<PipeForProviderTargetParametersBatchJobParametersContainerOverrides>,
    /// A list of dependencies for the job. A job can depend upon a maximum of 20 jobs. You can specify a SEQUENTIAL type dependency without specifying a job ID for array jobs so that each child array job completes sequentially, starting at index 0. You can also specify an N_TO_N type dependency with a job ID for array jobs. In that case, each index child of this job must wait for the corresponding index child of each dependency to complete before it can begin. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dependsOn")]
    pub depends_on: Option<Vec<PipeForProviderTargetParametersBatchJobParametersDependsOn>>,
    /// The job definition used by this job. This value can be one of name, name:revision, or the Amazon Resource Name (ARN) for the job definition. If name is specified without a revision then the latest active revision is used.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jobDefinition")]
    pub job_definition: Option<String>,
    /// The name of the job. It can be up to 128 letters long.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jobName")]
    pub job_name: Option<String>,
    /// Additional parameters passed to the job that replace parameter substitution placeholders that are set in the job definition. Parameters are specified as a key and value pair mapping. Parameters included here override any corresponding parameter defaults from the job definition. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub parameters: Option<HashMap<String, String>>,
    /// The retry strategy to use for failed jobs. When a retry strategy is specified here, it overrides the retry strategy defined in the job definition. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "retryStrategy")]
    pub retry_strategy: Option<PipeForProviderTargetParametersBatchJobParametersRetryStrategy>,
}

/// The array properties for the submitted job, such as the size of the array. The array size can be between 2 and 10,000. If you specify array properties for a job, it becomes an array job. This parameter is used only if the target is an AWS Batch job. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderTargetParametersBatchJobParametersArrayProperties {
    /// The size of the array, if this is an array batch job. Minimum value of 2. Maximum value of 10,000.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub size: Option<f64>,
}

/// The overrides that are sent to a container. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderTargetParametersBatchJobParametersContainerOverrides {
    /// List of commands to send to the container that overrides the default command from the Docker image or the task definition.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
    /// The environment variables to send to the container. You can add new environment variables, which are added to the container at launch, or you can override the existing environment variables from the Docker image or the task definition. Environment variables cannot start with " AWS Batch ". This naming convention is reserved for variables that AWS Batch sets. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub environment: Option<Vec<PipeForProviderTargetParametersBatchJobParametersContainerOverridesEnvironment>>,
    /// The instance type to use for a multi-node parallel job. This parameter isn't applicable to single-node container jobs or jobs that run on Fargate resources, and shouldn't be provided.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "instanceType")]
    pub instance_type: Option<String>,
    /// The type and amount of resources to assign to a container. This overrides the settings in the job definition. The supported resources include GPU, MEMORY, and VCPU. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "resourceRequirement")]
    pub resource_requirement: Option<Vec<PipeForProviderTargetParametersBatchJobParametersContainerOverridesResourceRequirement>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderTargetParametersBatchJobParametersContainerOverridesEnvironment {
    /// The name of the key-value pair. For environment variables, this is the name of the environment variable.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// The value of the key-value pair. For environment variables, this is the value of the environment variable.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderTargetParametersBatchJobParametersContainerOverridesResourceRequirement {
    /// The type of resource to assign to a container. The supported resources include GPU, MEMORY, and VCPU.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<String>,
    /// The quantity of the specified resource to reserve for the container. The values vary based on the type specified.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderTargetParametersBatchJobParametersDependsOn {
    /// The job ID of the AWS Batch job that's associated with this dependency.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jobId")]
    pub job_id: Option<String>,
    /// The type of the job dependency. Valid Values: N_TO_N, SEQUENTIAL.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<String>,
}

/// The retry strategy to use for failed jobs. When a retry strategy is specified here, it overrides the retry strategy defined in the job definition. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderTargetParametersBatchJobParametersRetryStrategy {
    /// The number of times to move a job to the RUNNABLE status. If the value of attempts is greater than one, the job is retried on failure the same number of attempts as the value. Maximum value of 10.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub attempts: Option<f64>,
}

/// The parameters for using an CloudWatch Logs log stream as a target. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderTargetParametersCloudwatchLogsParameters {
    /// The name of the log stream.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "logStreamName")]
    pub log_stream_name: Option<String>,
    /// The time the event occurred, expressed as the number of milliseconds after Jan 1, 1970 00:00:00 UTC. This is the JSON path to the field in the event e.g. $.detail.timestamp
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub timestamp: Option<String>,
}

/// The parameters for using an Amazon ECS task as a target. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderTargetParametersEcsTaskParameters {
    /// List of capacity provider strategies to use for the task. If a capacityProviderStrategy is specified, the launchType parameter must be omitted. If no capacityProviderStrategy or launchType is specified, the defaultCapacityProviderStrategy for the cluster is used. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "capacityProviderStrategy")]
    pub capacity_provider_strategy: Option<Vec<PipeForProviderTargetParametersEcsTaskParametersCapacityProviderStrategy>>,
    /// Specifies whether to enable Amazon ECS managed tags for the task. Valid values: true, false.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableEcsManagedTags")]
    pub enable_ecs_managed_tags: Option<bool>,
    /// Whether or not to enable the execute command functionality for the containers in this task. If true, this enables execute command functionality on all containers in the task. Valid values: true, false.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableExecuteCommand")]
    pub enable_execute_command: Option<bool>,
    /// Specifies an Amazon ECS task group for the task. The maximum length is 255 characters.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub group: Option<String>,
    /// Specifies the launch type on which your task is running. The launch type that you specify here must match one of the launch type (compatibilities) of the target task. The FARGATE value is supported only in the Regions where AWS Fargate with Amazon ECS is supported. Valid Values: EC2, FARGATE, EXTERNAL
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "launchType")]
    pub launch_type: Option<String>,
    /// Use this structure if the Amazon ECS task uses the awsvpc network mode. This structure specifies the VPC subnets and security groups associated with the task, and whether a public IP address is to be used. This structure is required if LaunchType is FARGATE because the awsvpc mode is required for Fargate tasks. If you specify NetworkConfiguration when the target ECS task does not use the awsvpc network mode, the task fails. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "networkConfiguration")]
    pub network_configuration: Option<PipeForProviderTargetParametersEcsTaskParametersNetworkConfiguration>,
    /// The overrides that are associated with a task. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub overrides: Option<PipeForProviderTargetParametersEcsTaskParametersOverrides>,
    /// An array of placement constraint objects to use for the task. You can specify up to 10 constraints per task (including constraints in the task definition and those specified at runtime). Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "placementConstraint")]
    pub placement_constraint: Option<Vec<PipeForProviderTargetParametersEcsTaskParametersPlacementConstraint>>,
    /// The placement strategy objects to use for the task. You can specify a maximum of five strategy rules per task. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "placementStrategy")]
    pub placement_strategy: Option<Vec<PipeForProviderTargetParametersEcsTaskParametersPlacementStrategy>>,
    /// Specifies the platform version for the task. Specify only the numeric portion of the platform version, such as 1.1.0. This structure is used only if LaunchType is FARGATE.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "platformVersion")]
    pub platform_version: Option<String>,
    /// Specifies whether to propagate the tags from the task definition to the task. If no value is specified, the tags are not propagated. Tags can only be propagated to the task during task creation. To add tags to a task after task creation, use the TagResource API action. Valid Values: TASK_DEFINITION
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "propagateTags")]
    pub propagate_tags: Option<String>,
    /// The reference ID to use for the task. Maximum length of 1,024.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "referenceId")]
    pub reference_id: Option<String>,
    /// Key-value map of tags that you apply to the task to help you categorize and organize them.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tags: Option<HashMap<String, String>>,
    /// The number of tasks to create based on TaskDefinition. The default is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "taskCount")]
    pub task_count: Option<f64>,
    /// The ARN of the task definition to use if the event target is an Amazon ECS task.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "taskDefinitionArn")]
    pub task_definition_arn: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderTargetParametersEcsTaskParametersCapacityProviderStrategy {
    /// The base value designates how many tasks, at a minimum, to run on the specified capacity provider. Only one capacity provider in a capacity provider strategy can have a base defined. If no value is specified, the default value of 0 is used. Maximum value of 100,000.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub base: Option<f64>,
    /// The short name of the capacity provider. Maximum value of 255.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "capacityProvider")]
    pub capacity_provider: Option<String>,
    /// The weight value designates the relative percentage of the total number of tasks launched that should use the specified capacity provider. The weight value is taken into consideration after the base value, if defined, is satisfied. Maximum value of 1,000.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub weight: Option<f64>,
}

/// Use this structure if the Amazon ECS task uses the awsvpc network mode. This structure specifies the VPC subnets and security groups associated with the task, and whether a public IP address is to be used. This structure is required if LaunchType is FARGATE because the awsvpc mode is required for Fargate tasks. If you specify NetworkConfiguration when the target ECS task does not use the awsvpc network mode, the task fails. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderTargetParametersEcsTaskParametersNetworkConfiguration {
    /// Use this structure to specify the VPC subnets and security groups for the task, and whether a public IP address is to be used. This structure is relevant only for ECS tasks that use the awsvpc network mode. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "awsVpcConfiguration")]
    pub aws_vpc_configuration: Option<PipeForProviderTargetParametersEcsTaskParametersNetworkConfigurationAwsVpcConfiguration>,
}

/// Use this structure to specify the VPC subnets and security groups for the task, and whether a public IP address is to be used. This structure is relevant only for ECS tasks that use the awsvpc network mode. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderTargetParametersEcsTaskParametersNetworkConfigurationAwsVpcConfiguration {
    /// Specifies whether the task's elastic network interface receives a public IP address. You can specify ENABLED only when LaunchType in EcsParameters is set to FARGATE. Valid Values: ENABLED, DISABLED.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "assignPublicIp")]
    pub assign_public_ip: Option<String>,
    /// Specifies the security groups associated with the task. These security groups must all be in the same VPC. You can specify as many as five security groups. If you do not specify a security group, the default security group for the VPC is used.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "securityGroups")]
    pub security_groups: Option<Vec<String>>,
    /// Specifies the subnets associated with the task. These subnets must all be in the same VPC. You can specify as many as 16 subnets.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub subnets: Option<Vec<String>>,
}

/// The overrides that are associated with a task. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderTargetParametersEcsTaskParametersOverrides {
    /// One or more container overrides that are sent to a task. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "containerOverride")]
    pub container_override: Option<Vec<PipeForProviderTargetParametersEcsTaskParametersOverridesContainerOverride>>,
    /// The cpu override for the task.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub cpu: Option<String>,
    /// The ephemeral storage setting override for the task.  Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "ephemeralStorage")]
    pub ephemeral_storage: Option<PipeForProviderTargetParametersEcsTaskParametersOverridesEphemeralStorage>,
    /// The Amazon Resource Name (ARN) of the task execution IAM role override for the task.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "executionRoleArn")]
    pub execution_role_arn: Option<String>,
    /// List of Elastic Inference accelerator overrides for the task. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "inferenceAcceleratorOverride")]
    pub inference_accelerator_override: Option<Vec<PipeForProviderTargetParametersEcsTaskParametersOverridesInferenceAcceleratorOverride>>,
    /// The memory override for the task.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub memory: Option<String>,
    /// The Amazon Resource Name (ARN) of the IAM role that containers in this task can assume. All containers in this task are granted the permissions that are specified in this role.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "taskRoleArn")]
    pub task_role_arn: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderTargetParametersEcsTaskParametersOverridesContainerOverride {
    /// List of commands to send to the container that overrides the default command from the Docker image or the task definition. You must also specify a container name.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
    /// The number of cpu units reserved for the container, instead of the default value from the task definition. You must also specify a container name.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub cpu: Option<f64>,
    /// The environment variables to send to the container. You can add new environment variables, which are added to the container at launch, or you can override the existing environment variables from the Docker image or the task definition. You must also specify a container name. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub environment: Option<Vec<PipeForProviderTargetParametersEcsTaskParametersOverridesContainerOverrideEnvironment>>,
    /// A list of files containing the environment variables to pass to a container, instead of the value from the container definition. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "environmentFile")]
    pub environment_file: Option<Vec<PipeForProviderTargetParametersEcsTaskParametersOverridesContainerOverrideEnvironmentFile>>,
    /// The hard limit (in MiB) of memory to present to the container, instead of the default value from the task definition. If your container attempts to exceed the memory specified here, the container is killed. You must also specify a container name.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub memory: Option<f64>,
    /// The soft limit (in MiB) of memory to reserve for the container, instead of the default value from the task definition. You must also specify a container name.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "memoryReservation")]
    pub memory_reservation: Option<f64>,
    /// The name of the container that receives the override. This parameter is required if any override is specified.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// The type and amount of a resource to assign to a container, instead of the default value from the task definition. The only supported resource is a GPU. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "resourceRequirement")]
    pub resource_requirement: Option<Vec<PipeForProviderTargetParametersEcsTaskParametersOverridesContainerOverrideResourceRequirement>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderTargetParametersEcsTaskParametersOverridesContainerOverrideEnvironment {
    /// The name of the key-value pair. For environment variables, this is the name of the environment variable.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// The value of the key-value pair. For environment variables, this is the value of the environment variable.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderTargetParametersEcsTaskParametersOverridesContainerOverrideEnvironmentFile {
    /// The file type to use. The only supported value is s3.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<String>,
    /// The Amazon Resource Name (ARN) of the Amazon S3 object containing the environment variable file.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderTargetParametersEcsTaskParametersOverridesContainerOverrideResourceRequirement {
    /// The type of resource to assign to a container. The supported values are GPU or InferenceAccelerator.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<String>,
    /// The value for the specified resource type. If the GPU type is used, the value is the number of physical GPUs the Amazon ECS container agent reserves for the container. The number of GPUs that's reserved for all containers in a task can't exceed the number of available GPUs on the container instance that the task is launched on. If the InferenceAccelerator type is used, the value matches the deviceName for an InferenceAccelerator specified in a task definition.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
}

/// The ephemeral storage setting override for the task.  Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderTargetParametersEcsTaskParametersOverridesEphemeralStorage {
    /// The total amount, in GiB, of ephemeral storage to set for the task. The minimum supported value is 21 GiB and the maximum supported value is 200 GiB.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sizeInGib")]
    pub size_in_gib: Option<f64>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderTargetParametersEcsTaskParametersOverridesInferenceAcceleratorOverride {
    /// The Elastic Inference accelerator device name to override for the task. This parameter must match a deviceName specified in the task definition.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "deviceName")]
    pub device_name: Option<String>,
    /// The Elastic Inference accelerator type to use.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "deviceType")]
    pub device_type: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderTargetParametersEcsTaskParametersPlacementConstraint {
    /// A cluster query language expression to apply to the constraint. You cannot specify an expression if the constraint type is distinctInstance. Maximum length of 2,000.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub expression: Option<String>,
    /// The type of constraint. Use distinctInstance to ensure that each task in a particular group is running on a different container instance. Use memberOf to restrict the selection to a group of valid candidates. Valid Values: distinctInstance, memberOf.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderTargetParametersEcsTaskParametersPlacementStrategy {
    /// The field to apply the placement strategy against. For the spread placement strategy, valid values are instanceId (or host, which has the same effect), or any platform or custom attribute that is applied to a container instance, such as attribute:ecs.availability-zone. For the binpack placement strategy, valid values are cpu and memory. For the random placement strategy, this field is not used. Maximum length of 255.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub field: Option<String>,
    /// The type of placement strategy. The random placement strategy randomly places tasks on available candidates. The spread placement strategy spreads placement across available candidates evenly based on the field parameter. The binpack strategy places tasks on available candidates that have the least available amount of the resource that is specified with the field parameter. For example, if you binpack on memory, a task is placed on the instance with the least amount of remaining memory (but still enough to run the task). Valid Values: random, spread, binpack.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<String>,
}

/// The parameters for using an EventBridge event bus as a target. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderTargetParametersEventbridgeEventBusParameters {
    /// A free-form string, with a maximum of 128 characters, used to decide what fields to expect in the event detail.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "detailType")]
    pub detail_type: Option<String>,
    /// The URL subdomain of the endpoint. For example, if the URL for Endpoint is https://abcde.veo.endpoints.event.amazonaws.com, then the EndpointId is abcde.veo.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "endpointId")]
    pub endpoint_id: Option<String>,
    /// List of AWS resources, identified by Amazon Resource Name (ARN), which the event primarily concerns. Any number, including zero, may be present.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resources: Option<Vec<String>>,
    /// The source of the event. Maximum length of 256.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub source: Option<String>,
    /// The time stamp of the event, per RFC3339. If no time stamp is provided, the time stamp of the PutEvents call is used. This is the JSON path to the field in the event e.g. $.detail.timestamp
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub time: Option<String>,
}

/// These are custom parameter to be used when the target is an API Gateway REST APIs or EventBridge ApiDestinations. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderTargetParametersHttpParameters {
    /// Key-value mapping of the headers that need to be sent as part of request invoking the API Gateway REST API or EventBridge ApiDestination. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "headerParameters")]
    pub header_parameters: Option<HashMap<String, String>>,
    /// The path parameter values to be used to populate API Gateway REST API or EventBridge ApiDestination path wildcards ("*").
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "pathParameterValues")]
    pub path_parameter_values: Option<Vec<String>>,
    /// Key-value mapping of the query strings that need to be sent as part of request invoking the API Gateway REST API or EventBridge ApiDestination. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryStringParameters")]
    pub query_string_parameters: Option<HashMap<String, String>>,
}

/// The parameters for using a Kinesis stream as a source. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderTargetParametersKinesisStreamParameters {
    /// Determines which shard in the stream the data record is assigned to. Partition keys are Unicode strings with a maximum length limit of 256 characters for each key. Amazon Kinesis Data Streams uses the partition key as input to a hash function that maps the partition key and associated data to a specific shard. Specifically, an MD5 hash function is used to map partition keys to 128-bit integer values and to map associated data records to shards. As a result of this hashing mechanism, all data records with the same partition key map to the same shard within the stream.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "partitionKey")]
    pub partition_key: Option<String>,
}

/// The parameters for using a Lambda function as a target. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderTargetParametersLambdaFunctionParameters {
    /// Specify whether to invoke the function synchronously or asynchronously. Valid Values: REQUEST_RESPONSE, FIRE_AND_FORGET.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "invocationType")]
    pub invocation_type: Option<String>,
}

/// These are custom parameters to be used when the target is a Amazon Redshift cluster to invoke the Amazon Redshift Data API BatchExecuteStatement. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderTargetParametersRedshiftDataParameters {
    /// The name of the database. Required when authenticating using temporary credentials.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub database: Option<String>,
    /// The database user name. Required when authenticating using temporary credentials.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dbUser")]
    pub db_user: Option<String>,
    /// The name or ARN of the secret that enables access to the database. Required when authenticating using Secrets Manager.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretManagerArn")]
    pub secret_manager_arn: Option<String>,
    /// List of SQL statements text to run, each of maximum length of 100,000.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sqls: Option<Vec<String>>,
    /// The name of the SQL statement. You can name the SQL statement when you create it to identify the query.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "statementName")]
    pub statement_name: Option<String>,
    /// Indicates whether to send an event back to EventBridge after the SQL statement runs.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "withEvent")]
    pub with_event: Option<bool>,
}

/// The parameters for using a SageMaker pipeline as a target. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderTargetParametersSagemakerPipelineParameters {
    /// List of Parameter names and values for SageMaker Model Building Pipeline execution. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "pipelineParameter")]
    pub pipeline_parameter: Option<Vec<PipeForProviderTargetParametersSagemakerPipelineParametersPipelineParameter>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderTargetParametersSagemakerPipelineParametersPipelineParameter {
    /// The name of the container that receives the override. This parameter is required if any override is specified.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Value of parameter to start execution of a SageMaker Model Building Pipeline. Maximum length of 1024.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
}

/// The parameters for using a Amazon SQS stream as a target. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderTargetParametersSqsQueueParameters {
    /// This parameter applies only to FIFO (first-in-first-out) queues. The token used for deduplication of sent messages.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "messageDeduplicationId")]
    pub message_deduplication_id: Option<String>,
    /// The FIFO message group ID to use as the target.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "messageGroupId")]
    pub message_group_id: Option<String>,
}

/// The parameters for using a Step Functions state machine as a target. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderTargetParametersStepFunctionStateMachineParameters {
    /// Specify whether to invoke the function synchronously or asynchronously. Valid Values: REQUEST_RESPONSE, FIRE_AND_FORGET.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "invocationType")]
    pub invocation_type: Option<String>,
}

/// Reference to a Queue in sqs to populate target.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderTargetRef {
    /// Name of the referenced object.
    pub name: String,
    /// Policies for referencing.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub policy: Option<PipeForProviderTargetRefPolicy>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderTargetRefPolicy {
    /// Resolution specifies whether resolution of this reference is required.
    /// The default is 'Required', which means the reconcile will fail if the
    /// reference cannot be resolved. 'Optional' means this reference will be
    /// a no-op if it cannot be resolved.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolution: Option<PipeForProviderTargetRefPolicyResolution>,
    /// Resolve specifies when this reference should be resolved. The default
    /// is 'IfNotPresent', which will attempt to resolve the reference only when
    /// the corresponding field is not present. Use 'Always' to resolve the
    /// reference on every reconcile.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolve: Option<PipeForProviderTargetRefPolicyResolve>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum PipeForProviderTargetRefPolicyResolution {
    Required,
    Optional,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum PipeForProviderTargetRefPolicyResolve {
    Always,
    IfNotPresent,
}

/// Selector for a Queue in sqs to populate target.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderTargetSelector {
    /// MatchControllerRef ensures an object with the same controller reference
    /// as the selecting object is selected.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchControllerRef")]
    pub match_controller_ref: Option<bool>,
    /// MatchLabels ensures an object with matching labels is selected.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<HashMap<String, String>>,
    /// Policies for selection.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub policy: Option<PipeForProviderTargetSelectorPolicy>,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeForProviderTargetSelectorPolicy {
    /// Resolution specifies whether resolution of this reference is required.
    /// The default is 'Required', which means the reconcile will fail if the
    /// reference cannot be resolved. 'Optional' means this reference will be
    /// a no-op if it cannot be resolved.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolution: Option<PipeForProviderTargetSelectorPolicyResolution>,
    /// Resolve specifies when this reference should be resolved. The default
    /// is 'IfNotPresent', which will attempt to resolve the reference only when
    /// the corresponding field is not present. Use 'Always' to resolve the
    /// reference on every reconcile.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolve: Option<PipeForProviderTargetSelectorPolicyResolve>,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum PipeForProviderTargetSelectorPolicyResolution {
    Required,
    Optional,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum PipeForProviderTargetSelectorPolicyResolve {
    Always,
    IfNotPresent,
}

/// THIS IS A BETA FIELD. It will be honored
/// unless the Management Policies feature flag is disabled.
/// InitProvider holds the same fields as ForProvider, with the exception
/// of Identifier and other resource reference fields. The fields that are
/// in InitProvider are merged into ForProvider when the resource is created.
/// The same fields are also added to the terraform ignore_changes hook, to
/// avoid updating them after creation. This is useful for fields that are
/// required on creation, but we do not desire to update them after creation,
/// for example because of an external controller is managing them, like an
/// autoscaler.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProvider {
    /// A description of the pipe. At most 512 characters.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub description: Option<String>,
    /// The state the pipe should be in. One of: RUNNING, STOPPED.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "desiredState")]
    pub desired_state: Option<String>,
    /// Enrichment resource of the pipe (typically an ARN). Read more about enrichment in the User Guide.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub enrichment: Option<String>,
    /// Parameters to configure enrichment for your pipe. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enrichmentParameters")]
    pub enrichment_parameters: Option<PipeInitProviderEnrichmentParameters>,
    /// Reference to a APIDestination in cloudwatchevents to populate enrichment.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enrichmentRef")]
    pub enrichment_ref: Option<PipeInitProviderEnrichmentRef>,
    /// Selector for a APIDestination in cloudwatchevents to populate enrichment.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enrichmentSelector")]
    pub enrichment_selector: Option<PipeInitProviderEnrichmentSelector>,
    /// Logging configuration settings for the pipe. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "logConfiguration")]
    pub log_configuration: Option<PipeInitProviderLogConfiguration>,
    /// ARN of the role that allows the pipe to send data to the target.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "roleArn")]
    pub role_arn: Option<String>,
    /// Reference to a Role in iam to populate roleArn.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "roleArnRef")]
    pub role_arn_ref: Option<PipeInitProviderRoleArnRef>,
    /// Selector for a Role in iam to populate roleArn.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "roleArnSelector")]
    pub role_arn_selector: Option<PipeInitProviderRoleArnSelector>,
    /// Source resource of the pipe. This field typically requires an ARN (Amazon Resource Name). However, when using a self-managed Kafka cluster, you should use a different format. Instead of an ARN, use 'smk://' followed by the bootstrap server's address.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub source: Option<String>,
    /// Parameters to configure a source for the pipe. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sourceParameters")]
    pub source_parameters: Option<PipeInitProviderSourceParameters>,
    /// Reference to a Queue in sqs to populate source.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sourceRef")]
    pub source_ref: Option<PipeInitProviderSourceRef>,
    /// Selector for a Queue in sqs to populate source.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sourceSelector")]
    pub source_selector: Option<PipeInitProviderSourceSelector>,
    /// Key-value map of resource tags.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tags: Option<HashMap<String, String>>,
    /// Target resource of the pipe (typically an ARN).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub target: Option<String>,
    /// Parameters to configure a target for your pipe. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "targetParameters")]
    pub target_parameters: Option<PipeInitProviderTargetParameters>,
    /// Reference to a Queue in sqs to populate target.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "targetRef")]
    pub target_ref: Option<PipeInitProviderTargetRef>,
    /// Selector for a Queue in sqs to populate target.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "targetSelector")]
    pub target_selector: Option<PipeInitProviderTargetSelector>,
}

/// Parameters to configure enrichment for your pipe. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderEnrichmentParameters {
    /// Contains the HTTP parameters to use when the target is a API Gateway REST endpoint or EventBridge ApiDestination. If you specify an API Gateway REST API or EventBridge ApiDestination as a target, you can use this parameter to specify headers, path parameters, and query string keys/values as part of your target invoking request. If you're using ApiDestinations, the corresponding Connection can also have these values configured. In case of any conflicting keys, values from the Connection take precedence. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpParameters")]
    pub http_parameters: Option<PipeInitProviderEnrichmentParametersHttpParameters>,
    /// Valid JSON text passed to the target. In this case, nothing from the event itself is passed to the target. Maximum length of 8192 characters.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "inputTemplate")]
    pub input_template: Option<String>,
}

/// Contains the HTTP parameters to use when the target is a API Gateway REST endpoint or EventBridge ApiDestination. If you specify an API Gateway REST API or EventBridge ApiDestination as a target, you can use this parameter to specify headers, path parameters, and query string keys/values as part of your target invoking request. If you're using ApiDestinations, the corresponding Connection can also have these values configured. In case of any conflicting keys, values from the Connection take precedence. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderEnrichmentParametersHttpParameters {
    /// Key-value mapping of the headers that need to be sent as part of request invoking the API Gateway REST API or EventBridge ApiDestination.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "headerParameters")]
    pub header_parameters: Option<HashMap<String, String>>,
    /// The path parameter values to be used to populate API Gateway REST API or EventBridge ApiDestination path wildcards ("*").
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "pathParameterValues")]
    pub path_parameter_values: Option<Vec<String>>,
    /// Key-value mapping of the query strings that need to be sent as part of request invoking the API Gateway REST API or EventBridge ApiDestination.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryStringParameters")]
    pub query_string_parameters: Option<HashMap<String, String>>,
}

/// Reference to a APIDestination in cloudwatchevents to populate enrichment.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderEnrichmentRef {
    /// Name of the referenced object.
    pub name: String,
    /// Policies for referencing.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub policy: Option<PipeInitProviderEnrichmentRefPolicy>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderEnrichmentRefPolicy {
    /// Resolution specifies whether resolution of this reference is required.
    /// The default is 'Required', which means the reconcile will fail if the
    /// reference cannot be resolved. 'Optional' means this reference will be
    /// a no-op if it cannot be resolved.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolution: Option<PipeInitProviderEnrichmentRefPolicyResolution>,
    /// Resolve specifies when this reference should be resolved. The default
    /// is 'IfNotPresent', which will attempt to resolve the reference only when
    /// the corresponding field is not present. Use 'Always' to resolve the
    /// reference on every reconcile.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolve: Option<PipeInitProviderEnrichmentRefPolicyResolve>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum PipeInitProviderEnrichmentRefPolicyResolution {
    Required,
    Optional,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum PipeInitProviderEnrichmentRefPolicyResolve {
    Always,
    IfNotPresent,
}

/// Selector for a APIDestination in cloudwatchevents to populate enrichment.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderEnrichmentSelector {
    /// MatchControllerRef ensures an object with the same controller reference
    /// as the selecting object is selected.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchControllerRef")]
    pub match_controller_ref: Option<bool>,
    /// MatchLabels ensures an object with matching labels is selected.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<HashMap<String, String>>,
    /// Policies for selection.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub policy: Option<PipeInitProviderEnrichmentSelectorPolicy>,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderEnrichmentSelectorPolicy {
    /// Resolution specifies whether resolution of this reference is required.
    /// The default is 'Required', which means the reconcile will fail if the
    /// reference cannot be resolved. 'Optional' means this reference will be
    /// a no-op if it cannot be resolved.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolution: Option<PipeInitProviderEnrichmentSelectorPolicyResolution>,
    /// Resolve specifies when this reference should be resolved. The default
    /// is 'IfNotPresent', which will attempt to resolve the reference only when
    /// the corresponding field is not present. Use 'Always' to resolve the
    /// reference on every reconcile.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolve: Option<PipeInitProviderEnrichmentSelectorPolicyResolve>,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum PipeInitProviderEnrichmentSelectorPolicyResolution {
    Required,
    Optional,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum PipeInitProviderEnrichmentSelectorPolicyResolve {
    Always,
    IfNotPresent,
}

/// Logging configuration settings for the pipe. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderLogConfiguration {
    /// Amazon CloudWatch Logs logging configuration settings for the pipe. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "cloudwatchLogsLogDestination")]
    pub cloudwatch_logs_log_destination: Option<PipeInitProviderLogConfigurationCloudwatchLogsLogDestination>,
    /// Amazon Kinesis Data Firehose logging configuration settings for the pipe. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "firehoseLogDestination")]
    pub firehose_log_destination: Option<PipeInitProviderLogConfigurationFirehoseLogDestination>,
    /// String list that specifies whether the execution data (specifically, the payload, awsRequest, and awsResponse fields) is included in the log messages for this pipe. This applies to all log destinations for the pipe. Valid values ALL.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "includeExecutionData")]
    pub include_execution_data: Option<Vec<String>>,
    /// The level of logging detail to include. Valid values OFF, ERROR, INFO and TRACE.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub level: Option<String>,
    /// Amazon S3 logging configuration settings for the pipe. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "s3LogDestination")]
    pub s3_log_destination: Option<PipeInitProviderLogConfigurationS3LogDestination>,
}

/// Amazon CloudWatch Logs logging configuration settings for the pipe. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderLogConfigurationCloudwatchLogsLogDestination {
    /// Amazon Web Services Resource Name (ARN) for the CloudWatch log group to which EventBridge sends the log records.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "logGroupArn")]
    pub log_group_arn: Option<String>,
    /// Reference to a Group in cloudwatchlogs to populate logGroupArn.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "logGroupArnRef")]
    pub log_group_arn_ref: Option<PipeInitProviderLogConfigurationCloudwatchLogsLogDestinationLogGroupArnRef>,
    /// Selector for a Group in cloudwatchlogs to populate logGroupArn.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "logGroupArnSelector")]
    pub log_group_arn_selector: Option<PipeInitProviderLogConfigurationCloudwatchLogsLogDestinationLogGroupArnSelector>,
}

/// Reference to a Group in cloudwatchlogs to populate logGroupArn.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderLogConfigurationCloudwatchLogsLogDestinationLogGroupArnRef {
    /// Name of the referenced object.
    pub name: String,
    /// Policies for referencing.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub policy: Option<PipeInitProviderLogConfigurationCloudwatchLogsLogDestinationLogGroupArnRefPolicy>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderLogConfigurationCloudwatchLogsLogDestinationLogGroupArnRefPolicy {
    /// Resolution specifies whether resolution of this reference is required.
    /// The default is 'Required', which means the reconcile will fail if the
    /// reference cannot be resolved. 'Optional' means this reference will be
    /// a no-op if it cannot be resolved.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolution: Option<PipeInitProviderLogConfigurationCloudwatchLogsLogDestinationLogGroupArnRefPolicyResolution>,
    /// Resolve specifies when this reference should be resolved. The default
    /// is 'IfNotPresent', which will attempt to resolve the reference only when
    /// the corresponding field is not present. Use 'Always' to resolve the
    /// reference on every reconcile.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolve: Option<PipeInitProviderLogConfigurationCloudwatchLogsLogDestinationLogGroupArnRefPolicyResolve>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum PipeInitProviderLogConfigurationCloudwatchLogsLogDestinationLogGroupArnRefPolicyResolution {
    Required,
    Optional,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum PipeInitProviderLogConfigurationCloudwatchLogsLogDestinationLogGroupArnRefPolicyResolve {
    Always,
    IfNotPresent,
}

/// Selector for a Group in cloudwatchlogs to populate logGroupArn.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderLogConfigurationCloudwatchLogsLogDestinationLogGroupArnSelector {
    /// MatchControllerRef ensures an object with the same controller reference
    /// as the selecting object is selected.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchControllerRef")]
    pub match_controller_ref: Option<bool>,
    /// MatchLabels ensures an object with matching labels is selected.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<HashMap<String, String>>,
    /// Policies for selection.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub policy: Option<PipeInitProviderLogConfigurationCloudwatchLogsLogDestinationLogGroupArnSelectorPolicy>,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderLogConfigurationCloudwatchLogsLogDestinationLogGroupArnSelectorPolicy {
    /// Resolution specifies whether resolution of this reference is required.
    /// The default is 'Required', which means the reconcile will fail if the
    /// reference cannot be resolved. 'Optional' means this reference will be
    /// a no-op if it cannot be resolved.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolution: Option<PipeInitProviderLogConfigurationCloudwatchLogsLogDestinationLogGroupArnSelectorPolicyResolution>,
    /// Resolve specifies when this reference should be resolved. The default
    /// is 'IfNotPresent', which will attempt to resolve the reference only when
    /// the corresponding field is not present. Use 'Always' to resolve the
    /// reference on every reconcile.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolve: Option<PipeInitProviderLogConfigurationCloudwatchLogsLogDestinationLogGroupArnSelectorPolicyResolve>,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum PipeInitProviderLogConfigurationCloudwatchLogsLogDestinationLogGroupArnSelectorPolicyResolution {
    Required,
    Optional,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum PipeInitProviderLogConfigurationCloudwatchLogsLogDestinationLogGroupArnSelectorPolicyResolve {
    Always,
    IfNotPresent,
}

/// Amazon Kinesis Data Firehose logging configuration settings for the pipe. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderLogConfigurationFirehoseLogDestination {
    /// Amazon Resource Name (ARN) of the Kinesis Data Firehose delivery stream to which EventBridge delivers the pipe log records.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "deliveryStreamArn")]
    pub delivery_stream_arn: Option<String>,
}

/// Amazon S3 logging configuration settings for the pipe. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderLogConfigurationS3LogDestination {
    /// Name of the Amazon S3 bucket to which EventBridge delivers the log records for the pipe.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketName")]
    pub bucket_name: Option<String>,
    /// Amazon Web Services account that owns the Amazon S3 bucket to which EventBridge delivers the log records for the pipe.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketOwner")]
    pub bucket_owner: Option<String>,
    /// EventBridge format for the log records. Valid values json, plain and w3c.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "outputFormat")]
    pub output_format: Option<String>,
    /// Prefix text with which to begin Amazon S3 log object names.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub prefix: Option<String>,
}

/// Reference to a Role in iam to populate roleArn.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderRoleArnRef {
    /// Name of the referenced object.
    pub name: String,
    /// Policies for referencing.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub policy: Option<PipeInitProviderRoleArnRefPolicy>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderRoleArnRefPolicy {
    /// Resolution specifies whether resolution of this reference is required.
    /// The default is 'Required', which means the reconcile will fail if the
    /// reference cannot be resolved. 'Optional' means this reference will be
    /// a no-op if it cannot be resolved.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolution: Option<PipeInitProviderRoleArnRefPolicyResolution>,
    /// Resolve specifies when this reference should be resolved. The default
    /// is 'IfNotPresent', which will attempt to resolve the reference only when
    /// the corresponding field is not present. Use 'Always' to resolve the
    /// reference on every reconcile.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolve: Option<PipeInitProviderRoleArnRefPolicyResolve>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum PipeInitProviderRoleArnRefPolicyResolution {
    Required,
    Optional,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum PipeInitProviderRoleArnRefPolicyResolve {
    Always,
    IfNotPresent,
}

/// Selector for a Role in iam to populate roleArn.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderRoleArnSelector {
    /// MatchControllerRef ensures an object with the same controller reference
    /// as the selecting object is selected.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchControllerRef")]
    pub match_controller_ref: Option<bool>,
    /// MatchLabels ensures an object with matching labels is selected.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<HashMap<String, String>>,
    /// Policies for selection.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub policy: Option<PipeInitProviderRoleArnSelectorPolicy>,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderRoleArnSelectorPolicy {
    /// Resolution specifies whether resolution of this reference is required.
    /// The default is 'Required', which means the reconcile will fail if the
    /// reference cannot be resolved. 'Optional' means this reference will be
    /// a no-op if it cannot be resolved.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolution: Option<PipeInitProviderRoleArnSelectorPolicyResolution>,
    /// Resolve specifies when this reference should be resolved. The default
    /// is 'IfNotPresent', which will attempt to resolve the reference only when
    /// the corresponding field is not present. Use 'Always' to resolve the
    /// reference on every reconcile.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolve: Option<PipeInitProviderRoleArnSelectorPolicyResolve>,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum PipeInitProviderRoleArnSelectorPolicyResolution {
    Required,
    Optional,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum PipeInitProviderRoleArnSelectorPolicyResolve {
    Always,
    IfNotPresent,
}

/// Parameters to configure a source for the pipe. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderSourceParameters {
    /// The parameters for using an Active MQ broker as a source. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "activemqBrokerParameters")]
    pub activemq_broker_parameters: Option<PipeInitProviderSourceParametersActivemqBrokerParameters>,
    /// The parameters for using a DynamoDB stream as a source.  Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dynamodbStreamParameters")]
    pub dynamodb_stream_parameters: Option<PipeInitProviderSourceParametersDynamodbStreamParameters>,
    /// The collection of event patterns used to filter events. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "filterCriteria")]
    pub filter_criteria: Option<PipeInitProviderSourceParametersFilterCriteria>,
    /// The parameters for using a Kinesis stream as a source. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kinesisStreamParameters")]
    pub kinesis_stream_parameters: Option<PipeInitProviderSourceParametersKinesisStreamParameters>,
    /// The parameters for using an MSK stream as a source. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "managedStreamingKafkaParameters")]
    pub managed_streaming_kafka_parameters: Option<PipeInitProviderSourceParametersManagedStreamingKafkaParameters>,
    /// The parameters for using a Rabbit MQ broker as a source. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "rabbitmqBrokerParameters")]
    pub rabbitmq_broker_parameters: Option<PipeInitProviderSourceParametersRabbitmqBrokerParameters>,
    /// The parameters for using a self-managed Apache Kafka stream as a source. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "selfManagedKafkaParameters")]
    pub self_managed_kafka_parameters: Option<PipeInitProviderSourceParametersSelfManagedKafkaParameters>,
    /// The parameters for using a Amazon SQS stream as a source. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sqsQueueParameters")]
    pub sqs_queue_parameters: Option<PipeInitProviderSourceParametersSqsQueueParameters>,
}

/// The parameters for using an Active MQ broker as a source. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderSourceParametersActivemqBrokerParameters {
    /// The maximum number of records to include in each batch. Maximum value of 10000.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "batchSize")]
    pub batch_size: Option<f64>,
    /// The credentials needed to access the resource. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub credentials: Option<PipeInitProviderSourceParametersActivemqBrokerParametersCredentials>,
    /// The maximum length of a time to wait for events. Maximum value of 300.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maximumBatchingWindowInSeconds")]
    pub maximum_batching_window_in_seconds: Option<f64>,
    /// The name of the destination queue to consume. Maximum length of 1000.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queueName")]
    pub queue_name: Option<String>,
}

/// The credentials needed to access the resource. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderSourceParametersActivemqBrokerParametersCredentials {
    /// The ARN of the Secrets Manager secret containing the basic auth credentials.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "basicAuth")]
    pub basic_auth: Option<String>,
}

/// The parameters for using a DynamoDB stream as a source.  Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderSourceParametersDynamodbStreamParameters {
    /// The maximum number of records to include in each batch. Maximum value of 10000.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "batchSize")]
    pub batch_size: Option<f64>,
    /// Define the target queue to send dead-letter queue events to. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "deadLetterConfig")]
    pub dead_letter_config: Option<PipeInitProviderSourceParametersDynamodbStreamParametersDeadLetterConfig>,
    /// The maximum length of a time to wait for events. Maximum value of 300.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maximumBatchingWindowInSeconds")]
    pub maximum_batching_window_in_seconds: Option<f64>,
    /// Discard records older than the specified age. The default value is -1, which sets the maximum age to infinite. When the value is set to infinite, EventBridge never discards old records. Maximum value of 604,800.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maximumRecordAgeInSeconds")]
    pub maximum_record_age_in_seconds: Option<f64>,
    /// Discard records after the specified number of retries. The default value is -1, which sets the maximum number of retries to infinite. When MaximumRetryAttempts is infinite, EventBridge retries failed records until the record expires in the event source. Maximum value of 10,000.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maximumRetryAttempts")]
    pub maximum_retry_attempts: Option<f64>,
    /// Define how to handle item process failures. AUTOMATIC_BISECT halves each batch and retry each half until all the records are processed or there is one failed message left in the batch. Valid values: AUTOMATIC_BISECT.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "onPartialBatchItemFailure")]
    pub on_partial_batch_item_failure: Option<String>,
    /// The number of batches to process concurrently from each shard. The default value is 1. Maximum value of 10.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "parallelizationFactor")]
    pub parallelization_factor: Option<f64>,
    /// The position in a stream from which to start reading. Valid values: TRIM_HORIZON, LATEST.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "startingPosition")]
    pub starting_position: Option<String>,
}

/// Define the target queue to send dead-letter queue events to. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderSourceParametersDynamodbStreamParametersDeadLetterConfig {
    /// The ARN of the Amazon SQS queue specified as the target for the dead-letter queue.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub arn: Option<String>,
}

/// The collection of event patterns used to filter events. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderSourceParametersFilterCriteria {
    /// An array of up to 5 event patterns. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub filter: Option<Vec<PipeInitProviderSourceParametersFilterCriteriaFilter>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderSourceParametersFilterCriteriaFilter {
    /// The event pattern. At most 4096 characters.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub pattern: Option<String>,
}

/// The parameters for using a Kinesis stream as a source. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderSourceParametersKinesisStreamParameters {
    /// The maximum number of records to include in each batch. Maximum value of 10000.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "batchSize")]
    pub batch_size: Option<f64>,
    /// Define the target queue to send dead-letter queue events to. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "deadLetterConfig")]
    pub dead_letter_config: Option<PipeInitProviderSourceParametersKinesisStreamParametersDeadLetterConfig>,
    /// The maximum length of a time to wait for events. Maximum value of 300.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maximumBatchingWindowInSeconds")]
    pub maximum_batching_window_in_seconds: Option<f64>,
    /// Discard records older than the specified age. The default value is -1, which sets the maximum age to infinite. When the value is set to infinite, EventBridge never discards old records. Maximum value of 604,800.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maximumRecordAgeInSeconds")]
    pub maximum_record_age_in_seconds: Option<f64>,
    /// Discard records after the specified number of retries. The default value is -1, which sets the maximum number of retries to infinite. When MaximumRetryAttempts is infinite, EventBridge retries failed records until the record expires in the event source. Maximum value of 10,000.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maximumRetryAttempts")]
    pub maximum_retry_attempts: Option<f64>,
    /// Define how to handle item process failures. AUTOMATIC_BISECT halves each batch and retry each half until all the records are processed or there is one failed message left in the batch. Valid values: AUTOMATIC_BISECT.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "onPartialBatchItemFailure")]
    pub on_partial_batch_item_failure: Option<String>,
    /// The number of batches to process concurrently from each shard. The default value is 1. Maximum value of 10.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "parallelizationFactor")]
    pub parallelization_factor: Option<f64>,
    /// The position in a stream from which to start reading. Valid values: TRIM_HORIZON, LATEST, AT_TIMESTAMP.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "startingPosition")]
    pub starting_position: Option<String>,
    /// With StartingPosition set to AT_TIMESTAMP, the time from which to start reading, in Unix time seconds.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "startingPositionTimestamp")]
    pub starting_position_timestamp: Option<String>,
}

/// Define the target queue to send dead-letter queue events to. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderSourceParametersKinesisStreamParametersDeadLetterConfig {
    /// The ARN of the Amazon SQS queue specified as the target for the dead-letter queue.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub arn: Option<String>,
}

/// The parameters for using an MSK stream as a source. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderSourceParametersManagedStreamingKafkaParameters {
    /// The maximum number of records to include in each batch. Maximum value of 10000.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "batchSize")]
    pub batch_size: Option<f64>,
    /// The name of the destination queue to consume. Maximum value of 200.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "consumerGroupId")]
    pub consumer_group_id: Option<String>,
    /// The credentials needed to access the resource. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub credentials: Option<PipeInitProviderSourceParametersManagedStreamingKafkaParametersCredentials>,
    /// The maximum length of a time to wait for events. Maximum value of 300.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maximumBatchingWindowInSeconds")]
    pub maximum_batching_window_in_seconds: Option<f64>,
    /// The position in a stream from which to start reading. Valid values: TRIM_HORIZON, LATEST.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "startingPosition")]
    pub starting_position: Option<String>,
    /// The name of the topic that the pipe will read from. Maximum length of 249.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "topicName")]
    pub topic_name: Option<String>,
}

/// The credentials needed to access the resource. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderSourceParametersManagedStreamingKafkaParametersCredentials {
    /// The ARN of the Secrets Manager secret containing the credentials.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clientCertificateTlsAuth")]
    pub client_certificate_tls_auth: Option<String>,
    /// The ARN of the Secrets Manager secret containing the credentials.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "saslScram512Auth")]
    pub sasl_scram512_auth: Option<String>,
}

/// The parameters for using a Rabbit MQ broker as a source. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderSourceParametersRabbitmqBrokerParameters {
    /// The maximum number of records to include in each batch. Maximum value of 10000.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "batchSize")]
    pub batch_size: Option<f64>,
    /// The credentials needed to access the resource. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub credentials: Option<PipeInitProviderSourceParametersRabbitmqBrokerParametersCredentials>,
    /// The maximum length of a time to wait for events. Maximum value of 300.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maximumBatchingWindowInSeconds")]
    pub maximum_batching_window_in_seconds: Option<f64>,
    /// The name of the destination queue to consume. Maximum length of 1000.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queueName")]
    pub queue_name: Option<String>,
    /// The name of the virtual host associated with the source broker. Maximum length of 200.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "virtualHost")]
    pub virtual_host: Option<String>,
}

/// The credentials needed to access the resource. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderSourceParametersRabbitmqBrokerParametersCredentials {
    /// The ARN of the Secrets Manager secret containing the credentials.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "basicAuth")]
    pub basic_auth: Option<String>,
}

/// The parameters for using a self-managed Apache Kafka stream as a source. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderSourceParametersSelfManagedKafkaParameters {
    /// An array of server URLs. Maximum number of 2 items, each of maximum length 300.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "additionalBootstrapServers")]
    pub additional_bootstrap_servers: Option<Vec<String>>,
    /// The maximum number of records to include in each batch. Maximum value of 10000.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "batchSize")]
    pub batch_size: Option<f64>,
    /// The name of the destination queue to consume. Maximum value of 200.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "consumerGroupId")]
    pub consumer_group_id: Option<String>,
    /// The credentials needed to access the resource. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub credentials: Option<PipeInitProviderSourceParametersSelfManagedKafkaParametersCredentials>,
    /// The maximum length of a time to wait for events. Maximum value of 300.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maximumBatchingWindowInSeconds")]
    pub maximum_batching_window_in_seconds: Option<f64>,
    /// The ARN of the Secrets Manager secret used for certification.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serverRootCaCertificate")]
    pub server_root_ca_certificate: Option<String>,
    /// The position in a stream from which to start reading. Valid values: TRIM_HORIZON, LATEST.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "startingPosition")]
    pub starting_position: Option<String>,
    /// The name of the topic that the pipe will read from. Maximum length of 249.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "topicName")]
    pub topic_name: Option<String>,
    /// This structure specifies the VPC subnets and security groups for the stream, and whether a public IP address is to be used. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub vpc: Option<PipeInitProviderSourceParametersSelfManagedKafkaParametersVpc>,
}

/// The credentials needed to access the resource. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderSourceParametersSelfManagedKafkaParametersCredentials {
    /// The ARN of the Secrets Manager secret containing the credentials.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "basicAuth")]
    pub basic_auth: Option<String>,
    /// The ARN of the Secrets Manager secret containing the credentials.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clientCertificateTlsAuth")]
    pub client_certificate_tls_auth: Option<String>,
    /// The ARN of the Secrets Manager secret containing the credentials.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "saslScram256Auth")]
    pub sasl_scram256_auth: Option<String>,
    /// The ARN of the Secrets Manager secret containing the credentials.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "saslScram512Auth")]
    pub sasl_scram512_auth: Option<String>,
}

/// This structure specifies the VPC subnets and security groups for the stream, and whether a public IP address is to be used. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderSourceParametersSelfManagedKafkaParametersVpc {
    /// List of security groups associated with the stream. These security groups must all be in the same VPC. You can specify as many as five security groups. If you do not specify a security group, the default security group for the VPC is used.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "securityGroups")]
    pub security_groups: Option<Vec<String>>,
    /// List of the subnets associated with the stream. These subnets must all be in the same VPC. You can specify as many as 16 subnets.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub subnets: Option<Vec<String>>,
}

/// The parameters for using a Amazon SQS stream as a source. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderSourceParametersSqsQueueParameters {
    /// The maximum number of records to include in each batch. Maximum value of 10000.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "batchSize")]
    pub batch_size: Option<f64>,
    /// The maximum length of a time to wait for events. Maximum value of 300.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maximumBatchingWindowInSeconds")]
    pub maximum_batching_window_in_seconds: Option<f64>,
}

/// Reference to a Queue in sqs to populate source.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderSourceRef {
    /// Name of the referenced object.
    pub name: String,
    /// Policies for referencing.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub policy: Option<PipeInitProviderSourceRefPolicy>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderSourceRefPolicy {
    /// Resolution specifies whether resolution of this reference is required.
    /// The default is 'Required', which means the reconcile will fail if the
    /// reference cannot be resolved. 'Optional' means this reference will be
    /// a no-op if it cannot be resolved.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolution: Option<PipeInitProviderSourceRefPolicyResolution>,
    /// Resolve specifies when this reference should be resolved. The default
    /// is 'IfNotPresent', which will attempt to resolve the reference only when
    /// the corresponding field is not present. Use 'Always' to resolve the
    /// reference on every reconcile.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolve: Option<PipeInitProviderSourceRefPolicyResolve>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum PipeInitProviderSourceRefPolicyResolution {
    Required,
    Optional,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum PipeInitProviderSourceRefPolicyResolve {
    Always,
    IfNotPresent,
}

/// Selector for a Queue in sqs to populate source.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderSourceSelector {
    /// MatchControllerRef ensures an object with the same controller reference
    /// as the selecting object is selected.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchControllerRef")]
    pub match_controller_ref: Option<bool>,
    /// MatchLabels ensures an object with matching labels is selected.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<HashMap<String, String>>,
    /// Policies for selection.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub policy: Option<PipeInitProviderSourceSelectorPolicy>,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderSourceSelectorPolicy {
    /// Resolution specifies whether resolution of this reference is required.
    /// The default is 'Required', which means the reconcile will fail if the
    /// reference cannot be resolved. 'Optional' means this reference will be
    /// a no-op if it cannot be resolved.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolution: Option<PipeInitProviderSourceSelectorPolicyResolution>,
    /// Resolve specifies when this reference should be resolved. The default
    /// is 'IfNotPresent', which will attempt to resolve the reference only when
    /// the corresponding field is not present. Use 'Always' to resolve the
    /// reference on every reconcile.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolve: Option<PipeInitProviderSourceSelectorPolicyResolve>,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum PipeInitProviderSourceSelectorPolicyResolution {
    Required,
    Optional,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum PipeInitProviderSourceSelectorPolicyResolve {
    Always,
    IfNotPresent,
}

/// Parameters to configure a target for your pipe. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderTargetParameters {
    /// The parameters for using an AWS Batch job as a target. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "batchJobParameters")]
    pub batch_job_parameters: Option<PipeInitProviderTargetParametersBatchJobParameters>,
    /// The parameters for using an CloudWatch Logs log stream as a target. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "cloudwatchLogsParameters")]
    pub cloudwatch_logs_parameters: Option<PipeInitProviderTargetParametersCloudwatchLogsParameters>,
    /// The parameters for using an Amazon ECS task as a target. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "ecsTaskParameters")]
    pub ecs_task_parameters: Option<PipeInitProviderTargetParametersEcsTaskParameters>,
    /// The parameters for using an EventBridge event bus as a target. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "eventbridgeEventBusParameters")]
    pub eventbridge_event_bus_parameters: Option<PipeInitProviderTargetParametersEventbridgeEventBusParameters>,
    /// These are custom parameter to be used when the target is an API Gateway REST APIs or EventBridge ApiDestinations. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpParameters")]
    pub http_parameters: Option<PipeInitProviderTargetParametersHttpParameters>,
    /// Valid JSON text passed to the target. In this case, nothing from the event itself is passed to the target. Maximum length of 8192 characters.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "inputTemplate")]
    pub input_template: Option<String>,
    /// The parameters for using a Kinesis stream as a source. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kinesisStreamParameters")]
    pub kinesis_stream_parameters: Option<PipeInitProviderTargetParametersKinesisStreamParameters>,
    /// The parameters for using a Lambda function as a target. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "lambdaFunctionParameters")]
    pub lambda_function_parameters: Option<PipeInitProviderTargetParametersLambdaFunctionParameters>,
    /// These are custom parameters to be used when the target is a Amazon Redshift cluster to invoke the Amazon Redshift Data API BatchExecuteStatement. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "redshiftDataParameters")]
    pub redshift_data_parameters: Option<PipeInitProviderTargetParametersRedshiftDataParameters>,
    /// The parameters for using a SageMaker pipeline as a target. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sagemakerPipelineParameters")]
    pub sagemaker_pipeline_parameters: Option<PipeInitProviderTargetParametersSagemakerPipelineParameters>,
    /// The parameters for using a Amazon SQS stream as a target. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sqsQueueParameters")]
    pub sqs_queue_parameters: Option<PipeInitProviderTargetParametersSqsQueueParameters>,
    /// The parameters for using a Step Functions state machine as a target. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "stepFunctionStateMachineParameters")]
    pub step_function_state_machine_parameters: Option<PipeInitProviderTargetParametersStepFunctionStateMachineParameters>,
}

/// The parameters for using an AWS Batch job as a target. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderTargetParametersBatchJobParameters {
    /// The array properties for the submitted job, such as the size of the array. The array size can be between 2 and 10,000. If you specify array properties for a job, it becomes an array job. This parameter is used only if the target is an AWS Batch job. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "arrayProperties")]
    pub array_properties: Option<PipeInitProviderTargetParametersBatchJobParametersArrayProperties>,
    /// The overrides that are sent to a container. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "containerOverrides")]
    pub container_overrides: Option<PipeInitProviderTargetParametersBatchJobParametersContainerOverrides>,
    /// A list of dependencies for the job. A job can depend upon a maximum of 20 jobs. You can specify a SEQUENTIAL type dependency without specifying a job ID for array jobs so that each child array job completes sequentially, starting at index 0. You can also specify an N_TO_N type dependency with a job ID for array jobs. In that case, each index child of this job must wait for the corresponding index child of each dependency to complete before it can begin. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dependsOn")]
    pub depends_on: Option<Vec<PipeInitProviderTargetParametersBatchJobParametersDependsOn>>,
    /// The job definition used by this job. This value can be one of name, name:revision, or the Amazon Resource Name (ARN) for the job definition. If name is specified without a revision then the latest active revision is used.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jobDefinition")]
    pub job_definition: Option<String>,
    /// The name of the job. It can be up to 128 letters long.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jobName")]
    pub job_name: Option<String>,
    /// Additional parameters passed to the job that replace parameter substitution placeholders that are set in the job definition. Parameters are specified as a key and value pair mapping. Parameters included here override any corresponding parameter defaults from the job definition. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub parameters: Option<HashMap<String, String>>,
    /// The retry strategy to use for failed jobs. When a retry strategy is specified here, it overrides the retry strategy defined in the job definition. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "retryStrategy")]
    pub retry_strategy: Option<PipeInitProviderTargetParametersBatchJobParametersRetryStrategy>,
}

/// The array properties for the submitted job, such as the size of the array. The array size can be between 2 and 10,000. If you specify array properties for a job, it becomes an array job. This parameter is used only if the target is an AWS Batch job. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderTargetParametersBatchJobParametersArrayProperties {
    /// The size of the array, if this is an array batch job. Minimum value of 2. Maximum value of 10,000.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub size: Option<f64>,
}

/// The overrides that are sent to a container. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderTargetParametersBatchJobParametersContainerOverrides {
    /// List of commands to send to the container that overrides the default command from the Docker image or the task definition.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
    /// The environment variables to send to the container. You can add new environment variables, which are added to the container at launch, or you can override the existing environment variables from the Docker image or the task definition. Environment variables cannot start with " AWS Batch ". This naming convention is reserved for variables that AWS Batch sets. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub environment: Option<Vec<PipeInitProviderTargetParametersBatchJobParametersContainerOverridesEnvironment>>,
    /// The instance type to use for a multi-node parallel job. This parameter isn't applicable to single-node container jobs or jobs that run on Fargate resources, and shouldn't be provided.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "instanceType")]
    pub instance_type: Option<String>,
    /// The type and amount of resources to assign to a container. This overrides the settings in the job definition. The supported resources include GPU, MEMORY, and VCPU. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "resourceRequirement")]
    pub resource_requirement: Option<Vec<PipeInitProviderTargetParametersBatchJobParametersContainerOverridesResourceRequirement>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderTargetParametersBatchJobParametersContainerOverridesEnvironment {
    /// The name of the key-value pair. For environment variables, this is the name of the environment variable.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// The value of the key-value pair. For environment variables, this is the value of the environment variable.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderTargetParametersBatchJobParametersContainerOverridesResourceRequirement {
    /// The type of resource to assign to a container. The supported resources include GPU, MEMORY, and VCPU.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<String>,
    /// The quantity of the specified resource to reserve for the container. The values vary based on the type specified.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderTargetParametersBatchJobParametersDependsOn {
    /// The job ID of the AWS Batch job that's associated with this dependency.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jobId")]
    pub job_id: Option<String>,
    /// The type of the job dependency. Valid Values: N_TO_N, SEQUENTIAL.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<String>,
}

/// The retry strategy to use for failed jobs. When a retry strategy is specified here, it overrides the retry strategy defined in the job definition. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderTargetParametersBatchJobParametersRetryStrategy {
    /// The number of times to move a job to the RUNNABLE status. If the value of attempts is greater than one, the job is retried on failure the same number of attempts as the value. Maximum value of 10.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub attempts: Option<f64>,
}

/// The parameters for using an CloudWatch Logs log stream as a target. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderTargetParametersCloudwatchLogsParameters {
    /// The name of the log stream.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "logStreamName")]
    pub log_stream_name: Option<String>,
    /// The time the event occurred, expressed as the number of milliseconds after Jan 1, 1970 00:00:00 UTC. This is the JSON path to the field in the event e.g. $.detail.timestamp
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub timestamp: Option<String>,
}

/// The parameters for using an Amazon ECS task as a target. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderTargetParametersEcsTaskParameters {
    /// List of capacity provider strategies to use for the task. If a capacityProviderStrategy is specified, the launchType parameter must be omitted. If no capacityProviderStrategy or launchType is specified, the defaultCapacityProviderStrategy for the cluster is used. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "capacityProviderStrategy")]
    pub capacity_provider_strategy: Option<Vec<PipeInitProviderTargetParametersEcsTaskParametersCapacityProviderStrategy>>,
    /// Specifies whether to enable Amazon ECS managed tags for the task. Valid values: true, false.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableEcsManagedTags")]
    pub enable_ecs_managed_tags: Option<bool>,
    /// Whether or not to enable the execute command functionality for the containers in this task. If true, this enables execute command functionality on all containers in the task. Valid values: true, false.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableExecuteCommand")]
    pub enable_execute_command: Option<bool>,
    /// Specifies an Amazon ECS task group for the task. The maximum length is 255 characters.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub group: Option<String>,
    /// Specifies the launch type on which your task is running. The launch type that you specify here must match one of the launch type (compatibilities) of the target task. The FARGATE value is supported only in the Regions where AWS Fargate with Amazon ECS is supported. Valid Values: EC2, FARGATE, EXTERNAL
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "launchType")]
    pub launch_type: Option<String>,
    /// Use this structure if the Amazon ECS task uses the awsvpc network mode. This structure specifies the VPC subnets and security groups associated with the task, and whether a public IP address is to be used. This structure is required if LaunchType is FARGATE because the awsvpc mode is required for Fargate tasks. If you specify NetworkConfiguration when the target ECS task does not use the awsvpc network mode, the task fails. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "networkConfiguration")]
    pub network_configuration: Option<PipeInitProviderTargetParametersEcsTaskParametersNetworkConfiguration>,
    /// The overrides that are associated with a task. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub overrides: Option<PipeInitProviderTargetParametersEcsTaskParametersOverrides>,
    /// An array of placement constraint objects to use for the task. You can specify up to 10 constraints per task (including constraints in the task definition and those specified at runtime). Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "placementConstraint")]
    pub placement_constraint: Option<Vec<PipeInitProviderTargetParametersEcsTaskParametersPlacementConstraint>>,
    /// The placement strategy objects to use for the task. You can specify a maximum of five strategy rules per task. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "placementStrategy")]
    pub placement_strategy: Option<Vec<PipeInitProviderTargetParametersEcsTaskParametersPlacementStrategy>>,
    /// Specifies the platform version for the task. Specify only the numeric portion of the platform version, such as 1.1.0. This structure is used only if LaunchType is FARGATE.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "platformVersion")]
    pub platform_version: Option<String>,
    /// Specifies whether to propagate the tags from the task definition to the task. If no value is specified, the tags are not propagated. Tags can only be propagated to the task during task creation. To add tags to a task after task creation, use the TagResource API action. Valid Values: TASK_DEFINITION
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "propagateTags")]
    pub propagate_tags: Option<String>,
    /// The reference ID to use for the task. Maximum length of 1,024.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "referenceId")]
    pub reference_id: Option<String>,
    /// Key-value map of tags that you apply to the task to help you categorize and organize them.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tags: Option<HashMap<String, String>>,
    /// The number of tasks to create based on TaskDefinition. The default is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "taskCount")]
    pub task_count: Option<f64>,
    /// The ARN of the task definition to use if the event target is an Amazon ECS task.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "taskDefinitionArn")]
    pub task_definition_arn: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderTargetParametersEcsTaskParametersCapacityProviderStrategy {
    /// The base value designates how many tasks, at a minimum, to run on the specified capacity provider. Only one capacity provider in a capacity provider strategy can have a base defined. If no value is specified, the default value of 0 is used. Maximum value of 100,000.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub base: Option<f64>,
    /// The short name of the capacity provider. Maximum value of 255.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "capacityProvider")]
    pub capacity_provider: Option<String>,
    /// The weight value designates the relative percentage of the total number of tasks launched that should use the specified capacity provider. The weight value is taken into consideration after the base value, if defined, is satisfied. Maximum value of 1,000.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub weight: Option<f64>,
}

/// Use this structure if the Amazon ECS task uses the awsvpc network mode. This structure specifies the VPC subnets and security groups associated with the task, and whether a public IP address is to be used. This structure is required if LaunchType is FARGATE because the awsvpc mode is required for Fargate tasks. If you specify NetworkConfiguration when the target ECS task does not use the awsvpc network mode, the task fails. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderTargetParametersEcsTaskParametersNetworkConfiguration {
    /// Use this structure to specify the VPC subnets and security groups for the task, and whether a public IP address is to be used. This structure is relevant only for ECS tasks that use the awsvpc network mode. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "awsVpcConfiguration")]
    pub aws_vpc_configuration: Option<PipeInitProviderTargetParametersEcsTaskParametersNetworkConfigurationAwsVpcConfiguration>,
}

/// Use this structure to specify the VPC subnets and security groups for the task, and whether a public IP address is to be used. This structure is relevant only for ECS tasks that use the awsvpc network mode. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderTargetParametersEcsTaskParametersNetworkConfigurationAwsVpcConfiguration {
    /// Specifies whether the task's elastic network interface receives a public IP address. You can specify ENABLED only when LaunchType in EcsParameters is set to FARGATE. Valid Values: ENABLED, DISABLED.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "assignPublicIp")]
    pub assign_public_ip: Option<String>,
    /// Specifies the security groups associated with the task. These security groups must all be in the same VPC. You can specify as many as five security groups. If you do not specify a security group, the default security group for the VPC is used.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "securityGroups")]
    pub security_groups: Option<Vec<String>>,
    /// Specifies the subnets associated with the task. These subnets must all be in the same VPC. You can specify as many as 16 subnets.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub subnets: Option<Vec<String>>,
}

/// The overrides that are associated with a task. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderTargetParametersEcsTaskParametersOverrides {
    /// One or more container overrides that are sent to a task. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "containerOverride")]
    pub container_override: Option<Vec<PipeInitProviderTargetParametersEcsTaskParametersOverridesContainerOverride>>,
    /// The cpu override for the task.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub cpu: Option<String>,
    /// The ephemeral storage setting override for the task.  Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "ephemeralStorage")]
    pub ephemeral_storage: Option<PipeInitProviderTargetParametersEcsTaskParametersOverridesEphemeralStorage>,
    /// The Amazon Resource Name (ARN) of the task execution IAM role override for the task.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "executionRoleArn")]
    pub execution_role_arn: Option<String>,
    /// List of Elastic Inference accelerator overrides for the task. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "inferenceAcceleratorOverride")]
    pub inference_accelerator_override: Option<Vec<PipeInitProviderTargetParametersEcsTaskParametersOverridesInferenceAcceleratorOverride>>,
    /// The memory override for the task.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub memory: Option<String>,
    /// The Amazon Resource Name (ARN) of the IAM role that containers in this task can assume. All containers in this task are granted the permissions that are specified in this role.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "taskRoleArn")]
    pub task_role_arn: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderTargetParametersEcsTaskParametersOverridesContainerOverride {
    /// List of commands to send to the container that overrides the default command from the Docker image or the task definition. You must also specify a container name.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
    /// The number of cpu units reserved for the container, instead of the default value from the task definition. You must also specify a container name.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub cpu: Option<f64>,
    /// The environment variables to send to the container. You can add new environment variables, which are added to the container at launch, or you can override the existing environment variables from the Docker image or the task definition. You must also specify a container name. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub environment: Option<Vec<PipeInitProviderTargetParametersEcsTaskParametersOverridesContainerOverrideEnvironment>>,
    /// A list of files containing the environment variables to pass to a container, instead of the value from the container definition. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "environmentFile")]
    pub environment_file: Option<Vec<PipeInitProviderTargetParametersEcsTaskParametersOverridesContainerOverrideEnvironmentFile>>,
    /// The hard limit (in MiB) of memory to present to the container, instead of the default value from the task definition. If your container attempts to exceed the memory specified here, the container is killed. You must also specify a container name.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub memory: Option<f64>,
    /// The soft limit (in MiB) of memory to reserve for the container, instead of the default value from the task definition. You must also specify a container name.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "memoryReservation")]
    pub memory_reservation: Option<f64>,
    /// The name of the container that receives the override. This parameter is required if any override is specified.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// The type and amount of a resource to assign to a container, instead of the default value from the task definition. The only supported resource is a GPU. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "resourceRequirement")]
    pub resource_requirement: Option<Vec<PipeInitProviderTargetParametersEcsTaskParametersOverridesContainerOverrideResourceRequirement>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderTargetParametersEcsTaskParametersOverridesContainerOverrideEnvironment {
    /// The name of the key-value pair. For environment variables, this is the name of the environment variable.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// The value of the key-value pair. For environment variables, this is the value of the environment variable.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderTargetParametersEcsTaskParametersOverridesContainerOverrideEnvironmentFile {
    /// The file type to use. The only supported value is s3.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<String>,
    /// The Amazon Resource Name (ARN) of the Amazon S3 object containing the environment variable file.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderTargetParametersEcsTaskParametersOverridesContainerOverrideResourceRequirement {
    /// The type of resource to assign to a container. The supported values are GPU or InferenceAccelerator.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<String>,
    /// The value for the specified resource type. If the GPU type is used, the value is the number of physical GPUs the Amazon ECS container agent reserves for the container. The number of GPUs that's reserved for all containers in a task can't exceed the number of available GPUs on the container instance that the task is launched on. If the InferenceAccelerator type is used, the value matches the deviceName for an InferenceAccelerator specified in a task definition.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
}

/// The ephemeral storage setting override for the task.  Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderTargetParametersEcsTaskParametersOverridesEphemeralStorage {
    /// The total amount, in GiB, of ephemeral storage to set for the task. The minimum supported value is 21 GiB and the maximum supported value is 200 GiB.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sizeInGib")]
    pub size_in_gib: Option<f64>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderTargetParametersEcsTaskParametersOverridesInferenceAcceleratorOverride {
    /// The Elastic Inference accelerator device name to override for the task. This parameter must match a deviceName specified in the task definition.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "deviceName")]
    pub device_name: Option<String>,
    /// The Elastic Inference accelerator type to use.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "deviceType")]
    pub device_type: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderTargetParametersEcsTaskParametersPlacementConstraint {
    /// A cluster query language expression to apply to the constraint. You cannot specify an expression if the constraint type is distinctInstance. Maximum length of 2,000.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub expression: Option<String>,
    /// The type of constraint. Use distinctInstance to ensure that each task in a particular group is running on a different container instance. Use memberOf to restrict the selection to a group of valid candidates. Valid Values: distinctInstance, memberOf.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderTargetParametersEcsTaskParametersPlacementStrategy {
    /// The field to apply the placement strategy against. For the spread placement strategy, valid values are instanceId (or host, which has the same effect), or any platform or custom attribute that is applied to a container instance, such as attribute:ecs.availability-zone. For the binpack placement strategy, valid values are cpu and memory. For the random placement strategy, this field is not used. Maximum length of 255.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub field: Option<String>,
    /// The type of placement strategy. The random placement strategy randomly places tasks on available candidates. The spread placement strategy spreads placement across available candidates evenly based on the field parameter. The binpack strategy places tasks on available candidates that have the least available amount of the resource that is specified with the field parameter. For example, if you binpack on memory, a task is placed on the instance with the least amount of remaining memory (but still enough to run the task). Valid Values: random, spread, binpack.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<String>,
}

/// The parameters for using an EventBridge event bus as a target. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderTargetParametersEventbridgeEventBusParameters {
    /// A free-form string, with a maximum of 128 characters, used to decide what fields to expect in the event detail.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "detailType")]
    pub detail_type: Option<String>,
    /// The URL subdomain of the endpoint. For example, if the URL for Endpoint is https://abcde.veo.endpoints.event.amazonaws.com, then the EndpointId is abcde.veo.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "endpointId")]
    pub endpoint_id: Option<String>,
    /// List of AWS resources, identified by Amazon Resource Name (ARN), which the event primarily concerns. Any number, including zero, may be present.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resources: Option<Vec<String>>,
    /// The source of the event. Maximum length of 256.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub source: Option<String>,
    /// The time stamp of the event, per RFC3339. If no time stamp is provided, the time stamp of the PutEvents call is used. This is the JSON path to the field in the event e.g. $.detail.timestamp
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub time: Option<String>,
}

/// These are custom parameter to be used when the target is an API Gateway REST APIs or EventBridge ApiDestinations. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderTargetParametersHttpParameters {
    /// Key-value mapping of the headers that need to be sent as part of request invoking the API Gateway REST API or EventBridge ApiDestination. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "headerParameters")]
    pub header_parameters: Option<HashMap<String, String>>,
    /// The path parameter values to be used to populate API Gateway REST API or EventBridge ApiDestination path wildcards ("*").
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "pathParameterValues")]
    pub path_parameter_values: Option<Vec<String>>,
    /// Key-value mapping of the query strings that need to be sent as part of request invoking the API Gateway REST API or EventBridge ApiDestination. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryStringParameters")]
    pub query_string_parameters: Option<HashMap<String, String>>,
}

/// The parameters for using a Kinesis stream as a source. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderTargetParametersKinesisStreamParameters {
    /// Determines which shard in the stream the data record is assigned to. Partition keys are Unicode strings with a maximum length limit of 256 characters for each key. Amazon Kinesis Data Streams uses the partition key as input to a hash function that maps the partition key and associated data to a specific shard. Specifically, an MD5 hash function is used to map partition keys to 128-bit integer values and to map associated data records to shards. As a result of this hashing mechanism, all data records with the same partition key map to the same shard within the stream.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "partitionKey")]
    pub partition_key: Option<String>,
}

/// The parameters for using a Lambda function as a target. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderTargetParametersLambdaFunctionParameters {
    /// Specify whether to invoke the function synchronously or asynchronously. Valid Values: REQUEST_RESPONSE, FIRE_AND_FORGET.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "invocationType")]
    pub invocation_type: Option<String>,
}

/// These are custom parameters to be used when the target is a Amazon Redshift cluster to invoke the Amazon Redshift Data API BatchExecuteStatement. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderTargetParametersRedshiftDataParameters {
    /// The name of the database. Required when authenticating using temporary credentials.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub database: Option<String>,
    /// The database user name. Required when authenticating using temporary credentials.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dbUser")]
    pub db_user: Option<String>,
    /// The name or ARN of the secret that enables access to the database. Required when authenticating using Secrets Manager.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretManagerArn")]
    pub secret_manager_arn: Option<String>,
    /// List of SQL statements text to run, each of maximum length of 100,000.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sqls: Option<Vec<String>>,
    /// The name of the SQL statement. You can name the SQL statement when you create it to identify the query.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "statementName")]
    pub statement_name: Option<String>,
    /// Indicates whether to send an event back to EventBridge after the SQL statement runs.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "withEvent")]
    pub with_event: Option<bool>,
}

/// The parameters for using a SageMaker pipeline as a target. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderTargetParametersSagemakerPipelineParameters {
    /// List of Parameter names and values for SageMaker Model Building Pipeline execution. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "pipelineParameter")]
    pub pipeline_parameter: Option<Vec<PipeInitProviderTargetParametersSagemakerPipelineParametersPipelineParameter>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderTargetParametersSagemakerPipelineParametersPipelineParameter {
    /// The name of the container that receives the override. This parameter is required if any override is specified.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Value of parameter to start execution of a SageMaker Model Building Pipeline. Maximum length of 1024.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
}

/// The parameters for using a Amazon SQS stream as a target. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderTargetParametersSqsQueueParameters {
    /// This parameter applies only to FIFO (first-in-first-out) queues. The token used for deduplication of sent messages.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "messageDeduplicationId")]
    pub message_deduplication_id: Option<String>,
    /// The FIFO message group ID to use as the target.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "messageGroupId")]
    pub message_group_id: Option<String>,
}

/// The parameters for using a Step Functions state machine as a target. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderTargetParametersStepFunctionStateMachineParameters {
    /// Specify whether to invoke the function synchronously or asynchronously. Valid Values: REQUEST_RESPONSE, FIRE_AND_FORGET.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "invocationType")]
    pub invocation_type: Option<String>,
}

/// Reference to a Queue in sqs to populate target.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderTargetRef {
    /// Name of the referenced object.
    pub name: String,
    /// Policies for referencing.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub policy: Option<PipeInitProviderTargetRefPolicy>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderTargetRefPolicy {
    /// Resolution specifies whether resolution of this reference is required.
    /// The default is 'Required', which means the reconcile will fail if the
    /// reference cannot be resolved. 'Optional' means this reference will be
    /// a no-op if it cannot be resolved.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolution: Option<PipeInitProviderTargetRefPolicyResolution>,
    /// Resolve specifies when this reference should be resolved. The default
    /// is 'IfNotPresent', which will attempt to resolve the reference only when
    /// the corresponding field is not present. Use 'Always' to resolve the
    /// reference on every reconcile.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolve: Option<PipeInitProviderTargetRefPolicyResolve>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum PipeInitProviderTargetRefPolicyResolution {
    Required,
    Optional,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum PipeInitProviderTargetRefPolicyResolve {
    Always,
    IfNotPresent,
}

/// Selector for a Queue in sqs to populate target.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderTargetSelector {
    /// MatchControllerRef ensures an object with the same controller reference
    /// as the selecting object is selected.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchControllerRef")]
    pub match_controller_ref: Option<bool>,
    /// MatchLabels ensures an object with matching labels is selected.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<HashMap<String, String>>,
    /// Policies for selection.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub policy: Option<PipeInitProviderTargetSelectorPolicy>,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeInitProviderTargetSelectorPolicy {
    /// Resolution specifies whether resolution of this reference is required.
    /// The default is 'Required', which means the reconcile will fail if the
    /// reference cannot be resolved. 'Optional' means this reference will be
    /// a no-op if it cannot be resolved.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolution: Option<PipeInitProviderTargetSelectorPolicyResolution>,
    /// Resolve specifies when this reference should be resolved. The default
    /// is 'IfNotPresent', which will attempt to resolve the reference only when
    /// the corresponding field is not present. Use 'Always' to resolve the
    /// reference on every reconcile.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolve: Option<PipeInitProviderTargetSelectorPolicyResolve>,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum PipeInitProviderTargetSelectorPolicyResolution {
    Required,
    Optional,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum PipeInitProviderTargetSelectorPolicyResolve {
    Always,
    IfNotPresent,
}

/// ProviderConfigReference specifies how the provider that will be used to
/// create, observe, update, and delete this managed resource should be
/// configured.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeProviderConfigRef {
    /// Name of the referenced object.
    pub name: String,
    /// Policies for referencing.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub policy: Option<PipeProviderConfigRefPolicy>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeProviderConfigRefPolicy {
    /// Resolution specifies whether resolution of this reference is required.
    /// The default is 'Required', which means the reconcile will fail if the
    /// reference cannot be resolved. 'Optional' means this reference will be
    /// a no-op if it cannot be resolved.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolution: Option<PipeProviderConfigRefPolicyResolution>,
    /// Resolve specifies when this reference should be resolved. The default
    /// is 'IfNotPresent', which will attempt to resolve the reference only when
    /// the corresponding field is not present. Use 'Always' to resolve the
    /// reference on every reconcile.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolve: Option<PipeProviderConfigRefPolicyResolve>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum PipeProviderConfigRefPolicyResolution {
    Required,
    Optional,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum PipeProviderConfigRefPolicyResolve {
    Always,
    IfNotPresent,
}

/// PublishConnectionDetailsTo specifies the connection secret config which
/// contains a name, metadata and a reference to secret store config to
/// which any connection details for this managed resource should be written.
/// Connection details frequently include the endpoint, username,
/// and password required to connect to the managed resource.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipePublishConnectionDetailsTo {
    /// SecretStoreConfigRef specifies which secret store config should be used
    /// for this ConnectionSecret.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configRef")]
    pub config_ref: Option<PipePublishConnectionDetailsToConfigRef>,
    /// Metadata is the metadata for connection secret.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub metadata: Option<PipePublishConnectionDetailsToMetadata>,
    /// Name is the name of the connection secret.
    pub name: String,
}

/// SecretStoreConfigRef specifies which secret store config should be used
/// for this ConnectionSecret.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipePublishConnectionDetailsToConfigRef {
    /// Name of the referenced object.
    pub name: String,
    /// Policies for referencing.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub policy: Option<PipePublishConnectionDetailsToConfigRefPolicy>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipePublishConnectionDetailsToConfigRefPolicy {
    /// Resolution specifies whether resolution of this reference is required.
    /// The default is 'Required', which means the reconcile will fail if the
    /// reference cannot be resolved. 'Optional' means this reference will be
    /// a no-op if it cannot be resolved.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolution: Option<PipePublishConnectionDetailsToConfigRefPolicyResolution>,
    /// Resolve specifies when this reference should be resolved. The default
    /// is 'IfNotPresent', which will attempt to resolve the reference only when
    /// the corresponding field is not present. Use 'Always' to resolve the
    /// reference on every reconcile.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolve: Option<PipePublishConnectionDetailsToConfigRefPolicyResolve>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum PipePublishConnectionDetailsToConfigRefPolicyResolution {
    Required,
    Optional,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum PipePublishConnectionDetailsToConfigRefPolicyResolve {
    Always,
    IfNotPresent,
}

/// Metadata is the metadata for connection secret.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipePublishConnectionDetailsToMetadata {
    /// Annotations are the annotations to be added to connection secret.
    /// - For Kubernetes secrets, this will be used as "metadata.annotations".
    /// - It is up to Secret Store implementation for others store types.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub annotations: Option<HashMap<String, String>>,
    /// Labels are the labels/tags to be added to connection secret.
    /// - For Kubernetes secrets, this will be used as "metadata.labels".
    /// - It is up to Secret Store implementation for others store types.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub labels: Option<HashMap<String, String>>,
    /// Type is the SecretType for the connection secret.
    /// - Only valid for Kubernetes Secret Stores.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<String>,
}

/// WriteConnectionSecretToReference specifies the namespace and name of a
/// Secret to which any connection details for this managed resource should
/// be written. Connection details frequently include the endpoint, username,
/// and password required to connect to the managed resource.
/// This field is planned to be replaced in a future release in favor of
/// PublishConnectionDetailsTo. Currently, both could be set independently
/// and connection details would be published to both without affecting
/// each other.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeWriteConnectionSecretToRef {
    /// Name of the secret.
    pub name: String,
    /// Namespace of the secret.
    pub namespace: String,
}

/// PipeStatus defines the observed state of Pipe.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatus {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "atProvider")]
    pub at_provider: Option<PipeStatusAtProvider>,
    /// Conditions of the resource.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub conditions: Option<Vec<Condition>>,
    /// ObservedGeneration is the latest metadata.generation
    /// which resulted in either a ready state, or stalled due to error
    /// it can not recover from without human intervention.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "observedGeneration")]
    pub observed_generation: Option<i64>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProvider {
    /// ARN of this pipe.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub arn: Option<String>,
    /// A description of the pipe. At most 512 characters.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub description: Option<String>,
    /// The state the pipe should be in. One of: RUNNING, STOPPED.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "desiredState")]
    pub desired_state: Option<String>,
    /// Enrichment resource of the pipe (typically an ARN). Read more about enrichment in the User Guide.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub enrichment: Option<String>,
    /// Parameters to configure enrichment for your pipe. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enrichmentParameters")]
    pub enrichment_parameters: Option<PipeStatusAtProviderEnrichmentParameters>,
    /// Same as name.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub id: Option<String>,
    /// Logging configuration settings for the pipe. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "logConfiguration")]
    pub log_configuration: Option<PipeStatusAtProviderLogConfiguration>,
    /// ARN of the role that allows the pipe to send data to the target.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "roleArn")]
    pub role_arn: Option<String>,
    /// Source resource of the pipe. This field typically requires an ARN (Amazon Resource Name). However, when using a self-managed Kafka cluster, you should use a different format. Instead of an ARN, use 'smk://' followed by the bootstrap server's address.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub source: Option<String>,
    /// Parameters to configure a source for the pipe. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sourceParameters")]
    pub source_parameters: Option<PipeStatusAtProviderSourceParameters>,
    /// Key-value map of resource tags.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tags: Option<HashMap<String, String>>,
    /// Map of tags assigned to the resource, including those inherited from the provider default_tags configuration block.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tagsAll")]
    pub tags_all: Option<HashMap<String, String>>,
    /// Target resource of the pipe (typically an ARN).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub target: Option<String>,
    /// Parameters to configure a target for your pipe. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "targetParameters")]
    pub target_parameters: Option<PipeStatusAtProviderTargetParameters>,
}

/// Parameters to configure enrichment for your pipe. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderEnrichmentParameters {
    /// Contains the HTTP parameters to use when the target is a API Gateway REST endpoint or EventBridge ApiDestination. If you specify an API Gateway REST API or EventBridge ApiDestination as a target, you can use this parameter to specify headers, path parameters, and query string keys/values as part of your target invoking request. If you're using ApiDestinations, the corresponding Connection can also have these values configured. In case of any conflicting keys, values from the Connection take precedence. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpParameters")]
    pub http_parameters: Option<PipeStatusAtProviderEnrichmentParametersHttpParameters>,
    /// Valid JSON text passed to the target. In this case, nothing from the event itself is passed to the target. Maximum length of 8192 characters.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "inputTemplate")]
    pub input_template: Option<String>,
}

/// Contains the HTTP parameters to use when the target is a API Gateway REST endpoint or EventBridge ApiDestination. If you specify an API Gateway REST API or EventBridge ApiDestination as a target, you can use this parameter to specify headers, path parameters, and query string keys/values as part of your target invoking request. If you're using ApiDestinations, the corresponding Connection can also have these values configured. In case of any conflicting keys, values from the Connection take precedence. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderEnrichmentParametersHttpParameters {
    /// Key-value mapping of the headers that need to be sent as part of request invoking the API Gateway REST API or EventBridge ApiDestination.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "headerParameters")]
    pub header_parameters: Option<HashMap<String, String>>,
    /// The path parameter values to be used to populate API Gateway REST API or EventBridge ApiDestination path wildcards ("*").
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "pathParameterValues")]
    pub path_parameter_values: Option<Vec<String>>,
    /// Key-value mapping of the query strings that need to be sent as part of request invoking the API Gateway REST API or EventBridge ApiDestination.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryStringParameters")]
    pub query_string_parameters: Option<HashMap<String, String>>,
}

/// Logging configuration settings for the pipe. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderLogConfiguration {
    /// Amazon CloudWatch Logs logging configuration settings for the pipe. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "cloudwatchLogsLogDestination")]
    pub cloudwatch_logs_log_destination: Option<PipeStatusAtProviderLogConfigurationCloudwatchLogsLogDestination>,
    /// Amazon Kinesis Data Firehose logging configuration settings for the pipe. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "firehoseLogDestination")]
    pub firehose_log_destination: Option<PipeStatusAtProviderLogConfigurationFirehoseLogDestination>,
    /// String list that specifies whether the execution data (specifically, the payload, awsRequest, and awsResponse fields) is included in the log messages for this pipe. This applies to all log destinations for the pipe. Valid values ALL.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "includeExecutionData")]
    pub include_execution_data: Option<Vec<String>>,
    /// The level of logging detail to include. Valid values OFF, ERROR, INFO and TRACE.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub level: Option<String>,
    /// Amazon S3 logging configuration settings for the pipe. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "s3LogDestination")]
    pub s3_log_destination: Option<PipeStatusAtProviderLogConfigurationS3LogDestination>,
}

/// Amazon CloudWatch Logs logging configuration settings for the pipe. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderLogConfigurationCloudwatchLogsLogDestination {
    /// Amazon Web Services Resource Name (ARN) for the CloudWatch log group to which EventBridge sends the log records.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "logGroupArn")]
    pub log_group_arn: Option<String>,
}

/// Amazon Kinesis Data Firehose logging configuration settings for the pipe. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderLogConfigurationFirehoseLogDestination {
    /// Amazon Resource Name (ARN) of the Kinesis Data Firehose delivery stream to which EventBridge delivers the pipe log records.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "deliveryStreamArn")]
    pub delivery_stream_arn: Option<String>,
}

/// Amazon S3 logging configuration settings for the pipe. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderLogConfigurationS3LogDestination {
    /// Name of the Amazon S3 bucket to which EventBridge delivers the log records for the pipe.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketName")]
    pub bucket_name: Option<String>,
    /// Amazon Web Services account that owns the Amazon S3 bucket to which EventBridge delivers the log records for the pipe.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketOwner")]
    pub bucket_owner: Option<String>,
    /// EventBridge format for the log records. Valid values json, plain and w3c.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "outputFormat")]
    pub output_format: Option<String>,
    /// Prefix text with which to begin Amazon S3 log object names.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub prefix: Option<String>,
}

/// Parameters to configure a source for the pipe. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderSourceParameters {
    /// The parameters for using an Active MQ broker as a source. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "activemqBrokerParameters")]
    pub activemq_broker_parameters: Option<PipeStatusAtProviderSourceParametersActivemqBrokerParameters>,
    /// The parameters for using a DynamoDB stream as a source.  Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dynamodbStreamParameters")]
    pub dynamodb_stream_parameters: Option<PipeStatusAtProviderSourceParametersDynamodbStreamParameters>,
    /// The collection of event patterns used to filter events. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "filterCriteria")]
    pub filter_criteria: Option<PipeStatusAtProviderSourceParametersFilterCriteria>,
    /// The parameters for using a Kinesis stream as a source. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kinesisStreamParameters")]
    pub kinesis_stream_parameters: Option<PipeStatusAtProviderSourceParametersKinesisStreamParameters>,
    /// The parameters for using an MSK stream as a source. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "managedStreamingKafkaParameters")]
    pub managed_streaming_kafka_parameters: Option<PipeStatusAtProviderSourceParametersManagedStreamingKafkaParameters>,
    /// The parameters for using a Rabbit MQ broker as a source. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "rabbitmqBrokerParameters")]
    pub rabbitmq_broker_parameters: Option<PipeStatusAtProviderSourceParametersRabbitmqBrokerParameters>,
    /// The parameters for using a self-managed Apache Kafka stream as a source. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "selfManagedKafkaParameters")]
    pub self_managed_kafka_parameters: Option<PipeStatusAtProviderSourceParametersSelfManagedKafkaParameters>,
    /// The parameters for using a Amazon SQS stream as a source. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sqsQueueParameters")]
    pub sqs_queue_parameters: Option<PipeStatusAtProviderSourceParametersSqsQueueParameters>,
}

/// The parameters for using an Active MQ broker as a source. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderSourceParametersActivemqBrokerParameters {
    /// The maximum number of records to include in each batch. Maximum value of 10000.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "batchSize")]
    pub batch_size: Option<f64>,
    /// The credentials needed to access the resource. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub credentials: Option<PipeStatusAtProviderSourceParametersActivemqBrokerParametersCredentials>,
    /// The maximum length of a time to wait for events. Maximum value of 300.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maximumBatchingWindowInSeconds")]
    pub maximum_batching_window_in_seconds: Option<f64>,
    /// The name of the destination queue to consume. Maximum length of 1000.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queueName")]
    pub queue_name: Option<String>,
}

/// The credentials needed to access the resource. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderSourceParametersActivemqBrokerParametersCredentials {
    /// The ARN of the Secrets Manager secret containing the basic auth credentials.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "basicAuth")]
    pub basic_auth: Option<String>,
}

/// The parameters for using a DynamoDB stream as a source.  Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderSourceParametersDynamodbStreamParameters {
    /// The maximum number of records to include in each batch. Maximum value of 10000.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "batchSize")]
    pub batch_size: Option<f64>,
    /// Define the target queue to send dead-letter queue events to. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "deadLetterConfig")]
    pub dead_letter_config: Option<PipeStatusAtProviderSourceParametersDynamodbStreamParametersDeadLetterConfig>,
    /// The maximum length of a time to wait for events. Maximum value of 300.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maximumBatchingWindowInSeconds")]
    pub maximum_batching_window_in_seconds: Option<f64>,
    /// Discard records older than the specified age. The default value is -1, which sets the maximum age to infinite. When the value is set to infinite, EventBridge never discards old records. Maximum value of 604,800.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maximumRecordAgeInSeconds")]
    pub maximum_record_age_in_seconds: Option<f64>,
    /// Discard records after the specified number of retries. The default value is -1, which sets the maximum number of retries to infinite. When MaximumRetryAttempts is infinite, EventBridge retries failed records until the record expires in the event source. Maximum value of 10,000.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maximumRetryAttempts")]
    pub maximum_retry_attempts: Option<f64>,
    /// Define how to handle item process failures. AUTOMATIC_BISECT halves each batch and retry each half until all the records are processed or there is one failed message left in the batch. Valid values: AUTOMATIC_BISECT.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "onPartialBatchItemFailure")]
    pub on_partial_batch_item_failure: Option<String>,
    /// The number of batches to process concurrently from each shard. The default value is 1. Maximum value of 10.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "parallelizationFactor")]
    pub parallelization_factor: Option<f64>,
    /// The position in a stream from which to start reading. Valid values: TRIM_HORIZON, LATEST.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "startingPosition")]
    pub starting_position: Option<String>,
}

/// Define the target queue to send dead-letter queue events to. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderSourceParametersDynamodbStreamParametersDeadLetterConfig {
    /// The ARN of the Amazon SQS queue specified as the target for the dead-letter queue.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub arn: Option<String>,
}

/// The collection of event patterns used to filter events. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderSourceParametersFilterCriteria {
    /// An array of up to 5 event patterns. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub filter: Option<Vec<PipeStatusAtProviderSourceParametersFilterCriteriaFilter>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderSourceParametersFilterCriteriaFilter {
    /// The event pattern. At most 4096 characters.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub pattern: Option<String>,
}

/// The parameters for using a Kinesis stream as a source. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderSourceParametersKinesisStreamParameters {
    /// The maximum number of records to include in each batch. Maximum value of 10000.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "batchSize")]
    pub batch_size: Option<f64>,
    /// Define the target queue to send dead-letter queue events to. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "deadLetterConfig")]
    pub dead_letter_config: Option<PipeStatusAtProviderSourceParametersKinesisStreamParametersDeadLetterConfig>,
    /// The maximum length of a time to wait for events. Maximum value of 300.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maximumBatchingWindowInSeconds")]
    pub maximum_batching_window_in_seconds: Option<f64>,
    /// Discard records older than the specified age. The default value is -1, which sets the maximum age to infinite. When the value is set to infinite, EventBridge never discards old records. Maximum value of 604,800.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maximumRecordAgeInSeconds")]
    pub maximum_record_age_in_seconds: Option<f64>,
    /// Discard records after the specified number of retries. The default value is -1, which sets the maximum number of retries to infinite. When MaximumRetryAttempts is infinite, EventBridge retries failed records until the record expires in the event source. Maximum value of 10,000.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maximumRetryAttempts")]
    pub maximum_retry_attempts: Option<f64>,
    /// Define how to handle item process failures. AUTOMATIC_BISECT halves each batch and retry each half until all the records are processed or there is one failed message left in the batch. Valid values: AUTOMATIC_BISECT.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "onPartialBatchItemFailure")]
    pub on_partial_batch_item_failure: Option<String>,
    /// The number of batches to process concurrently from each shard. The default value is 1. Maximum value of 10.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "parallelizationFactor")]
    pub parallelization_factor: Option<f64>,
    /// The position in a stream from which to start reading. Valid values: TRIM_HORIZON, LATEST, AT_TIMESTAMP.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "startingPosition")]
    pub starting_position: Option<String>,
    /// With StartingPosition set to AT_TIMESTAMP, the time from which to start reading, in Unix time seconds.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "startingPositionTimestamp")]
    pub starting_position_timestamp: Option<String>,
}

/// Define the target queue to send dead-letter queue events to. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderSourceParametersKinesisStreamParametersDeadLetterConfig {
    /// The ARN of the Amazon SQS queue specified as the target for the dead-letter queue.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub arn: Option<String>,
}

/// The parameters for using an MSK stream as a source. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderSourceParametersManagedStreamingKafkaParameters {
    /// The maximum number of records to include in each batch. Maximum value of 10000.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "batchSize")]
    pub batch_size: Option<f64>,
    /// The name of the destination queue to consume. Maximum value of 200.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "consumerGroupId")]
    pub consumer_group_id: Option<String>,
    /// The credentials needed to access the resource. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub credentials: Option<PipeStatusAtProviderSourceParametersManagedStreamingKafkaParametersCredentials>,
    /// The maximum length of a time to wait for events. Maximum value of 300.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maximumBatchingWindowInSeconds")]
    pub maximum_batching_window_in_seconds: Option<f64>,
    /// The position in a stream from which to start reading. Valid values: TRIM_HORIZON, LATEST.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "startingPosition")]
    pub starting_position: Option<String>,
    /// The name of the topic that the pipe will read from. Maximum length of 249.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "topicName")]
    pub topic_name: Option<String>,
}

/// The credentials needed to access the resource. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderSourceParametersManagedStreamingKafkaParametersCredentials {
    /// The ARN of the Secrets Manager secret containing the credentials.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clientCertificateTlsAuth")]
    pub client_certificate_tls_auth: Option<String>,
    /// The ARN of the Secrets Manager secret containing the credentials.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "saslScram512Auth")]
    pub sasl_scram512_auth: Option<String>,
}

/// The parameters for using a Rabbit MQ broker as a source. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderSourceParametersRabbitmqBrokerParameters {
    /// The maximum number of records to include in each batch. Maximum value of 10000.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "batchSize")]
    pub batch_size: Option<f64>,
    /// The credentials needed to access the resource. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub credentials: Option<PipeStatusAtProviderSourceParametersRabbitmqBrokerParametersCredentials>,
    /// The maximum length of a time to wait for events. Maximum value of 300.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maximumBatchingWindowInSeconds")]
    pub maximum_batching_window_in_seconds: Option<f64>,
    /// The name of the destination queue to consume. Maximum length of 1000.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queueName")]
    pub queue_name: Option<String>,
    /// The name of the virtual host associated with the source broker. Maximum length of 200.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "virtualHost")]
    pub virtual_host: Option<String>,
}

/// The credentials needed to access the resource. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderSourceParametersRabbitmqBrokerParametersCredentials {
    /// The ARN of the Secrets Manager secret containing the credentials.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "basicAuth")]
    pub basic_auth: Option<String>,
}

/// The parameters for using a self-managed Apache Kafka stream as a source. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderSourceParametersSelfManagedKafkaParameters {
    /// An array of server URLs. Maximum number of 2 items, each of maximum length 300.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "additionalBootstrapServers")]
    pub additional_bootstrap_servers: Option<Vec<String>>,
    /// The maximum number of records to include in each batch. Maximum value of 10000.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "batchSize")]
    pub batch_size: Option<f64>,
    /// The name of the destination queue to consume. Maximum value of 200.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "consumerGroupId")]
    pub consumer_group_id: Option<String>,
    /// The credentials needed to access the resource. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub credentials: Option<PipeStatusAtProviderSourceParametersSelfManagedKafkaParametersCredentials>,
    /// The maximum length of a time to wait for events. Maximum value of 300.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maximumBatchingWindowInSeconds")]
    pub maximum_batching_window_in_seconds: Option<f64>,
    /// The ARN of the Secrets Manager secret used for certification.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serverRootCaCertificate")]
    pub server_root_ca_certificate: Option<String>,
    /// The position in a stream from which to start reading. Valid values: TRIM_HORIZON, LATEST.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "startingPosition")]
    pub starting_position: Option<String>,
    /// The name of the topic that the pipe will read from. Maximum length of 249.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "topicName")]
    pub topic_name: Option<String>,
    /// This structure specifies the VPC subnets and security groups for the stream, and whether a public IP address is to be used. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub vpc: Option<PipeStatusAtProviderSourceParametersSelfManagedKafkaParametersVpc>,
}

/// The credentials needed to access the resource. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderSourceParametersSelfManagedKafkaParametersCredentials {
    /// The ARN of the Secrets Manager secret containing the credentials.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "basicAuth")]
    pub basic_auth: Option<String>,
    /// The ARN of the Secrets Manager secret containing the credentials.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clientCertificateTlsAuth")]
    pub client_certificate_tls_auth: Option<String>,
    /// The ARN of the Secrets Manager secret containing the credentials.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "saslScram256Auth")]
    pub sasl_scram256_auth: Option<String>,
    /// The ARN of the Secrets Manager secret containing the credentials.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "saslScram512Auth")]
    pub sasl_scram512_auth: Option<String>,
}

/// This structure specifies the VPC subnets and security groups for the stream, and whether a public IP address is to be used. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderSourceParametersSelfManagedKafkaParametersVpc {
    /// List of security groups associated with the stream. These security groups must all be in the same VPC. You can specify as many as five security groups. If you do not specify a security group, the default security group for the VPC is used.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "securityGroups")]
    pub security_groups: Option<Vec<String>>,
    /// List of the subnets associated with the stream. These subnets must all be in the same VPC. You can specify as many as 16 subnets.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub subnets: Option<Vec<String>>,
}

/// The parameters for using a Amazon SQS stream as a source. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderSourceParametersSqsQueueParameters {
    /// The maximum number of records to include in each batch. Maximum value of 10000.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "batchSize")]
    pub batch_size: Option<f64>,
    /// The maximum length of a time to wait for events. Maximum value of 300.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maximumBatchingWindowInSeconds")]
    pub maximum_batching_window_in_seconds: Option<f64>,
}

/// Parameters to configure a target for your pipe. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderTargetParameters {
    /// The parameters for using an AWS Batch job as a target. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "batchJobParameters")]
    pub batch_job_parameters: Option<PipeStatusAtProviderTargetParametersBatchJobParameters>,
    /// The parameters for using an CloudWatch Logs log stream as a target. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "cloudwatchLogsParameters")]
    pub cloudwatch_logs_parameters: Option<PipeStatusAtProviderTargetParametersCloudwatchLogsParameters>,
    /// The parameters for using an Amazon ECS task as a target. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "ecsTaskParameters")]
    pub ecs_task_parameters: Option<PipeStatusAtProviderTargetParametersEcsTaskParameters>,
    /// The parameters for using an EventBridge event bus as a target. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "eventbridgeEventBusParameters")]
    pub eventbridge_event_bus_parameters: Option<PipeStatusAtProviderTargetParametersEventbridgeEventBusParameters>,
    /// These are custom parameter to be used when the target is an API Gateway REST APIs or EventBridge ApiDestinations. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpParameters")]
    pub http_parameters: Option<PipeStatusAtProviderTargetParametersHttpParameters>,
    /// Valid JSON text passed to the target. In this case, nothing from the event itself is passed to the target. Maximum length of 8192 characters.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "inputTemplate")]
    pub input_template: Option<String>,
    /// The parameters for using a Kinesis stream as a source. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kinesisStreamParameters")]
    pub kinesis_stream_parameters: Option<PipeStatusAtProviderTargetParametersKinesisStreamParameters>,
    /// The parameters for using a Lambda function as a target. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "lambdaFunctionParameters")]
    pub lambda_function_parameters: Option<PipeStatusAtProviderTargetParametersLambdaFunctionParameters>,
    /// These are custom parameters to be used when the target is a Amazon Redshift cluster to invoke the Amazon Redshift Data API BatchExecuteStatement. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "redshiftDataParameters")]
    pub redshift_data_parameters: Option<PipeStatusAtProviderTargetParametersRedshiftDataParameters>,
    /// The parameters for using a SageMaker pipeline as a target. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sagemakerPipelineParameters")]
    pub sagemaker_pipeline_parameters: Option<PipeStatusAtProviderTargetParametersSagemakerPipelineParameters>,
    /// The parameters for using a Amazon SQS stream as a target. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sqsQueueParameters")]
    pub sqs_queue_parameters: Option<PipeStatusAtProviderTargetParametersSqsQueueParameters>,
    /// The parameters for using a Step Functions state machine as a target. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "stepFunctionStateMachineParameters")]
    pub step_function_state_machine_parameters: Option<PipeStatusAtProviderTargetParametersStepFunctionStateMachineParameters>,
}

/// The parameters for using an AWS Batch job as a target. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderTargetParametersBatchJobParameters {
    /// The array properties for the submitted job, such as the size of the array. The array size can be between 2 and 10,000. If you specify array properties for a job, it becomes an array job. This parameter is used only if the target is an AWS Batch job. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "arrayProperties")]
    pub array_properties: Option<PipeStatusAtProviderTargetParametersBatchJobParametersArrayProperties>,
    /// The overrides that are sent to a container. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "containerOverrides")]
    pub container_overrides: Option<PipeStatusAtProviderTargetParametersBatchJobParametersContainerOverrides>,
    /// A list of dependencies for the job. A job can depend upon a maximum of 20 jobs. You can specify a SEQUENTIAL type dependency without specifying a job ID for array jobs so that each child array job completes sequentially, starting at index 0. You can also specify an N_TO_N type dependency with a job ID for array jobs. In that case, each index child of this job must wait for the corresponding index child of each dependency to complete before it can begin. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dependsOn")]
    pub depends_on: Option<Vec<PipeStatusAtProviderTargetParametersBatchJobParametersDependsOn>>,
    /// The job definition used by this job. This value can be one of name, name:revision, or the Amazon Resource Name (ARN) for the job definition. If name is specified without a revision then the latest active revision is used.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jobDefinition")]
    pub job_definition: Option<String>,
    /// The name of the job. It can be up to 128 letters long.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jobName")]
    pub job_name: Option<String>,
    /// Additional parameters passed to the job that replace parameter substitution placeholders that are set in the job definition. Parameters are specified as a key and value pair mapping. Parameters included here override any corresponding parameter defaults from the job definition. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub parameters: Option<HashMap<String, String>>,
    /// The retry strategy to use for failed jobs. When a retry strategy is specified here, it overrides the retry strategy defined in the job definition. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "retryStrategy")]
    pub retry_strategy: Option<PipeStatusAtProviderTargetParametersBatchJobParametersRetryStrategy>,
}

/// The array properties for the submitted job, such as the size of the array. The array size can be between 2 and 10,000. If you specify array properties for a job, it becomes an array job. This parameter is used only if the target is an AWS Batch job. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderTargetParametersBatchJobParametersArrayProperties {
    /// The size of the array, if this is an array batch job. Minimum value of 2. Maximum value of 10,000.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub size: Option<f64>,
}

/// The overrides that are sent to a container. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderTargetParametersBatchJobParametersContainerOverrides {
    /// List of commands to send to the container that overrides the default command from the Docker image or the task definition.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
    /// The environment variables to send to the container. You can add new environment variables, which are added to the container at launch, or you can override the existing environment variables from the Docker image or the task definition. Environment variables cannot start with " AWS Batch ". This naming convention is reserved for variables that AWS Batch sets. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub environment: Option<Vec<PipeStatusAtProviderTargetParametersBatchJobParametersContainerOverridesEnvironment>>,
    /// The instance type to use for a multi-node parallel job. This parameter isn't applicable to single-node container jobs or jobs that run on Fargate resources, and shouldn't be provided.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "instanceType")]
    pub instance_type: Option<String>,
    /// The type and amount of resources to assign to a container. This overrides the settings in the job definition. The supported resources include GPU, MEMORY, and VCPU. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "resourceRequirement")]
    pub resource_requirement: Option<Vec<PipeStatusAtProviderTargetParametersBatchJobParametersContainerOverridesResourceRequirement>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderTargetParametersBatchJobParametersContainerOverridesEnvironment {
    /// The name of the key-value pair. For environment variables, this is the name of the environment variable.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// The value of the key-value pair. For environment variables, this is the value of the environment variable.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderTargetParametersBatchJobParametersContainerOverridesResourceRequirement {
    /// The type of resource to assign to a container. The supported resources include GPU, MEMORY, and VCPU.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<String>,
    /// The quantity of the specified resource to reserve for the container. The values vary based on the type specified.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderTargetParametersBatchJobParametersDependsOn {
    /// The job ID of the AWS Batch job that's associated with this dependency.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jobId")]
    pub job_id: Option<String>,
    /// The type of the job dependency. Valid Values: N_TO_N, SEQUENTIAL.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<String>,
}

/// The retry strategy to use for failed jobs. When a retry strategy is specified here, it overrides the retry strategy defined in the job definition. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderTargetParametersBatchJobParametersRetryStrategy {
    /// The number of times to move a job to the RUNNABLE status. If the value of attempts is greater than one, the job is retried on failure the same number of attempts as the value. Maximum value of 10.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub attempts: Option<f64>,
}

/// The parameters for using an CloudWatch Logs log stream as a target. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderTargetParametersCloudwatchLogsParameters {
    /// The name of the log stream.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "logStreamName")]
    pub log_stream_name: Option<String>,
    /// The time the event occurred, expressed as the number of milliseconds after Jan 1, 1970 00:00:00 UTC. This is the JSON path to the field in the event e.g. $.detail.timestamp
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub timestamp: Option<String>,
}

/// The parameters for using an Amazon ECS task as a target. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderTargetParametersEcsTaskParameters {
    /// List of capacity provider strategies to use for the task. If a capacityProviderStrategy is specified, the launchType parameter must be omitted. If no capacityProviderStrategy or launchType is specified, the defaultCapacityProviderStrategy for the cluster is used. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "capacityProviderStrategy")]
    pub capacity_provider_strategy: Option<Vec<PipeStatusAtProviderTargetParametersEcsTaskParametersCapacityProviderStrategy>>,
    /// Specifies whether to enable Amazon ECS managed tags for the task. Valid values: true, false.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableEcsManagedTags")]
    pub enable_ecs_managed_tags: Option<bool>,
    /// Whether or not to enable the execute command functionality for the containers in this task. If true, this enables execute command functionality on all containers in the task. Valid values: true, false.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableExecuteCommand")]
    pub enable_execute_command: Option<bool>,
    /// Specifies an Amazon ECS task group for the task. The maximum length is 255 characters.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub group: Option<String>,
    /// Specifies the launch type on which your task is running. The launch type that you specify here must match one of the launch type (compatibilities) of the target task. The FARGATE value is supported only in the Regions where AWS Fargate with Amazon ECS is supported. Valid Values: EC2, FARGATE, EXTERNAL
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "launchType")]
    pub launch_type: Option<String>,
    /// Use this structure if the Amazon ECS task uses the awsvpc network mode. This structure specifies the VPC subnets and security groups associated with the task, and whether a public IP address is to be used. This structure is required if LaunchType is FARGATE because the awsvpc mode is required for Fargate tasks. If you specify NetworkConfiguration when the target ECS task does not use the awsvpc network mode, the task fails. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "networkConfiguration")]
    pub network_configuration: Option<PipeStatusAtProviderTargetParametersEcsTaskParametersNetworkConfiguration>,
    /// The overrides that are associated with a task. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub overrides: Option<PipeStatusAtProviderTargetParametersEcsTaskParametersOverrides>,
    /// An array of placement constraint objects to use for the task. You can specify up to 10 constraints per task (including constraints in the task definition and those specified at runtime). Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "placementConstraint")]
    pub placement_constraint: Option<Vec<PipeStatusAtProviderTargetParametersEcsTaskParametersPlacementConstraint>>,
    /// The placement strategy objects to use for the task. You can specify a maximum of five strategy rules per task. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "placementStrategy")]
    pub placement_strategy: Option<Vec<PipeStatusAtProviderTargetParametersEcsTaskParametersPlacementStrategy>>,
    /// Specifies the platform version for the task. Specify only the numeric portion of the platform version, such as 1.1.0. This structure is used only if LaunchType is FARGATE.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "platformVersion")]
    pub platform_version: Option<String>,
    /// Specifies whether to propagate the tags from the task definition to the task. If no value is specified, the tags are not propagated. Tags can only be propagated to the task during task creation. To add tags to a task after task creation, use the TagResource API action. Valid Values: TASK_DEFINITION
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "propagateTags")]
    pub propagate_tags: Option<String>,
    /// The reference ID to use for the task. Maximum length of 1,024.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "referenceId")]
    pub reference_id: Option<String>,
    /// Key-value map of tags that you apply to the task to help you categorize and organize them.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tags: Option<HashMap<String, String>>,
    /// The number of tasks to create based on TaskDefinition. The default is 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "taskCount")]
    pub task_count: Option<f64>,
    /// The ARN of the task definition to use if the event target is an Amazon ECS task.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "taskDefinitionArn")]
    pub task_definition_arn: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderTargetParametersEcsTaskParametersCapacityProviderStrategy {
    /// The base value designates how many tasks, at a minimum, to run on the specified capacity provider. Only one capacity provider in a capacity provider strategy can have a base defined. If no value is specified, the default value of 0 is used. Maximum value of 100,000.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub base: Option<f64>,
    /// The short name of the capacity provider. Maximum value of 255.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "capacityProvider")]
    pub capacity_provider: Option<String>,
    /// The weight value designates the relative percentage of the total number of tasks launched that should use the specified capacity provider. The weight value is taken into consideration after the base value, if defined, is satisfied. Maximum value of 1,000.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub weight: Option<f64>,
}

/// Use this structure if the Amazon ECS task uses the awsvpc network mode. This structure specifies the VPC subnets and security groups associated with the task, and whether a public IP address is to be used. This structure is required if LaunchType is FARGATE because the awsvpc mode is required for Fargate tasks. If you specify NetworkConfiguration when the target ECS task does not use the awsvpc network mode, the task fails. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderTargetParametersEcsTaskParametersNetworkConfiguration {
    /// Use this structure to specify the VPC subnets and security groups for the task, and whether a public IP address is to be used. This structure is relevant only for ECS tasks that use the awsvpc network mode. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "awsVpcConfiguration")]
    pub aws_vpc_configuration: Option<PipeStatusAtProviderTargetParametersEcsTaskParametersNetworkConfigurationAwsVpcConfiguration>,
}

/// Use this structure to specify the VPC subnets and security groups for the task, and whether a public IP address is to be used. This structure is relevant only for ECS tasks that use the awsvpc network mode. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderTargetParametersEcsTaskParametersNetworkConfigurationAwsVpcConfiguration {
    /// Specifies whether the task's elastic network interface receives a public IP address. You can specify ENABLED only when LaunchType in EcsParameters is set to FARGATE. Valid Values: ENABLED, DISABLED.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "assignPublicIp")]
    pub assign_public_ip: Option<String>,
    /// Specifies the security groups associated with the task. These security groups must all be in the same VPC. You can specify as many as five security groups. If you do not specify a security group, the default security group for the VPC is used.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "securityGroups")]
    pub security_groups: Option<Vec<String>>,
    /// Specifies the subnets associated with the task. These subnets must all be in the same VPC. You can specify as many as 16 subnets.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub subnets: Option<Vec<String>>,
}

/// The overrides that are associated with a task. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderTargetParametersEcsTaskParametersOverrides {
    /// One or more container overrides that are sent to a task. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "containerOverride")]
    pub container_override: Option<Vec<PipeStatusAtProviderTargetParametersEcsTaskParametersOverridesContainerOverride>>,
    /// The cpu override for the task.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub cpu: Option<String>,
    /// The ephemeral storage setting override for the task.  Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "ephemeralStorage")]
    pub ephemeral_storage: Option<PipeStatusAtProviderTargetParametersEcsTaskParametersOverridesEphemeralStorage>,
    /// The Amazon Resource Name (ARN) of the task execution IAM role override for the task.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "executionRoleArn")]
    pub execution_role_arn: Option<String>,
    /// List of Elastic Inference accelerator overrides for the task. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "inferenceAcceleratorOverride")]
    pub inference_accelerator_override: Option<Vec<PipeStatusAtProviderTargetParametersEcsTaskParametersOverridesInferenceAcceleratorOverride>>,
    /// The memory override for the task.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub memory: Option<String>,
    /// The Amazon Resource Name (ARN) of the IAM role that containers in this task can assume. All containers in this task are granted the permissions that are specified in this role.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "taskRoleArn")]
    pub task_role_arn: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderTargetParametersEcsTaskParametersOverridesContainerOverride {
    /// List of commands to send to the container that overrides the default command from the Docker image or the task definition. You must also specify a container name.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub command: Option<Vec<String>>,
    /// The number of cpu units reserved for the container, instead of the default value from the task definition. You must also specify a container name.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub cpu: Option<f64>,
    /// The environment variables to send to the container. You can add new environment variables, which are added to the container at launch, or you can override the existing environment variables from the Docker image or the task definition. You must also specify a container name. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub environment: Option<Vec<PipeStatusAtProviderTargetParametersEcsTaskParametersOverridesContainerOverrideEnvironment>>,
    /// A list of files containing the environment variables to pass to a container, instead of the value from the container definition. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "environmentFile")]
    pub environment_file: Option<Vec<PipeStatusAtProviderTargetParametersEcsTaskParametersOverridesContainerOverrideEnvironmentFile>>,
    /// The hard limit (in MiB) of memory to present to the container, instead of the default value from the task definition. If your container attempts to exceed the memory specified here, the container is killed. You must also specify a container name.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub memory: Option<f64>,
    /// The soft limit (in MiB) of memory to reserve for the container, instead of the default value from the task definition. You must also specify a container name.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "memoryReservation")]
    pub memory_reservation: Option<f64>,
    /// The name of the container that receives the override. This parameter is required if any override is specified.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// The type and amount of a resource to assign to a container, instead of the default value from the task definition. The only supported resource is a GPU. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "resourceRequirement")]
    pub resource_requirement: Option<Vec<PipeStatusAtProviderTargetParametersEcsTaskParametersOverridesContainerOverrideResourceRequirement>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderTargetParametersEcsTaskParametersOverridesContainerOverrideEnvironment {
    /// The name of the key-value pair. For environment variables, this is the name of the environment variable.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// The value of the key-value pair. For environment variables, this is the value of the environment variable.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderTargetParametersEcsTaskParametersOverridesContainerOverrideEnvironmentFile {
    /// The file type to use. The only supported value is s3.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<String>,
    /// The Amazon Resource Name (ARN) of the Amazon S3 object containing the environment variable file.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderTargetParametersEcsTaskParametersOverridesContainerOverrideResourceRequirement {
    /// The type of resource to assign to a container. The supported values are GPU or InferenceAccelerator.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<String>,
    /// The value for the specified resource type. If the GPU type is used, the value is the number of physical GPUs the Amazon ECS container agent reserves for the container. The number of GPUs that's reserved for all containers in a task can't exceed the number of available GPUs on the container instance that the task is launched on. If the InferenceAccelerator type is used, the value matches the deviceName for an InferenceAccelerator specified in a task definition.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
}

/// The ephemeral storage setting override for the task.  Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderTargetParametersEcsTaskParametersOverridesEphemeralStorage {
    /// The total amount, in GiB, of ephemeral storage to set for the task. The minimum supported value is 21 GiB and the maximum supported value is 200 GiB.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sizeInGib")]
    pub size_in_gib: Option<f64>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderTargetParametersEcsTaskParametersOverridesInferenceAcceleratorOverride {
    /// The Elastic Inference accelerator device name to override for the task. This parameter must match a deviceName specified in the task definition.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "deviceName")]
    pub device_name: Option<String>,
    /// The Elastic Inference accelerator type to use.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "deviceType")]
    pub device_type: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderTargetParametersEcsTaskParametersPlacementConstraint {
    /// A cluster query language expression to apply to the constraint. You cannot specify an expression if the constraint type is distinctInstance. Maximum length of 2,000.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub expression: Option<String>,
    /// The type of constraint. Use distinctInstance to ensure that each task in a particular group is running on a different container instance. Use memberOf to restrict the selection to a group of valid candidates. Valid Values: distinctInstance, memberOf.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderTargetParametersEcsTaskParametersPlacementStrategy {
    /// The field to apply the placement strategy against. For the spread placement strategy, valid values are instanceId (or host, which has the same effect), or any platform or custom attribute that is applied to a container instance, such as attribute:ecs.availability-zone. For the binpack placement strategy, valid values are cpu and memory. For the random placement strategy, this field is not used. Maximum length of 255.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub field: Option<String>,
    /// The type of placement strategy. The random placement strategy randomly places tasks on available candidates. The spread placement strategy spreads placement across available candidates evenly based on the field parameter. The binpack strategy places tasks on available candidates that have the least available amount of the resource that is specified with the field parameter. For example, if you binpack on memory, a task is placed on the instance with the least amount of remaining memory (but still enough to run the task). Valid Values: random, spread, binpack.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<String>,
}

/// The parameters for using an EventBridge event bus as a target. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderTargetParametersEventbridgeEventBusParameters {
    /// A free-form string, with a maximum of 128 characters, used to decide what fields to expect in the event detail.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "detailType")]
    pub detail_type: Option<String>,
    /// The URL subdomain of the endpoint. For example, if the URL for Endpoint is https://abcde.veo.endpoints.event.amazonaws.com, then the EndpointId is abcde.veo.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "endpointId")]
    pub endpoint_id: Option<String>,
    /// List of AWS resources, identified by Amazon Resource Name (ARN), which the event primarily concerns. Any number, including zero, may be present.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resources: Option<Vec<String>>,
    /// The source of the event. Maximum length of 256.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub source: Option<String>,
    /// The time stamp of the event, per RFC3339. If no time stamp is provided, the time stamp of the PutEvents call is used. This is the JSON path to the field in the event e.g. $.detail.timestamp
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub time: Option<String>,
}

/// These are custom parameter to be used when the target is an API Gateway REST APIs or EventBridge ApiDestinations. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderTargetParametersHttpParameters {
    /// Key-value mapping of the headers that need to be sent as part of request invoking the API Gateway REST API or EventBridge ApiDestination. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "headerParameters")]
    pub header_parameters: Option<HashMap<String, String>>,
    /// The path parameter values to be used to populate API Gateway REST API or EventBridge ApiDestination path wildcards ("*").
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "pathParameterValues")]
    pub path_parameter_values: Option<Vec<String>>,
    /// Key-value mapping of the query strings that need to be sent as part of request invoking the API Gateway REST API or EventBridge ApiDestination. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryStringParameters")]
    pub query_string_parameters: Option<HashMap<String, String>>,
}

/// The parameters for using a Kinesis stream as a source. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderTargetParametersKinesisStreamParameters {
    /// Determines which shard in the stream the data record is assigned to. Partition keys are Unicode strings with a maximum length limit of 256 characters for each key. Amazon Kinesis Data Streams uses the partition key as input to a hash function that maps the partition key and associated data to a specific shard. Specifically, an MD5 hash function is used to map partition keys to 128-bit integer values and to map associated data records to shards. As a result of this hashing mechanism, all data records with the same partition key map to the same shard within the stream.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "partitionKey")]
    pub partition_key: Option<String>,
}

/// The parameters for using a Lambda function as a target. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderTargetParametersLambdaFunctionParameters {
    /// Specify whether to invoke the function synchronously or asynchronously. Valid Values: REQUEST_RESPONSE, FIRE_AND_FORGET.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "invocationType")]
    pub invocation_type: Option<String>,
}

/// These are custom parameters to be used when the target is a Amazon Redshift cluster to invoke the Amazon Redshift Data API BatchExecuteStatement. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderTargetParametersRedshiftDataParameters {
    /// The name of the database. Required when authenticating using temporary credentials.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub database: Option<String>,
    /// The database user name. Required when authenticating using temporary credentials.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dbUser")]
    pub db_user: Option<String>,
    /// The name or ARN of the secret that enables access to the database. Required when authenticating using Secrets Manager.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secretManagerArn")]
    pub secret_manager_arn: Option<String>,
    /// List of SQL statements text to run, each of maximum length of 100,000.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sqls: Option<Vec<String>>,
    /// The name of the SQL statement. You can name the SQL statement when you create it to identify the query.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "statementName")]
    pub statement_name: Option<String>,
    /// Indicates whether to send an event back to EventBridge after the SQL statement runs.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "withEvent")]
    pub with_event: Option<bool>,
}

/// The parameters for using a SageMaker pipeline as a target. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderTargetParametersSagemakerPipelineParameters {
    /// List of Parameter names and values for SageMaker Model Building Pipeline execution. Detailed below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "pipelineParameter")]
    pub pipeline_parameter: Option<Vec<PipeStatusAtProviderTargetParametersSagemakerPipelineParametersPipelineParameter>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderTargetParametersSagemakerPipelineParametersPipelineParameter {
    /// The name of the container that receives the override. This parameter is required if any override is specified.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Value of parameter to start execution of a SageMaker Model Building Pipeline. Maximum length of 1024.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub value: Option<String>,
}

/// The parameters for using a Amazon SQS stream as a target. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderTargetParametersSqsQueueParameters {
    /// This parameter applies only to FIFO (first-in-first-out) queues. The token used for deduplication of sent messages.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "messageDeduplicationId")]
    pub message_deduplication_id: Option<String>,
    /// The FIFO message group ID to use as the target.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "messageGroupId")]
    pub message_group_id: Option<String>,
}

/// The parameters for using a Step Functions state machine as a target. Detailed below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct PipeStatusAtProviderTargetParametersStepFunctionStateMachineParameters {
    /// Specify whether to invoke the function synchronously or asynchronously. Valid Values: REQUEST_RESPONSE, FIRE_AND_FORGET.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "invocationType")]
    pub invocation_type: Option<String>,
}

