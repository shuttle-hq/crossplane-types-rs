// WARNING: generated by kopium - manual changes will be overwritten
// kopium version: 0.21.2

#[allow(unused_imports)]
mod prelude {
    pub use kube::CustomResource;
    pub use schemars::JsonSchema;
    pub use serde::{Serialize, Deserialize};
    pub use std::collections::HashMap;
    pub use k8s_openapi::apimachinery::pkg::apis::meta::v1::Condition;
}
use self::prelude::*;

/// WorkflowTemplateSpec defines the desired state of WorkflowTemplate
#[derive(CustomResource, Serialize, Deserialize, Clone, Debug, JsonSchema)]
#[kube(group = "dataproc.gcp.upbound.io", version = "v1beta2", kind = "WorkflowTemplate", plural = "workflowtemplates")]
#[kube(status = "WorkflowTemplateStatus")]
pub struct WorkflowTemplateSpec {
    /// DeletionPolicy specifies what will happen to the underlying external
    /// when this managed resource is deleted - either "Delete" or "Orphan" the
    /// external resource.
    /// This field is planned to be deprecated in favor of the ManagementPolicies
    /// field in a future release. Currently, both could be set independently and
    /// non-default values would be honored if the feature flag is enabled.
    /// See the design doc for more information: https://github.com/crossplane/crossplane/blob/499895a25d1a1a0ba1604944ef98ac7a1a71f197/design/design-doc-observe-only-resources.md?plain=1#L223
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "deletionPolicy")]
    pub deletion_policy: Option<WorkflowTemplateDeletionPolicy>,
    #[serde(rename = "forProvider")]
    pub for_provider: WorkflowTemplateForProvider,
    /// THIS IS A BETA FIELD. It will be honored
    /// unless the Management Policies feature flag is disabled.
    /// InitProvider holds the same fields as ForProvider, with the exception
    /// of Identifier and other resource reference fields. The fields that are
    /// in InitProvider are merged into ForProvider when the resource is created.
    /// The same fields are also added to the terraform ignore_changes hook, to
    /// avoid updating them after creation. This is useful for fields that are
    /// required on creation, but we do not desire to update them after creation,
    /// for example because of an external controller is managing them, like an
    /// autoscaler.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initProvider")]
    pub init_provider: Option<WorkflowTemplateInitProvider>,
    /// THIS IS A BETA FIELD. It is on by default but can be opted out
    /// through a Crossplane feature flag.
    /// ManagementPolicies specify the array of actions Crossplane is allowed to
    /// take on the managed and external resources.
    /// This field is planned to replace the DeletionPolicy field in a future
    /// release. Currently, both could be set independently and non-default
    /// values would be honored if the feature flag is enabled. If both are
    /// custom, the DeletionPolicy field will be ignored.
    /// See the design doc for more information: https://github.com/crossplane/crossplane/blob/499895a25d1a1a0ba1604944ef98ac7a1a71f197/design/design-doc-observe-only-resources.md?plain=1#L223
    /// and this one: https://github.com/crossplane/crossplane/blob/444267e84783136daa93568b364a5f01228cacbe/design/one-pager-ignore-changes.md
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "managementPolicies")]
    pub management_policies: Option<Vec<String>>,
    /// ProviderConfigReference specifies how the provider that will be used to
    /// create, observe, update, and delete this managed resource should be
    /// configured.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "providerConfigRef")]
    pub provider_config_ref: Option<WorkflowTemplateProviderConfigRef>,
    /// PublishConnectionDetailsTo specifies the connection secret config which
    /// contains a name, metadata and a reference to secret store config to
    /// which any connection details for this managed resource should be written.
    /// Connection details frequently include the endpoint, username,
    /// and password required to connect to the managed resource.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "publishConnectionDetailsTo")]
    pub publish_connection_details_to: Option<WorkflowTemplatePublishConnectionDetailsTo>,
    /// WriteConnectionSecretToReference specifies the namespace and name of a
    /// Secret to which any connection details for this managed resource should
    /// be written. Connection details frequently include the endpoint, username,
    /// and password required to connect to the managed resource.
    /// This field is planned to be replaced in a future release in favor of
    /// PublishConnectionDetailsTo. Currently, both could be set independently
    /// and connection details would be published to both without affecting
    /// each other.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "writeConnectionSecretToRef")]
    pub write_connection_secret_to_ref: Option<WorkflowTemplateWriteConnectionSecretToRef>,
}

/// WorkflowTemplateSpec defines the desired state of WorkflowTemplate
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum WorkflowTemplateDeletionPolicy {
    Orphan,
    Delete,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateForProvider {
    /// (Beta only) Optional. Timeout duration for the DAG of jobs. You can use "s", "m", "h", and "d" suffixes for second, minute, hour, and day duration values, respectively. The timeout duration must be from 10 minutes ("10m") to 24 hours ("24h" or "1d"). The timer begins when the first job is submitted. If the workflow is running at the end of the timeout period, any remaining jobs are cancelled, the workflow is ended, and if the workflow was running on a (/dataproc/docs/concepts/workflows/using-workflows#configuring_or_selecting_a_cluster), the cluster is deleted.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dagTimeout")]
    pub dag_timeout: Option<String>,
    /// Required. The Directed Acyclic Graph of Jobs to submit.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub jobs: Option<Vec<WorkflowTemplateForProviderJobs>>,
    /// The labels to associate with this cluster. Label keys must be between 1 and 63 characters long, and must conform to the following PCRE regular expression: {0,63} No more than 32 labels can be associated with a given cluster.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub labels: Option<HashMap<String, String>>,
    /// The location for the resource
    pub location: String,
    /// Template parameters whose values are substituted into the template. Values for parameters must be provided when the template is instantiated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub parameters: Option<Vec<WorkflowTemplateForProviderParameters>>,
    /// Required. WorkflowTemplate scheduling information.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub placement: Option<WorkflowTemplateForProviderPlacement>,
    /// The project for the resource
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub project: Option<String>,
    /// Used to perform a consistent read-modify-write. This field should be left blank for a CreateWorkflowTemplate request. It is required for an UpdateWorkflowTemplate request, and must match the current server version. A typical update template flow would fetch the current template with a GetWorkflowTemplate request, which will return the current template with the version field filled in with the current server version. The user updates other fields in the template, then returns it as part of the UpdateWorkflowTemplate request.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub version: Option<f64>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateForProviderJobs {
    /// Job is a Hadoop job.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hadoopJob")]
    pub hadoop_job: Option<WorkflowTemplateForProviderJobsHadoopJob>,
    /// Job is a Hive job.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hiveJob")]
    pub hive_job: Option<WorkflowTemplateForProviderJobsHiveJob>,
    /// The labels to associate with this job. Label keys must be between 1 and 63 characters long, and must conform to the following regular expression: {0,63} No more than 32 labels can be associated with a given job.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub labels: Option<HashMap<String, String>>,
    /// Job is a Pig job.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "pigJob")]
    pub pig_job: Option<WorkflowTemplateForProviderJobsPigJob>,
    /// The optional list of prerequisite job step_ids. If not specified, the job will start at the beginning of workflow.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "prerequisiteStepIds")]
    pub prerequisite_step_ids: Option<Vec<String>>,
    /// Job is a Presto job.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "prestoJob")]
    pub presto_job: Option<WorkflowTemplateForProviderJobsPrestoJob>,
    /// Job is a PySpark job.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "pysparkJob")]
    pub pyspark_job: Option<WorkflowTemplateForProviderJobsPysparkJob>,
    /// Job scheduling configuration.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheduling: Option<WorkflowTemplateForProviderJobsScheduling>,
    /// Job is a Spark job.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sparkJob")]
    pub spark_job: Option<WorkflowTemplateForProviderJobsSparkJob>,
    /// Job is a SparkR job.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sparkRJob")]
    pub spark_r_job: Option<WorkflowTemplateForProviderJobsSparkRJob>,
    /// Job is a SparkSql job.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sparkSqlJob")]
    pub spark_sql_job: Option<WorkflowTemplateForProviderJobsSparkSqlJob>,
    /// Required. The step id. The id must be unique among all jobs within the template. The step id is used as prefix for job id, as job goog-dataproc-workflow-step-id label, and in field from other steps. The id must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), and hyphens (-). Cannot begin or end with underscore or hyphen. Must consist of between 3 and 50 characters.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "stepId")]
    pub step_id: Option<String>,
}

/// Job is a Hadoop job.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateForProviderJobsHadoopJob {
    /// HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "archiveUris")]
    pub archive_uris: Option<Vec<String>>,
    /// The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub args: Option<Vec<String>>,
    /// HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fileUris")]
    pub file_uris: Option<Vec<String>>,
    /// HCFS URIs of jar files to be added to the Spark CLASSPATH.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jarFileUris")]
    pub jar_file_uris: Option<Vec<String>>,
    /// The runtime log config for job execution.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "loggingConfig")]
    pub logging_config: Option<WorkflowTemplateForProviderJobsHadoopJobLoggingConfig>,
    /// The name of the driver's main class. The jar file that contains the class must be in the default CLASSPATH or specified in jar_file_uris.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mainClass")]
    pub main_class: Option<String>,
    /// The HCFS URI of the jar file that contains the main class.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mainJarFileUri")]
    pub main_jar_file_uri: Option<String>,
    /// A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub properties: Option<HashMap<String, String>>,
}

/// The runtime log config for job execution.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateForProviderJobsHadoopJobLoggingConfig {
    /// The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "driverLogLevels")]
    pub driver_log_levels: Option<HashMap<String, String>>,
}

/// Job is a Hive job.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateForProviderJobsHiveJob {
    /// Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "continueOnFailure")]
    pub continue_on_failure: Option<bool>,
    /// HCFS URIs of jar files to be added to the Spark CLASSPATH.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jarFileUris")]
    pub jar_file_uris: Option<Vec<String>>,
    /// A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub properties: Option<HashMap<String, String>>,
    /// The HCFS URI of the script that contains SQL queries.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryFileUri")]
    pub query_file_uri: Option<String>,
    /// A list of queries.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryList")]
    pub query_list: Option<WorkflowTemplateForProviderJobsHiveJobQueryList>,
    /// Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "scriptVariables")]
    pub script_variables: Option<HashMap<String, String>>,
}

/// A list of queries.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateForProviderJobsHiveJobQueryList {
    /// Required. The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": } }
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub queries: Option<Vec<String>>,
}

/// Job is a Pig job.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateForProviderJobsPigJob {
    /// Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "continueOnFailure")]
    pub continue_on_failure: Option<bool>,
    /// HCFS URIs of jar files to be added to the Spark CLASSPATH.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jarFileUris")]
    pub jar_file_uris: Option<Vec<String>>,
    /// The runtime log config for job execution.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "loggingConfig")]
    pub logging_config: Option<WorkflowTemplateForProviderJobsPigJobLoggingConfig>,
    /// A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub properties: Option<HashMap<String, String>>,
    /// The HCFS URI of the script that contains SQL queries.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryFileUri")]
    pub query_file_uri: Option<String>,
    /// A list of queries.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryList")]
    pub query_list: Option<WorkflowTemplateForProviderJobsPigJobQueryList>,
    /// Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "scriptVariables")]
    pub script_variables: Option<HashMap<String, String>>,
}

/// The runtime log config for job execution.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateForProviderJobsPigJobLoggingConfig {
    /// The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "driverLogLevels")]
    pub driver_log_levels: Option<HashMap<String, String>>,
}

/// A list of queries.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateForProviderJobsPigJobQueryList {
    /// Required. The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": } }
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub queries: Option<Vec<String>>,
}

/// Job is a Presto job.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateForProviderJobsPrestoJob {
    /// Presto client tags to attach to this query
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clientTags")]
    pub client_tags: Option<Vec<String>>,
    /// Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "continueOnFailure")]
    pub continue_on_failure: Option<bool>,
    /// The runtime log config for job execution.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "loggingConfig")]
    pub logging_config: Option<WorkflowTemplateForProviderJobsPrestoJobLoggingConfig>,
    /// The format in which query output will be displayed. See the Presto documentation for supported output formats
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "outputFormat")]
    pub output_format: Option<String>,
    /// A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub properties: Option<HashMap<String, String>>,
    /// The HCFS URI of the script that contains SQL queries.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryFileUri")]
    pub query_file_uri: Option<String>,
    /// A list of queries.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryList")]
    pub query_list: Option<WorkflowTemplateForProviderJobsPrestoJobQueryList>,
}

/// The runtime log config for job execution.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateForProviderJobsPrestoJobLoggingConfig {
    /// The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "driverLogLevels")]
    pub driver_log_levels: Option<HashMap<String, String>>,
}

/// A list of queries.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateForProviderJobsPrestoJobQueryList {
    /// Required. The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": } }
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub queries: Option<Vec<String>>,
}

/// Job is a PySpark job.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateForProviderJobsPysparkJob {
    /// HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "archiveUris")]
    pub archive_uris: Option<Vec<String>>,
    /// The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub args: Option<Vec<String>>,
    /// HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fileUris")]
    pub file_uris: Option<Vec<String>>,
    /// HCFS URIs of jar files to be added to the Spark CLASSPATH.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jarFileUris")]
    pub jar_file_uris: Option<Vec<String>>,
    /// The runtime log config for job execution.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "loggingConfig")]
    pub logging_config: Option<WorkflowTemplateForProviderJobsPysparkJobLoggingConfig>,
    /// Required. The HCFS URI of the main Python file to use as the driver. Must be a .py file.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mainPythonFileUri")]
    pub main_python_file_uri: Option<String>,
    /// A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub properties: Option<HashMap<String, String>>,
    /// HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "pythonFileUris")]
    pub python_file_uris: Option<Vec<String>>,
}

/// The runtime log config for job execution.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateForProviderJobsPysparkJobLoggingConfig {
    /// The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "driverLogLevels")]
    pub driver_log_levels: Option<HashMap<String, String>>,
}

/// Job scheduling configuration.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateForProviderJobsScheduling {
    /// Maximum number of times per hour a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed. A job may be reported as thrashing if driver exits with non-zero code 4 times within 10 minute window. Maximum value is 10.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxFailuresPerHour")]
    pub max_failures_per_hour: Option<f64>,
    /// Maximum number of times in total a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed. Maximum value is 240
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxFailuresTotal")]
    pub max_failures_total: Option<f64>,
}

/// Job is a Spark job.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateForProviderJobsSparkJob {
    /// HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "archiveUris")]
    pub archive_uris: Option<Vec<String>>,
    /// The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub args: Option<Vec<String>>,
    /// HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fileUris")]
    pub file_uris: Option<Vec<String>>,
    /// HCFS URIs of jar files to be added to the Spark CLASSPATH.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jarFileUris")]
    pub jar_file_uris: Option<Vec<String>>,
    /// The runtime log config for job execution.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "loggingConfig")]
    pub logging_config: Option<WorkflowTemplateForProviderJobsSparkJobLoggingConfig>,
    /// The name of the driver's main class. The jar file that contains the class must be in the default CLASSPATH or specified in jar_file_uris.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mainClass")]
    pub main_class: Option<String>,
    /// The HCFS URI of the jar file that contains the main class.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mainJarFileUri")]
    pub main_jar_file_uri: Option<String>,
    /// A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub properties: Option<HashMap<String, String>>,
}

/// The runtime log config for job execution.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateForProviderJobsSparkJobLoggingConfig {
    /// The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "driverLogLevels")]
    pub driver_log_levels: Option<HashMap<String, String>>,
}

/// Job is a SparkR job.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateForProviderJobsSparkRJob {
    /// HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "archiveUris")]
    pub archive_uris: Option<Vec<String>>,
    /// The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub args: Option<Vec<String>>,
    /// HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fileUris")]
    pub file_uris: Option<Vec<String>>,
    /// The runtime log config for job execution.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "loggingConfig")]
    pub logging_config: Option<WorkflowTemplateForProviderJobsSparkRJobLoggingConfig>,
    /// Required. The HCFS URI of the main R file to use as the driver. Must be a .R file.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mainRFileUri")]
    pub main_r_file_uri: Option<String>,
    /// A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub properties: Option<HashMap<String, String>>,
}

/// The runtime log config for job execution.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateForProviderJobsSparkRJobLoggingConfig {
    /// The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "driverLogLevels")]
    pub driver_log_levels: Option<HashMap<String, String>>,
}

/// Job is a SparkSql job.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateForProviderJobsSparkSqlJob {
    /// HCFS URIs of jar files to be added to the Spark CLASSPATH.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jarFileUris")]
    pub jar_file_uris: Option<Vec<String>>,
    /// The runtime log config for job execution.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "loggingConfig")]
    pub logging_config: Option<WorkflowTemplateForProviderJobsSparkSqlJobLoggingConfig>,
    /// A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub properties: Option<HashMap<String, String>>,
    /// The HCFS URI of the script that contains SQL queries.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryFileUri")]
    pub query_file_uri: Option<String>,
    /// A list of queries.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryList")]
    pub query_list: Option<WorkflowTemplateForProviderJobsSparkSqlJobQueryList>,
    /// Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "scriptVariables")]
    pub script_variables: Option<HashMap<String, String>>,
}

/// The runtime log config for job execution.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateForProviderJobsSparkSqlJobLoggingConfig {
    /// The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "driverLogLevels")]
    pub driver_log_levels: Option<HashMap<String, String>>,
}

/// A list of queries.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateForProviderJobsSparkSqlJobQueryList {
    /// Required. The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": } }
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub queries: Option<Vec<String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateForProviderParameters {
    /// Brief description of the parameter. Must not exceed 1024 characters.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub description: Option<String>,
    /// Required. Paths to all fields that the parameter replaces. A field is allowed to appear in at most one parameter's list of field paths. A field path is similar in syntax to a .sparkJob.args
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub fields: Option<Vec<String>>,
    /// Required. Parameter name. The parameter name is used as the key, and paired with the parameter value, which are passed to the template when the template is instantiated. The name must contain only capital letters (A-Z), numbers (0-9), and underscores (_), and must not start with a number. The maximum length is 40 characters.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Validation rules to be applied to this parameter's value.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub validation: Option<WorkflowTemplateForProviderParametersValidation>,
}

/// Validation rules to be applied to this parameter's value.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateForProviderParametersValidation {
    /// Validation based on regular expressions.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub regex: Option<WorkflowTemplateForProviderParametersValidationRegex>,
    /// Required. List of allowed values for the parameter.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<WorkflowTemplateForProviderParametersValidationValues>,
}

/// Validation based on regular expressions.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateForProviderParametersValidationRegex {
    /// Required. RE2 regular expressions used to validate the parameter's value. The value must match the regex in its entirety (substring matches are not sufficient).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub regexes: Option<Vec<String>>,
}

/// Required. List of allowed values for the parameter.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateForProviderParametersValidationValues {
    /// Required. List of allowed values for the parameter.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// Required. WorkflowTemplate scheduling information.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateForProviderPlacement {
    /// A selector that chooses target cluster for jobs based on metadata. The selector is evaluated at the time each job is submitted.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clusterSelector")]
    pub cluster_selector: Option<WorkflowTemplateForProviderPlacementClusterSelector>,
    /// A cluster that is managed by the workflow.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "managedCluster")]
    pub managed_cluster: Option<WorkflowTemplateForProviderPlacementManagedCluster>,
}

/// A selector that chooses target cluster for jobs based on metadata. The selector is evaluated at the time each job is submitted.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateForProviderPlacementClusterSelector {
    /// Required. The cluster labels. Cluster must have all labels to match.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clusterLabels")]
    pub cluster_labels: Option<HashMap<String, String>>,
    /// The zone where the Compute Engine cluster will be located. On a create request, it is required in the "global" region. If omitted in a non-global Dataproc region, the service will pick a zone in the corresponding Compute Engine region. On a get request, zone will always be present. A full URL, partial URI, or short name are valid. Examples: * https://www.googleapis.com/compute/v1/projects/ * us-central1-f
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub zone: Option<String>,
}

/// A cluster that is managed by the workflow.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateForProviderPlacementManagedCluster {
    /// Required. The cluster name prefix. A unique cluster name will be formed by appending a random suffix. The name must contain only lower-case letters (a-z), numbers (0-9), and hyphens (-). Must begin with a letter. Cannot begin or end with hyphen. Must consist of between 2 and 35 characters.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clusterName")]
    pub cluster_name: Option<String>,
    /// Required. The cluster configuration.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub config: Option<WorkflowTemplateForProviderPlacementManagedClusterConfig>,
    /// The labels to associate with this cluster. Label keys must be between 1 and 63 characters long, and must conform to the following PCRE regular expression: {0,63} No more than 32 labels can be associated with a given cluster.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub labels: Option<HashMap<String, String>>,
}

/// Required. The cluster configuration.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateForProviderPlacementManagedClusterConfig {
    /// Autoscaling config for the policy associated with the cluster. Cluster does not autoscale if this field is unset.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "autoscalingConfig")]
    pub autoscaling_config: Option<WorkflowTemplateForProviderPlacementManagedClusterConfigAutoscalingConfig>,
    /// Encryption settings for the cluster.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "encryptionConfig")]
    pub encryption_config: Option<WorkflowTemplateForProviderPlacementManagedClusterConfigEncryptionConfig>,
    /// Port/endpoint configuration for this cluster
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "endpointConfig")]
    pub endpoint_config: Option<WorkflowTemplateForProviderPlacementManagedClusterConfigEndpointConfig>,
    /// The shared Compute Engine config settings for all instances in a cluster.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gceClusterConfig")]
    pub gce_cluster_config: Option<WorkflowTemplateForProviderPlacementManagedClusterConfigGceClusterConfig>,
    /// Commands to execute on each node after config is completed. By default, executables are run on master and all worker nodes. You can test a node's role metadata to run an executable on a master or worker node, as shown below using curl (you can also use wget): ROLE=$(curl -H Metadata-Flavor:Google http://metadata/computeMetadata/v1/instance/attributes/dataproc-role) if ; then ... master specific actions ... else ... worker specific actions ... fi
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initializationActions")]
    pub initialization_actions: Option<Vec<WorkflowTemplateForProviderPlacementManagedClusterConfigInitializationActions>>,
    /// Lifecycle setting for the cluster.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "lifecycleConfig")]
    pub lifecycle_config: Option<WorkflowTemplateForProviderPlacementManagedClusterConfigLifecycleConfig>,
    /// The Compute Engine config settings for additional worker instances in a cluster.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "masterConfig")]
    pub master_config: Option<WorkflowTemplateForProviderPlacementManagedClusterConfigMasterConfig>,
    /// The Compute Engine config settings for additional worker instances in a cluster.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secondaryWorkerConfig")]
    pub secondary_worker_config: Option<WorkflowTemplateForProviderPlacementManagedClusterConfigSecondaryWorkerConfig>,
    /// Security settings for the cluster.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "securityConfig")]
    pub security_config: Option<WorkflowTemplateForProviderPlacementManagedClusterConfigSecurityConfig>,
    /// The config settings for software inside the cluster.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "softwareConfig")]
    pub software_config: Option<WorkflowTemplateForProviderPlacementManagedClusterConfigSoftwareConfig>,
    /// A Cloud Storage bucket used to stage job dependencies, config files, and job driver console output. If you do not specify a staging bucket, Cloud Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's staging bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket (see (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "stagingBucket")]
    pub staging_bucket: Option<String>,
    /// A Cloud Storage bucket used to store ephemeral cluster and jobs data, such as Spark and MapReduce history files. If you do not specify a temp bucket, Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's temp bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket. The default bucket has a TTL of 90 days, but you can use any TTL (or none) if you specify a bucket.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tempBucket")]
    pub temp_bucket: Option<String>,
    /// The Compute Engine config settings for additional worker instances in a cluster.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "workerConfig")]
    pub worker_config: Option<WorkflowTemplateForProviderPlacementManagedClusterConfigWorkerConfig>,
}

/// Autoscaling config for the policy associated with the cluster. Cluster does not autoscale if this field is unset.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateForProviderPlacementManagedClusterConfigAutoscalingConfig {
    /// The autoscaling policy used by the cluster. Only resource names including projectid and location (region) are valid. Examples: * https://www.googleapis.com/compute/v1/projects/ Note that the policy must be in the same project and Dataproc region.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub policy: Option<String>,
}

/// Encryption settings for the cluster.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateForProviderPlacementManagedClusterConfigEncryptionConfig {
    /// The Cloud KMS key name to use for PD disk encryption for all instances in the cluster.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gcePdKmsKeyName")]
    pub gce_pd_kms_key_name: Option<String>,
}

/// Port/endpoint configuration for this cluster
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateForProviderPlacementManagedClusterConfigEndpointConfig {
    /// If true, enable http access to specific ports on the cluster from external sources. Defaults to false.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableHttpPortAccess")]
    pub enable_http_port_access: Option<bool>,
}

/// The shared Compute Engine config settings for all instances in a cluster.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateForProviderPlacementManagedClusterConfigGceClusterConfig {
    /// If true, all instances in the cluster will only have internal IP addresses. By default, clusters are not restricted to internal IP addresses, and will have ephemeral external IP addresses assigned to each instance. This internal_ip_only restriction can only be enabled for subnetwork enabled networks, and all off-cluster dependencies must be configured to be accessible without external IP addresses.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "internalIpOnly")]
    pub internal_ip_only: Option<bool>,
    /// The Compute Engine metadata entries to add to all instances (see (https://cloud.google.com/compute/docs/storing-retrieving-metadata#project_and_instance_metadata)).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub metadata: Option<HashMap<String, String>>,
    /// The Compute Engine network to be used for machine communications. Cannot be specified with subnetwork_uri. If neither network_uri nor subnetwork_uri is specified, the "default" network of the project is used, if it exists. Cannot be a "Custom Subnet Network" (see /regions/global/default*default`
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub network: Option<String>,
    /// Node Group Affinity for sole-tenant clusters.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodeGroupAffinity")]
    pub node_group_affinity: Option<WorkflowTemplateForProviderPlacementManagedClusterConfigGceClusterConfigNodeGroupAffinity>,
    /// The type of IPv6 access for a cluster. Possible values: PRIVATE_IPV6_GOOGLE_ACCESS_UNSPECIFIED, INHERIT_FROM_SUBNETWORK, OUTBOUND, BIDIRECTIONAL
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "privateIpv6GoogleAccess")]
    pub private_ipv6_google_access: Option<String>,
    /// Reservation Affinity for consuming Zonal reservation.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "reservationAffinity")]
    pub reservation_affinity: Option<WorkflowTemplateForProviderPlacementManagedClusterConfigGceClusterConfigReservationAffinity>,
    /// The (https://cloud.google.com/compute/docs/access/service-accounts#default_service_account) is used.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceAccount")]
    pub service_account: Option<String>,
    /// The URIs of service account scopes to be included in Compute Engine instances. The following base set of scopes is always included: * https://www.googleapis.com/auth/cloud.useraccounts.readonly * https://www.googleapis.com/auth/devstorage.read_write * https://www.googleapis.com/auth/logging.write If no scopes are specified, the following defaults are also provided: * https://www.googleapis.com/auth/bigquery * https://www.googleapis.com/auth/bigtable.admin.table * https://www.googleapis.com/auth/bigtable.data * https://www.googleapis.com/auth/devstorage.full_control
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceAccountScopes")]
    pub service_account_scopes: Option<Vec<String>>,
    /// Shielded Instance Config for clusters using Compute Engine Shielded VMs. Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "shieldedInstanceConfig")]
    pub shielded_instance_config: Option<WorkflowTemplateForProviderPlacementManagedClusterConfigGceClusterConfigShieldedInstanceConfig>,
    /// The Compute Engine subnetwork to be used for machine communications. Cannot be specified with network_uri. A full URL, partial URI, or short name are valid. Examples: * https://www.googleapis.com/compute/v1/projects//regions/us-east1/subnetworks/sub0 * sub0
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub subnetwork: Option<String>,
    /// The Compute Engine tags to add to all instances (see (https://cloud.google.com/compute/docs/label-or-tag-resources#tags)).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tags: Option<Vec<String>>,
    /// The zone where the Compute Engine cluster will be located. On a create request, it is required in the "global" region. If omitted in a non-global Dataproc region, the service will pick a zone in the corresponding Compute Engine region. On a get request, zone will always be present. A full URL, partial URI, or short name are valid. Examples: * https://www.googleapis.com/compute/v1/projects/ * us-central1-f
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub zone: Option<String>,
}

/// Node Group Affinity for sole-tenant clusters.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateForProviderPlacementManagedClusterConfigGceClusterConfigNodeGroupAffinity {
    /// Required. The URI of a sole-tenant /zones/us-central1-a/nodeGroups/node-group-1*node-group-1`
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodeGroup")]
    pub node_group: Option<String>,
}

/// Reservation Affinity for consuming Zonal reservation.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateForProviderPlacementManagedClusterConfigGceClusterConfigReservationAffinity {
    /// Type of reservation to consume Possible values: TYPE_UNSPECIFIED, NO_RESERVATION, ANY_RESERVATION, SPECIFIC_RESERVATION
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "consumeReservationType")]
    pub consume_reservation_type: Option<String>,
    /// Corresponds to the label key of reservation resource.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub key: Option<String>,
    /// Required. List of allowed values for the parameter.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// Shielded Instance Config for clusters using Compute Engine Shielded VMs. Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateForProviderPlacementManagedClusterConfigGceClusterConfigShieldedInstanceConfig {
    /// Defines whether instances have Integrity Monitoring enabled.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableIntegrityMonitoring")]
    pub enable_integrity_monitoring: Option<bool>,
    /// Defines whether instances have Secure Boot enabled.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableSecureBoot")]
    pub enable_secure_boot: Option<bool>,
    /// Defines whether instances have the vTPM enabled.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableVtpm")]
    pub enable_vtpm: Option<bool>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateForProviderPlacementManagedClusterConfigInitializationActions {
    /// Required. Cloud Storage URI of executable file.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "executableFile")]
    pub executable_file: Option<String>,
    /// Amount of time executable has to complete. Default is 10 minutes (see JSON representation of (https://developers.google.com/protocol-buffers/docs/proto3#json)). Cluster creation fails with an explanatory error message (the name of the executable that caused the error and the exceeded timeout period) if the executable is not completed at end of the timeout period.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "executionTimeout")]
    pub execution_timeout: Option<String>,
}

/// Lifecycle setting for the cluster.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateForProviderPlacementManagedClusterConfigLifecycleConfig {
    /// The time when cluster will be auto-deleted (see JSON representation of (https://developers.google.com/protocol-buffers/docs/proto3#json)).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "autoDeleteTime")]
    pub auto_delete_time: Option<String>,
    /// The lifetime duration of cluster. The cluster will be auto-deleted at the end of this period. Minimum value is 10 minutes; maximum value is 14 days (see JSON representation of (https://developers.google.com/protocol-buffers/docs/proto3#json)).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "autoDeleteTtl")]
    pub auto_delete_ttl: Option<String>,
    /// The duration to keep the cluster alive while idling (when no jobs are running). Passing this threshold will cause the cluster to be deleted. Minimum value is 5 minutes; maximum value is 14 days (see JSON representation of (https://developers.google.com/protocol-buffers/docs/proto3#json).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "idleDeleteTtl")]
    pub idle_delete_ttl: Option<String>,
}

/// The Compute Engine config settings for additional worker instances in a cluster.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateForProviderPlacementManagedClusterConfigMasterConfig {
    /// The Compute Engine accelerator configuration for these instances.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub accelerators: Option<Vec<WorkflowTemplateForProviderPlacementManagedClusterConfigMasterConfigAccelerators>>,
    /// Disk option config settings.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "diskConfig")]
    pub disk_config: Option<WorkflowTemplateForProviderPlacementManagedClusterConfigMasterConfigDiskConfig>,
    /// The Compute Engine image resource used for cluster instances. The URI can represent an image or image family. Image examples: * https://www.googleapis.com/compute/beta/projects/ If the URI is unspecified, it will be inferred from SoftwareConfig.image_version or the system default.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub image: Option<String>,
    /// The Compute Engine machine type used for cluster instances. A full URL, partial URI, or short name are valid. Examples: * https://www.googleapis.com/compute/v1/projects/(https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the machine type resource, for example, n1-standard-2`.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "machineType")]
    pub machine_type: Option<String>,
    /// Specifies the minimum cpu platform for the Instance Group. See (https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "minCpuPlatform")]
    pub min_cpu_platform: Option<String>,
    /// The number of VM instances in the instance group. For master instance groups, must be set to 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "numInstances")]
    pub num_instances: Option<f64>,
    /// Specifies the preemptibility of the instance group. The default value for master and worker groups is NON_PREEMPTIBLE. This default cannot be changed. The default value for secondary instances is PREEMPTIBLE. Possible values: PREEMPTIBILITY_UNSPECIFIED, NON_PREEMPTIBLE, PREEMPTIBLE
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub preemptibility: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateForProviderPlacementManagedClusterConfigMasterConfigAccelerators {
    /// The number of the accelerator cards of this type exposed to this instance.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "acceleratorCount")]
    pub accelerator_count: Option<f64>,
    /// Full URL, partial URI, or short name of the accelerator type resource to expose to this instance. See (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the accelerator type resource, for example, nvidia-tesla-k80.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "acceleratorType")]
    pub accelerator_type: Option<String>,
}

/// Disk option config settings.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateForProviderPlacementManagedClusterConfigMasterConfigDiskConfig {
    /// Size in GB of the boot disk (default is 500GB).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bootDiskSizeGb")]
    pub boot_disk_size_gb: Option<f64>,
    /// Type of the boot disk (default is "pd-standard"). Valid values: "pd-ssd" (Persistent Disk Solid State Drive) or "pd-standard" (Persistent Disk Hard Disk Drive).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bootDiskType")]
    pub boot_disk_type: Option<String>,
    /// Number of attached SSDs, from 0 to 4 (default is 0). If SSDs are not attached, the boot disk is used to store runtime logs and (https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data. If one or more SSDs are attached, this runtime bulk data is spread across them, and the boot disk contains only basic config and installed binaries.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "numLocalSsds")]
    pub num_local_ssds: Option<f64>,
}

/// The Compute Engine config settings for additional worker instances in a cluster.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateForProviderPlacementManagedClusterConfigSecondaryWorkerConfig {
    /// The Compute Engine accelerator configuration for these instances.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub accelerators: Option<Vec<WorkflowTemplateForProviderPlacementManagedClusterConfigSecondaryWorkerConfigAccelerators>>,
    /// Disk option config settings.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "diskConfig")]
    pub disk_config: Option<WorkflowTemplateForProviderPlacementManagedClusterConfigSecondaryWorkerConfigDiskConfig>,
    /// The Compute Engine image resource used for cluster instances. The URI can represent an image or image family. Image examples: * https://www.googleapis.com/compute/beta/projects/ If the URI is unspecified, it will be inferred from SoftwareConfig.image_version or the system default.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub image: Option<String>,
    /// The Compute Engine machine type used for cluster instances. A full URL, partial URI, or short name are valid. Examples: * https://www.googleapis.com/compute/v1/projects/(https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the machine type resource, for example, n1-standard-2`.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "machineType")]
    pub machine_type: Option<String>,
    /// Specifies the minimum cpu platform for the Instance Group. See (https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "minCpuPlatform")]
    pub min_cpu_platform: Option<String>,
    /// The number of VM instances in the instance group. For master instance groups, must be set to 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "numInstances")]
    pub num_instances: Option<f64>,
    /// Specifies the preemptibility of the instance group. The default value for master and worker groups is NON_PREEMPTIBLE. This default cannot be changed. The default value for secondary instances is PREEMPTIBLE. Possible values: PREEMPTIBILITY_UNSPECIFIED, NON_PREEMPTIBLE, PREEMPTIBLE
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub preemptibility: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateForProviderPlacementManagedClusterConfigSecondaryWorkerConfigAccelerators {
    /// The number of the accelerator cards of this type exposed to this instance.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "acceleratorCount")]
    pub accelerator_count: Option<f64>,
    /// Full URL, partial URI, or short name of the accelerator type resource to expose to this instance. See (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the accelerator type resource, for example, nvidia-tesla-k80.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "acceleratorType")]
    pub accelerator_type: Option<String>,
}

/// Disk option config settings.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateForProviderPlacementManagedClusterConfigSecondaryWorkerConfigDiskConfig {
    /// Size in GB of the boot disk (default is 500GB).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bootDiskSizeGb")]
    pub boot_disk_size_gb: Option<f64>,
    /// Type of the boot disk (default is "pd-standard"). Valid values: "pd-ssd" (Persistent Disk Solid State Drive) or "pd-standard" (Persistent Disk Hard Disk Drive).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bootDiskType")]
    pub boot_disk_type: Option<String>,
    /// Number of attached SSDs, from 0 to 4 (default is 0). If SSDs are not attached, the boot disk is used to store runtime logs and (https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data. If one or more SSDs are attached, this runtime bulk data is spread across them, and the boot disk contains only basic config and installed binaries.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "numLocalSsds")]
    pub num_local_ssds: Option<f64>,
}

/// Security settings for the cluster.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateForProviderPlacementManagedClusterConfigSecurityConfig {
    /// Kerberos related configuration.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kerberosConfig")]
    pub kerberos_config: Option<WorkflowTemplateForProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig>,
}

/// Kerberos related configuration.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateForProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig {
    /// The admin server (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "crossRealmTrustAdminServer")]
    pub cross_realm_trust_admin_server: Option<String>,
    /// The KDC (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "crossRealmTrustKdc")]
    pub cross_realm_trust_kdc: Option<String>,
    /// The remote realm the Dataproc on-cluster KDC will trust, should the user enable cross realm trust.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "crossRealmTrustRealm")]
    pub cross_realm_trust_realm: Option<String>,
    /// The Cloud Storage URI of a KMS encrypted file containing the shared password between the on-cluster Kerberos realm and the remote trusted realm, in a cross realm trust relationship.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "crossRealmTrustSharedPassword")]
    pub cross_realm_trust_shared_password: Option<String>,
    /// Flag to indicate whether to Kerberize the cluster (default: false). Set this field to true to enable Kerberos on a cluster.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableKerberos")]
    pub enable_kerberos: Option<bool>,
    /// The Cloud Storage URI of a KMS encrypted file containing the master key of the KDC database.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kdcDbKey")]
    pub kdc_db_key: Option<String>,
    /// The Cloud Storage URI of a KMS encrypted file containing the password to the user provided key. For the self-signed certificate, this password is generated by Dataproc.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "keyPassword")]
    pub key_password: Option<String>,
    /// The Cloud Storage URI of the keystore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub keystore: Option<String>,
    /// The Cloud Storage URI of a KMS encrypted file containing the password to the user provided keystore. For the self-signed certificate, this password is generated by Dataproc.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "keystorePassword")]
    pub keystore_password: Option<String>,
    /// The uri of the KMS key used to encrypt various sensitive files.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kmsKey")]
    pub kms_key: Option<String>,
    /// The name of the on-cluster Kerberos realm. If not specified, the uppercased domain of hostnames will be the realm.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub realm: Option<String>,
    /// The Cloud Storage URI of a KMS encrypted file containing the root principal password.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "rootPrincipalPassword")]
    pub root_principal_password: Option<String>,
    /// The lifetime of the ticket granting ticket, in hours. If not specified, or user specifies 0, then default value 10 will be used.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tgtLifetimeHours")]
    pub tgt_lifetime_hours: Option<f64>,
    /// The Cloud Storage URI of the truststore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub truststore: Option<String>,
    /// The Cloud Storage URI of a KMS encrypted file containing the password to the user provided truststore. For the self-signed certificate, this password is generated by Dataproc.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "truststorePassword")]
    pub truststore_password: Option<String>,
}

/// The config settings for software inside the cluster.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateForProviderPlacementManagedClusterConfigSoftwareConfig {
    /// The version of software inside the cluster. It must be one of the supported Dataproc Versions, such as "1.2" (including a subminor version, such as "1.2.29"), or the "preview" version. If unspecified, it defaults to the latest Debian version.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "imageVersion")]
    pub image_version: Option<String>,
    /// The set of components to activate on the cluster.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "optionalComponents")]
    pub optional_components: Option<Vec<String>>,
    /// A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub properties: Option<HashMap<String, String>>,
}

/// The Compute Engine config settings for additional worker instances in a cluster.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateForProviderPlacementManagedClusterConfigWorkerConfig {
    /// The Compute Engine accelerator configuration for these instances.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub accelerators: Option<Vec<WorkflowTemplateForProviderPlacementManagedClusterConfigWorkerConfigAccelerators>>,
    /// Disk option config settings.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "diskConfig")]
    pub disk_config: Option<WorkflowTemplateForProviderPlacementManagedClusterConfigWorkerConfigDiskConfig>,
    /// The Compute Engine image resource used for cluster instances. The URI can represent an image or image family. Image examples: * https://www.googleapis.com/compute/beta/projects/ If the URI is unspecified, it will be inferred from SoftwareConfig.image_version or the system default.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub image: Option<String>,
    /// The Compute Engine machine type used for cluster instances. A full URL, partial URI, or short name are valid. Examples: * https://www.googleapis.com/compute/v1/projects/(https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the machine type resource, for example, n1-standard-2`.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "machineType")]
    pub machine_type: Option<String>,
    /// Specifies the minimum cpu platform for the Instance Group. See (https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "minCpuPlatform")]
    pub min_cpu_platform: Option<String>,
    /// The number of VM instances in the instance group. For master instance groups, must be set to 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "numInstances")]
    pub num_instances: Option<f64>,
    /// Specifies the preemptibility of the instance group. The default value for master and worker groups is NON_PREEMPTIBLE. This default cannot be changed. The default value for secondary instances is PREEMPTIBLE. Possible values: PREEMPTIBILITY_UNSPECIFIED, NON_PREEMPTIBLE, PREEMPTIBLE
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub preemptibility: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateForProviderPlacementManagedClusterConfigWorkerConfigAccelerators {
    /// The number of the accelerator cards of this type exposed to this instance.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "acceleratorCount")]
    pub accelerator_count: Option<f64>,
    /// Full URL, partial URI, or short name of the accelerator type resource to expose to this instance. See (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the accelerator type resource, for example, nvidia-tesla-k80.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "acceleratorType")]
    pub accelerator_type: Option<String>,
}

/// Disk option config settings.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateForProviderPlacementManagedClusterConfigWorkerConfigDiskConfig {
    /// Size in GB of the boot disk (default is 500GB).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bootDiskSizeGb")]
    pub boot_disk_size_gb: Option<f64>,
    /// Type of the boot disk (default is "pd-standard"). Valid values: "pd-ssd" (Persistent Disk Solid State Drive) or "pd-standard" (Persistent Disk Hard Disk Drive).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bootDiskType")]
    pub boot_disk_type: Option<String>,
    /// Number of attached SSDs, from 0 to 4 (default is 0). If SSDs are not attached, the boot disk is used to store runtime logs and (https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data. If one or more SSDs are attached, this runtime bulk data is spread across them, and the boot disk contains only basic config and installed binaries.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "numLocalSsds")]
    pub num_local_ssds: Option<f64>,
}

/// THIS IS A BETA FIELD. It will be honored
/// unless the Management Policies feature flag is disabled.
/// InitProvider holds the same fields as ForProvider, with the exception
/// of Identifier and other resource reference fields. The fields that are
/// in InitProvider are merged into ForProvider when the resource is created.
/// The same fields are also added to the terraform ignore_changes hook, to
/// avoid updating them after creation. This is useful for fields that are
/// required on creation, but we do not desire to update them after creation,
/// for example because of an external controller is managing them, like an
/// autoscaler.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateInitProvider {
    /// (Beta only) Optional. Timeout duration for the DAG of jobs. You can use "s", "m", "h", and "d" suffixes for second, minute, hour, and day duration values, respectively. The timeout duration must be from 10 minutes ("10m") to 24 hours ("24h" or "1d"). The timer begins when the first job is submitted. If the workflow is running at the end of the timeout period, any remaining jobs are cancelled, the workflow is ended, and if the workflow was running on a (/dataproc/docs/concepts/workflows/using-workflows#configuring_or_selecting_a_cluster), the cluster is deleted.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dagTimeout")]
    pub dag_timeout: Option<String>,
    /// Required. The Directed Acyclic Graph of Jobs to submit.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub jobs: Option<Vec<WorkflowTemplateInitProviderJobs>>,
    /// The labels to associate with this cluster. Label keys must be between 1 and 63 characters long, and must conform to the following PCRE regular expression: {0,63} No more than 32 labels can be associated with a given cluster.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub labels: Option<HashMap<String, String>>,
    /// Template parameters whose values are substituted into the template. Values for parameters must be provided when the template is instantiated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub parameters: Option<Vec<WorkflowTemplateInitProviderParameters>>,
    /// Required. WorkflowTemplate scheduling information.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub placement: Option<WorkflowTemplateInitProviderPlacement>,
    /// The project for the resource
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub project: Option<String>,
    /// Used to perform a consistent read-modify-write. This field should be left blank for a CreateWorkflowTemplate request. It is required for an UpdateWorkflowTemplate request, and must match the current server version. A typical update template flow would fetch the current template with a GetWorkflowTemplate request, which will return the current template with the version field filled in with the current server version. The user updates other fields in the template, then returns it as part of the UpdateWorkflowTemplate request.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub version: Option<f64>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateInitProviderJobs {
    /// Job is a Hadoop job.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hadoopJob")]
    pub hadoop_job: Option<WorkflowTemplateInitProviderJobsHadoopJob>,
    /// Job is a Hive job.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hiveJob")]
    pub hive_job: Option<WorkflowTemplateInitProviderJobsHiveJob>,
    /// The labels to associate with this job. Label keys must be between 1 and 63 characters long, and must conform to the following regular expression: {0,63} No more than 32 labels can be associated with a given job.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub labels: Option<HashMap<String, String>>,
    /// Job is a Pig job.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "pigJob")]
    pub pig_job: Option<WorkflowTemplateInitProviderJobsPigJob>,
    /// The optional list of prerequisite job step_ids. If not specified, the job will start at the beginning of workflow.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "prerequisiteStepIds")]
    pub prerequisite_step_ids: Option<Vec<String>>,
    /// Job is a Presto job.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "prestoJob")]
    pub presto_job: Option<WorkflowTemplateInitProviderJobsPrestoJob>,
    /// Job is a PySpark job.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "pysparkJob")]
    pub pyspark_job: Option<WorkflowTemplateInitProviderJobsPysparkJob>,
    /// Job scheduling configuration.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheduling: Option<WorkflowTemplateInitProviderJobsScheduling>,
    /// Job is a Spark job.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sparkJob")]
    pub spark_job: Option<WorkflowTemplateInitProviderJobsSparkJob>,
    /// Job is a SparkR job.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sparkRJob")]
    pub spark_r_job: Option<WorkflowTemplateInitProviderJobsSparkRJob>,
    /// Job is a SparkSql job.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sparkSqlJob")]
    pub spark_sql_job: Option<WorkflowTemplateInitProviderJobsSparkSqlJob>,
    /// Required. The step id. The id must be unique among all jobs within the template. The step id is used as prefix for job id, as job goog-dataproc-workflow-step-id label, and in field from other steps. The id must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), and hyphens (-). Cannot begin or end with underscore or hyphen. Must consist of between 3 and 50 characters.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "stepId")]
    pub step_id: Option<String>,
}

/// Job is a Hadoop job.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateInitProviderJobsHadoopJob {
    /// HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "archiveUris")]
    pub archive_uris: Option<Vec<String>>,
    /// The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub args: Option<Vec<String>>,
    /// HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fileUris")]
    pub file_uris: Option<Vec<String>>,
    /// HCFS URIs of jar files to be added to the Spark CLASSPATH.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jarFileUris")]
    pub jar_file_uris: Option<Vec<String>>,
    /// The runtime log config for job execution.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "loggingConfig")]
    pub logging_config: Option<WorkflowTemplateInitProviderJobsHadoopJobLoggingConfig>,
    /// The name of the driver's main class. The jar file that contains the class must be in the default CLASSPATH or specified in jar_file_uris.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mainClass")]
    pub main_class: Option<String>,
    /// The HCFS URI of the jar file that contains the main class.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mainJarFileUri")]
    pub main_jar_file_uri: Option<String>,
    /// A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub properties: Option<HashMap<String, String>>,
}

/// The runtime log config for job execution.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateInitProviderJobsHadoopJobLoggingConfig {
    /// The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "driverLogLevels")]
    pub driver_log_levels: Option<HashMap<String, String>>,
}

/// Job is a Hive job.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateInitProviderJobsHiveJob {
    /// Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "continueOnFailure")]
    pub continue_on_failure: Option<bool>,
    /// HCFS URIs of jar files to be added to the Spark CLASSPATH.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jarFileUris")]
    pub jar_file_uris: Option<Vec<String>>,
    /// A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub properties: Option<HashMap<String, String>>,
    /// The HCFS URI of the script that contains SQL queries.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryFileUri")]
    pub query_file_uri: Option<String>,
    /// A list of queries.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryList")]
    pub query_list: Option<WorkflowTemplateInitProviderJobsHiveJobQueryList>,
    /// Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "scriptVariables")]
    pub script_variables: Option<HashMap<String, String>>,
}

/// A list of queries.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateInitProviderJobsHiveJobQueryList {
    /// Required. The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": } }
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub queries: Option<Vec<String>>,
}

/// Job is a Pig job.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateInitProviderJobsPigJob {
    /// Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "continueOnFailure")]
    pub continue_on_failure: Option<bool>,
    /// HCFS URIs of jar files to be added to the Spark CLASSPATH.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jarFileUris")]
    pub jar_file_uris: Option<Vec<String>>,
    /// The runtime log config for job execution.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "loggingConfig")]
    pub logging_config: Option<WorkflowTemplateInitProviderJobsPigJobLoggingConfig>,
    /// A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub properties: Option<HashMap<String, String>>,
    /// The HCFS URI of the script that contains SQL queries.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryFileUri")]
    pub query_file_uri: Option<String>,
    /// A list of queries.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryList")]
    pub query_list: Option<WorkflowTemplateInitProviderJobsPigJobQueryList>,
    /// Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "scriptVariables")]
    pub script_variables: Option<HashMap<String, String>>,
}

/// The runtime log config for job execution.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateInitProviderJobsPigJobLoggingConfig {
    /// The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "driverLogLevels")]
    pub driver_log_levels: Option<HashMap<String, String>>,
}

/// A list of queries.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateInitProviderJobsPigJobQueryList {
    /// Required. The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": } }
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub queries: Option<Vec<String>>,
}

/// Job is a Presto job.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateInitProviderJobsPrestoJob {
    /// Presto client tags to attach to this query
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clientTags")]
    pub client_tags: Option<Vec<String>>,
    /// Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "continueOnFailure")]
    pub continue_on_failure: Option<bool>,
    /// The runtime log config for job execution.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "loggingConfig")]
    pub logging_config: Option<WorkflowTemplateInitProviderJobsPrestoJobLoggingConfig>,
    /// The format in which query output will be displayed. See the Presto documentation for supported output formats
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "outputFormat")]
    pub output_format: Option<String>,
    /// A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub properties: Option<HashMap<String, String>>,
    /// The HCFS URI of the script that contains SQL queries.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryFileUri")]
    pub query_file_uri: Option<String>,
    /// A list of queries.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryList")]
    pub query_list: Option<WorkflowTemplateInitProviderJobsPrestoJobQueryList>,
}

/// The runtime log config for job execution.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateInitProviderJobsPrestoJobLoggingConfig {
    /// The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "driverLogLevels")]
    pub driver_log_levels: Option<HashMap<String, String>>,
}

/// A list of queries.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateInitProviderJobsPrestoJobQueryList {
    /// Required. The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": } }
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub queries: Option<Vec<String>>,
}

/// Job is a PySpark job.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateInitProviderJobsPysparkJob {
    /// HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "archiveUris")]
    pub archive_uris: Option<Vec<String>>,
    /// The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub args: Option<Vec<String>>,
    /// HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fileUris")]
    pub file_uris: Option<Vec<String>>,
    /// HCFS URIs of jar files to be added to the Spark CLASSPATH.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jarFileUris")]
    pub jar_file_uris: Option<Vec<String>>,
    /// The runtime log config for job execution.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "loggingConfig")]
    pub logging_config: Option<WorkflowTemplateInitProviderJobsPysparkJobLoggingConfig>,
    /// Required. The HCFS URI of the main Python file to use as the driver. Must be a .py file.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mainPythonFileUri")]
    pub main_python_file_uri: Option<String>,
    /// A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub properties: Option<HashMap<String, String>>,
    /// HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "pythonFileUris")]
    pub python_file_uris: Option<Vec<String>>,
}

/// The runtime log config for job execution.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateInitProviderJobsPysparkJobLoggingConfig {
    /// The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "driverLogLevels")]
    pub driver_log_levels: Option<HashMap<String, String>>,
}

/// Job scheduling configuration.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateInitProviderJobsScheduling {
    /// Maximum number of times per hour a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed. A job may be reported as thrashing if driver exits with non-zero code 4 times within 10 minute window. Maximum value is 10.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxFailuresPerHour")]
    pub max_failures_per_hour: Option<f64>,
    /// Maximum number of times in total a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed. Maximum value is 240
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxFailuresTotal")]
    pub max_failures_total: Option<f64>,
}

/// Job is a Spark job.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateInitProviderJobsSparkJob {
    /// HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "archiveUris")]
    pub archive_uris: Option<Vec<String>>,
    /// The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub args: Option<Vec<String>>,
    /// HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fileUris")]
    pub file_uris: Option<Vec<String>>,
    /// HCFS URIs of jar files to be added to the Spark CLASSPATH.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jarFileUris")]
    pub jar_file_uris: Option<Vec<String>>,
    /// The runtime log config for job execution.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "loggingConfig")]
    pub logging_config: Option<WorkflowTemplateInitProviderJobsSparkJobLoggingConfig>,
    /// The name of the driver's main class. The jar file that contains the class must be in the default CLASSPATH or specified in jar_file_uris.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mainClass")]
    pub main_class: Option<String>,
    /// The HCFS URI of the jar file that contains the main class.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mainJarFileUri")]
    pub main_jar_file_uri: Option<String>,
    /// A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub properties: Option<HashMap<String, String>>,
}

/// The runtime log config for job execution.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateInitProviderJobsSparkJobLoggingConfig {
    /// The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "driverLogLevels")]
    pub driver_log_levels: Option<HashMap<String, String>>,
}

/// Job is a SparkR job.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateInitProviderJobsSparkRJob {
    /// HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "archiveUris")]
    pub archive_uris: Option<Vec<String>>,
    /// The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub args: Option<Vec<String>>,
    /// HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fileUris")]
    pub file_uris: Option<Vec<String>>,
    /// The runtime log config for job execution.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "loggingConfig")]
    pub logging_config: Option<WorkflowTemplateInitProviderJobsSparkRJobLoggingConfig>,
    /// Required. The HCFS URI of the main R file to use as the driver. Must be a .R file.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mainRFileUri")]
    pub main_r_file_uri: Option<String>,
    /// A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub properties: Option<HashMap<String, String>>,
}

/// The runtime log config for job execution.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateInitProviderJobsSparkRJobLoggingConfig {
    /// The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "driverLogLevels")]
    pub driver_log_levels: Option<HashMap<String, String>>,
}

/// Job is a SparkSql job.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateInitProviderJobsSparkSqlJob {
    /// HCFS URIs of jar files to be added to the Spark CLASSPATH.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jarFileUris")]
    pub jar_file_uris: Option<Vec<String>>,
    /// The runtime log config for job execution.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "loggingConfig")]
    pub logging_config: Option<WorkflowTemplateInitProviderJobsSparkSqlJobLoggingConfig>,
    /// A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub properties: Option<HashMap<String, String>>,
    /// The HCFS URI of the script that contains SQL queries.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryFileUri")]
    pub query_file_uri: Option<String>,
    /// A list of queries.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryList")]
    pub query_list: Option<WorkflowTemplateInitProviderJobsSparkSqlJobQueryList>,
    /// Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "scriptVariables")]
    pub script_variables: Option<HashMap<String, String>>,
}

/// The runtime log config for job execution.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateInitProviderJobsSparkSqlJobLoggingConfig {
    /// The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "driverLogLevels")]
    pub driver_log_levels: Option<HashMap<String, String>>,
}

/// A list of queries.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateInitProviderJobsSparkSqlJobQueryList {
    /// Required. The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": } }
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub queries: Option<Vec<String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateInitProviderParameters {
    /// Brief description of the parameter. Must not exceed 1024 characters.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub description: Option<String>,
    /// Required. Paths to all fields that the parameter replaces. A field is allowed to appear in at most one parameter's list of field paths. A field path is similar in syntax to a .sparkJob.args
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub fields: Option<Vec<String>>,
    /// Required. Parameter name. The parameter name is used as the key, and paired with the parameter value, which are passed to the template when the template is instantiated. The name must contain only capital letters (A-Z), numbers (0-9), and underscores (_), and must not start with a number. The maximum length is 40 characters.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Validation rules to be applied to this parameter's value.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub validation: Option<WorkflowTemplateInitProviderParametersValidation>,
}

/// Validation rules to be applied to this parameter's value.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateInitProviderParametersValidation {
    /// Validation based on regular expressions.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub regex: Option<WorkflowTemplateInitProviderParametersValidationRegex>,
    /// Required. List of allowed values for the parameter.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<WorkflowTemplateInitProviderParametersValidationValues>,
}

/// Validation based on regular expressions.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateInitProviderParametersValidationRegex {
    /// Required. RE2 regular expressions used to validate the parameter's value. The value must match the regex in its entirety (substring matches are not sufficient).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub regexes: Option<Vec<String>>,
}

/// Required. List of allowed values for the parameter.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateInitProviderParametersValidationValues {
    /// Required. List of allowed values for the parameter.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// Required. WorkflowTemplate scheduling information.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateInitProviderPlacement {
    /// A selector that chooses target cluster for jobs based on metadata. The selector is evaluated at the time each job is submitted.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clusterSelector")]
    pub cluster_selector: Option<WorkflowTemplateInitProviderPlacementClusterSelector>,
    /// A cluster that is managed by the workflow.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "managedCluster")]
    pub managed_cluster: Option<WorkflowTemplateInitProviderPlacementManagedCluster>,
}

/// A selector that chooses target cluster for jobs based on metadata. The selector is evaluated at the time each job is submitted.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateInitProviderPlacementClusterSelector {
    /// Required. The cluster labels. Cluster must have all labels to match.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clusterLabels")]
    pub cluster_labels: Option<HashMap<String, String>>,
    /// The zone where the Compute Engine cluster will be located. On a create request, it is required in the "global" region. If omitted in a non-global Dataproc region, the service will pick a zone in the corresponding Compute Engine region. On a get request, zone will always be present. A full URL, partial URI, or short name are valid. Examples: * https://www.googleapis.com/compute/v1/projects/ * us-central1-f
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub zone: Option<String>,
}

/// A cluster that is managed by the workflow.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateInitProviderPlacementManagedCluster {
    /// Required. The cluster name prefix. A unique cluster name will be formed by appending a random suffix. The name must contain only lower-case letters (a-z), numbers (0-9), and hyphens (-). Must begin with a letter. Cannot begin or end with hyphen. Must consist of between 2 and 35 characters.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clusterName")]
    pub cluster_name: Option<String>,
    /// Required. The cluster configuration.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub config: Option<WorkflowTemplateInitProviderPlacementManagedClusterConfig>,
    /// The labels to associate with this cluster. Label keys must be between 1 and 63 characters long, and must conform to the following PCRE regular expression: {0,63} No more than 32 labels can be associated with a given cluster.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub labels: Option<HashMap<String, String>>,
}

/// Required. The cluster configuration.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateInitProviderPlacementManagedClusterConfig {
    /// Autoscaling config for the policy associated with the cluster. Cluster does not autoscale if this field is unset.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "autoscalingConfig")]
    pub autoscaling_config: Option<WorkflowTemplateInitProviderPlacementManagedClusterConfigAutoscalingConfig>,
    /// Encryption settings for the cluster.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "encryptionConfig")]
    pub encryption_config: Option<WorkflowTemplateInitProviderPlacementManagedClusterConfigEncryptionConfig>,
    /// Port/endpoint configuration for this cluster
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "endpointConfig")]
    pub endpoint_config: Option<WorkflowTemplateInitProviderPlacementManagedClusterConfigEndpointConfig>,
    /// The shared Compute Engine config settings for all instances in a cluster.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gceClusterConfig")]
    pub gce_cluster_config: Option<WorkflowTemplateInitProviderPlacementManagedClusterConfigGceClusterConfig>,
    /// Commands to execute on each node after config is completed. By default, executables are run on master and all worker nodes. You can test a node's role metadata to run an executable on a master or worker node, as shown below using curl (you can also use wget): ROLE=$(curl -H Metadata-Flavor:Google http://metadata/computeMetadata/v1/instance/attributes/dataproc-role) if ; then ... master specific actions ... else ... worker specific actions ... fi
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initializationActions")]
    pub initialization_actions: Option<Vec<WorkflowTemplateInitProviderPlacementManagedClusterConfigInitializationActions>>,
    /// Lifecycle setting for the cluster.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "lifecycleConfig")]
    pub lifecycle_config: Option<WorkflowTemplateInitProviderPlacementManagedClusterConfigLifecycleConfig>,
    /// The Compute Engine config settings for additional worker instances in a cluster.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "masterConfig")]
    pub master_config: Option<WorkflowTemplateInitProviderPlacementManagedClusterConfigMasterConfig>,
    /// The Compute Engine config settings for additional worker instances in a cluster.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secondaryWorkerConfig")]
    pub secondary_worker_config: Option<WorkflowTemplateInitProviderPlacementManagedClusterConfigSecondaryWorkerConfig>,
    /// Security settings for the cluster.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "securityConfig")]
    pub security_config: Option<WorkflowTemplateInitProviderPlacementManagedClusterConfigSecurityConfig>,
    /// The config settings for software inside the cluster.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "softwareConfig")]
    pub software_config: Option<WorkflowTemplateInitProviderPlacementManagedClusterConfigSoftwareConfig>,
    /// A Cloud Storage bucket used to stage job dependencies, config files, and job driver console output. If you do not specify a staging bucket, Cloud Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's staging bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket (see (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "stagingBucket")]
    pub staging_bucket: Option<String>,
    /// A Cloud Storage bucket used to store ephemeral cluster and jobs data, such as Spark and MapReduce history files. If you do not specify a temp bucket, Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's temp bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket. The default bucket has a TTL of 90 days, but you can use any TTL (or none) if you specify a bucket.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tempBucket")]
    pub temp_bucket: Option<String>,
    /// The Compute Engine config settings for additional worker instances in a cluster.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "workerConfig")]
    pub worker_config: Option<WorkflowTemplateInitProviderPlacementManagedClusterConfigWorkerConfig>,
}

/// Autoscaling config for the policy associated with the cluster. Cluster does not autoscale if this field is unset.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateInitProviderPlacementManagedClusterConfigAutoscalingConfig {
    /// The autoscaling policy used by the cluster. Only resource names including projectid and location (region) are valid. Examples: * https://www.googleapis.com/compute/v1/projects/ Note that the policy must be in the same project and Dataproc region.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub policy: Option<String>,
}

/// Encryption settings for the cluster.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateInitProviderPlacementManagedClusterConfigEncryptionConfig {
    /// The Cloud KMS key name to use for PD disk encryption for all instances in the cluster.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gcePdKmsKeyName")]
    pub gce_pd_kms_key_name: Option<String>,
}

/// Port/endpoint configuration for this cluster
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateInitProviderPlacementManagedClusterConfigEndpointConfig {
    /// If true, enable http access to specific ports on the cluster from external sources. Defaults to false.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableHttpPortAccess")]
    pub enable_http_port_access: Option<bool>,
}

/// The shared Compute Engine config settings for all instances in a cluster.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateInitProviderPlacementManagedClusterConfigGceClusterConfig {
    /// If true, all instances in the cluster will only have internal IP addresses. By default, clusters are not restricted to internal IP addresses, and will have ephemeral external IP addresses assigned to each instance. This internal_ip_only restriction can only be enabled for subnetwork enabled networks, and all off-cluster dependencies must be configured to be accessible without external IP addresses.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "internalIpOnly")]
    pub internal_ip_only: Option<bool>,
    /// The Compute Engine metadata entries to add to all instances (see (https://cloud.google.com/compute/docs/storing-retrieving-metadata#project_and_instance_metadata)).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub metadata: Option<HashMap<String, String>>,
    /// The Compute Engine network to be used for machine communications. Cannot be specified with subnetwork_uri. If neither network_uri nor subnetwork_uri is specified, the "default" network of the project is used, if it exists. Cannot be a "Custom Subnet Network" (see /regions/global/default*default`
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub network: Option<String>,
    /// Node Group Affinity for sole-tenant clusters.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodeGroupAffinity")]
    pub node_group_affinity: Option<WorkflowTemplateInitProviderPlacementManagedClusterConfigGceClusterConfigNodeGroupAffinity>,
    /// The type of IPv6 access for a cluster. Possible values: PRIVATE_IPV6_GOOGLE_ACCESS_UNSPECIFIED, INHERIT_FROM_SUBNETWORK, OUTBOUND, BIDIRECTIONAL
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "privateIpv6GoogleAccess")]
    pub private_ipv6_google_access: Option<String>,
    /// Reservation Affinity for consuming Zonal reservation.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "reservationAffinity")]
    pub reservation_affinity: Option<WorkflowTemplateInitProviderPlacementManagedClusterConfigGceClusterConfigReservationAffinity>,
    /// The (https://cloud.google.com/compute/docs/access/service-accounts#default_service_account) is used.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceAccount")]
    pub service_account: Option<String>,
    /// The URIs of service account scopes to be included in Compute Engine instances. The following base set of scopes is always included: * https://www.googleapis.com/auth/cloud.useraccounts.readonly * https://www.googleapis.com/auth/devstorage.read_write * https://www.googleapis.com/auth/logging.write If no scopes are specified, the following defaults are also provided: * https://www.googleapis.com/auth/bigquery * https://www.googleapis.com/auth/bigtable.admin.table * https://www.googleapis.com/auth/bigtable.data * https://www.googleapis.com/auth/devstorage.full_control
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceAccountScopes")]
    pub service_account_scopes: Option<Vec<String>>,
    /// Shielded Instance Config for clusters using Compute Engine Shielded VMs. Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "shieldedInstanceConfig")]
    pub shielded_instance_config: Option<WorkflowTemplateInitProviderPlacementManagedClusterConfigGceClusterConfigShieldedInstanceConfig>,
    /// The Compute Engine subnetwork to be used for machine communications. Cannot be specified with network_uri. A full URL, partial URI, or short name are valid. Examples: * https://www.googleapis.com/compute/v1/projects//regions/us-east1/subnetworks/sub0 * sub0
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub subnetwork: Option<String>,
    /// The Compute Engine tags to add to all instances (see (https://cloud.google.com/compute/docs/label-or-tag-resources#tags)).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tags: Option<Vec<String>>,
    /// The zone where the Compute Engine cluster will be located. On a create request, it is required in the "global" region. If omitted in a non-global Dataproc region, the service will pick a zone in the corresponding Compute Engine region. On a get request, zone will always be present. A full URL, partial URI, or short name are valid. Examples: * https://www.googleapis.com/compute/v1/projects/ * us-central1-f
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub zone: Option<String>,
}

/// Node Group Affinity for sole-tenant clusters.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateInitProviderPlacementManagedClusterConfigGceClusterConfigNodeGroupAffinity {
    /// Required. The URI of a sole-tenant /zones/us-central1-a/nodeGroups/node-group-1*node-group-1`
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodeGroup")]
    pub node_group: Option<String>,
}

/// Reservation Affinity for consuming Zonal reservation.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateInitProviderPlacementManagedClusterConfigGceClusterConfigReservationAffinity {
    /// Type of reservation to consume Possible values: TYPE_UNSPECIFIED, NO_RESERVATION, ANY_RESERVATION, SPECIFIC_RESERVATION
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "consumeReservationType")]
    pub consume_reservation_type: Option<String>,
    /// Corresponds to the label key of reservation resource.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub key: Option<String>,
    /// Required. List of allowed values for the parameter.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// Shielded Instance Config for clusters using Compute Engine Shielded VMs. Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateInitProviderPlacementManagedClusterConfigGceClusterConfigShieldedInstanceConfig {
    /// Defines whether instances have Integrity Monitoring enabled.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableIntegrityMonitoring")]
    pub enable_integrity_monitoring: Option<bool>,
    /// Defines whether instances have Secure Boot enabled.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableSecureBoot")]
    pub enable_secure_boot: Option<bool>,
    /// Defines whether instances have the vTPM enabled.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableVtpm")]
    pub enable_vtpm: Option<bool>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateInitProviderPlacementManagedClusterConfigInitializationActions {
    /// Required. Cloud Storage URI of executable file.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "executableFile")]
    pub executable_file: Option<String>,
    /// Amount of time executable has to complete. Default is 10 minutes (see JSON representation of (https://developers.google.com/protocol-buffers/docs/proto3#json)). Cluster creation fails with an explanatory error message (the name of the executable that caused the error and the exceeded timeout period) if the executable is not completed at end of the timeout period.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "executionTimeout")]
    pub execution_timeout: Option<String>,
}

/// Lifecycle setting for the cluster.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateInitProviderPlacementManagedClusterConfigLifecycleConfig {
    /// The time when cluster will be auto-deleted (see JSON representation of (https://developers.google.com/protocol-buffers/docs/proto3#json)).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "autoDeleteTime")]
    pub auto_delete_time: Option<String>,
    /// The lifetime duration of cluster. The cluster will be auto-deleted at the end of this period. Minimum value is 10 minutes; maximum value is 14 days (see JSON representation of (https://developers.google.com/protocol-buffers/docs/proto3#json)).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "autoDeleteTtl")]
    pub auto_delete_ttl: Option<String>,
    /// The duration to keep the cluster alive while idling (when no jobs are running). Passing this threshold will cause the cluster to be deleted. Minimum value is 5 minutes; maximum value is 14 days (see JSON representation of (https://developers.google.com/protocol-buffers/docs/proto3#json).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "idleDeleteTtl")]
    pub idle_delete_ttl: Option<String>,
}

/// The Compute Engine config settings for additional worker instances in a cluster.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateInitProviderPlacementManagedClusterConfigMasterConfig {
    /// The Compute Engine accelerator configuration for these instances.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub accelerators: Option<Vec<WorkflowTemplateInitProviderPlacementManagedClusterConfigMasterConfigAccelerators>>,
    /// Disk option config settings.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "diskConfig")]
    pub disk_config: Option<WorkflowTemplateInitProviderPlacementManagedClusterConfigMasterConfigDiskConfig>,
    /// The Compute Engine image resource used for cluster instances. The URI can represent an image or image family. Image examples: * https://www.googleapis.com/compute/beta/projects/ If the URI is unspecified, it will be inferred from SoftwareConfig.image_version or the system default.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub image: Option<String>,
    /// The Compute Engine machine type used for cluster instances. A full URL, partial URI, or short name are valid. Examples: * https://www.googleapis.com/compute/v1/projects/(https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the machine type resource, for example, n1-standard-2`.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "machineType")]
    pub machine_type: Option<String>,
    /// Specifies the minimum cpu platform for the Instance Group. See (https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "minCpuPlatform")]
    pub min_cpu_platform: Option<String>,
    /// The number of VM instances in the instance group. For master instance groups, must be set to 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "numInstances")]
    pub num_instances: Option<f64>,
    /// Specifies the preemptibility of the instance group. The default value for master and worker groups is NON_PREEMPTIBLE. This default cannot be changed. The default value for secondary instances is PREEMPTIBLE. Possible values: PREEMPTIBILITY_UNSPECIFIED, NON_PREEMPTIBLE, PREEMPTIBLE
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub preemptibility: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateInitProviderPlacementManagedClusterConfigMasterConfigAccelerators {
    /// The number of the accelerator cards of this type exposed to this instance.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "acceleratorCount")]
    pub accelerator_count: Option<f64>,
    /// Full URL, partial URI, or short name of the accelerator type resource to expose to this instance. See (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the accelerator type resource, for example, nvidia-tesla-k80.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "acceleratorType")]
    pub accelerator_type: Option<String>,
}

/// Disk option config settings.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateInitProviderPlacementManagedClusterConfigMasterConfigDiskConfig {
    /// Size in GB of the boot disk (default is 500GB).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bootDiskSizeGb")]
    pub boot_disk_size_gb: Option<f64>,
    /// Type of the boot disk (default is "pd-standard"). Valid values: "pd-ssd" (Persistent Disk Solid State Drive) or "pd-standard" (Persistent Disk Hard Disk Drive).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bootDiskType")]
    pub boot_disk_type: Option<String>,
    /// Number of attached SSDs, from 0 to 4 (default is 0). If SSDs are not attached, the boot disk is used to store runtime logs and (https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data. If one or more SSDs are attached, this runtime bulk data is spread across them, and the boot disk contains only basic config and installed binaries.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "numLocalSsds")]
    pub num_local_ssds: Option<f64>,
}

/// The Compute Engine config settings for additional worker instances in a cluster.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateInitProviderPlacementManagedClusterConfigSecondaryWorkerConfig {
    /// The Compute Engine accelerator configuration for these instances.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub accelerators: Option<Vec<WorkflowTemplateInitProviderPlacementManagedClusterConfigSecondaryWorkerConfigAccelerators>>,
    /// Disk option config settings.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "diskConfig")]
    pub disk_config: Option<WorkflowTemplateInitProviderPlacementManagedClusterConfigSecondaryWorkerConfigDiskConfig>,
    /// The Compute Engine image resource used for cluster instances. The URI can represent an image or image family. Image examples: * https://www.googleapis.com/compute/beta/projects/ If the URI is unspecified, it will be inferred from SoftwareConfig.image_version or the system default.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub image: Option<String>,
    /// The Compute Engine machine type used for cluster instances. A full URL, partial URI, or short name are valid. Examples: * https://www.googleapis.com/compute/v1/projects/(https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the machine type resource, for example, n1-standard-2`.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "machineType")]
    pub machine_type: Option<String>,
    /// Specifies the minimum cpu platform for the Instance Group. See (https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "minCpuPlatform")]
    pub min_cpu_platform: Option<String>,
    /// The number of VM instances in the instance group. For master instance groups, must be set to 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "numInstances")]
    pub num_instances: Option<f64>,
    /// Specifies the preemptibility of the instance group. The default value for master and worker groups is NON_PREEMPTIBLE. This default cannot be changed. The default value for secondary instances is PREEMPTIBLE. Possible values: PREEMPTIBILITY_UNSPECIFIED, NON_PREEMPTIBLE, PREEMPTIBLE
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub preemptibility: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateInitProviderPlacementManagedClusterConfigSecondaryWorkerConfigAccelerators {
    /// The number of the accelerator cards of this type exposed to this instance.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "acceleratorCount")]
    pub accelerator_count: Option<f64>,
    /// Full URL, partial URI, or short name of the accelerator type resource to expose to this instance. See (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the accelerator type resource, for example, nvidia-tesla-k80.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "acceleratorType")]
    pub accelerator_type: Option<String>,
}

/// Disk option config settings.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateInitProviderPlacementManagedClusterConfigSecondaryWorkerConfigDiskConfig {
    /// Size in GB of the boot disk (default is 500GB).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bootDiskSizeGb")]
    pub boot_disk_size_gb: Option<f64>,
    /// Type of the boot disk (default is "pd-standard"). Valid values: "pd-ssd" (Persistent Disk Solid State Drive) or "pd-standard" (Persistent Disk Hard Disk Drive).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bootDiskType")]
    pub boot_disk_type: Option<String>,
    /// Number of attached SSDs, from 0 to 4 (default is 0). If SSDs are not attached, the boot disk is used to store runtime logs and (https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data. If one or more SSDs are attached, this runtime bulk data is spread across them, and the boot disk contains only basic config and installed binaries.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "numLocalSsds")]
    pub num_local_ssds: Option<f64>,
}

/// Security settings for the cluster.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateInitProviderPlacementManagedClusterConfigSecurityConfig {
    /// Kerberos related configuration.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kerberosConfig")]
    pub kerberos_config: Option<WorkflowTemplateInitProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig>,
}

/// Kerberos related configuration.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateInitProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig {
    /// The admin server (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "crossRealmTrustAdminServer")]
    pub cross_realm_trust_admin_server: Option<String>,
    /// The KDC (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "crossRealmTrustKdc")]
    pub cross_realm_trust_kdc: Option<String>,
    /// The remote realm the Dataproc on-cluster KDC will trust, should the user enable cross realm trust.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "crossRealmTrustRealm")]
    pub cross_realm_trust_realm: Option<String>,
    /// The Cloud Storage URI of a KMS encrypted file containing the shared password between the on-cluster Kerberos realm and the remote trusted realm, in a cross realm trust relationship.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "crossRealmTrustSharedPassword")]
    pub cross_realm_trust_shared_password: Option<String>,
    /// Flag to indicate whether to Kerberize the cluster (default: false). Set this field to true to enable Kerberos on a cluster.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableKerberos")]
    pub enable_kerberos: Option<bool>,
    /// The Cloud Storage URI of a KMS encrypted file containing the master key of the KDC database.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kdcDbKey")]
    pub kdc_db_key: Option<String>,
    /// The Cloud Storage URI of a KMS encrypted file containing the password to the user provided key. For the self-signed certificate, this password is generated by Dataproc.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "keyPassword")]
    pub key_password: Option<String>,
    /// The Cloud Storage URI of the keystore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub keystore: Option<String>,
    /// The Cloud Storage URI of a KMS encrypted file containing the password to the user provided keystore. For the self-signed certificate, this password is generated by Dataproc.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "keystorePassword")]
    pub keystore_password: Option<String>,
    /// The uri of the KMS key used to encrypt various sensitive files.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kmsKey")]
    pub kms_key: Option<String>,
    /// The name of the on-cluster Kerberos realm. If not specified, the uppercased domain of hostnames will be the realm.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub realm: Option<String>,
    /// The Cloud Storage URI of a KMS encrypted file containing the root principal password.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "rootPrincipalPassword")]
    pub root_principal_password: Option<String>,
    /// The lifetime of the ticket granting ticket, in hours. If not specified, or user specifies 0, then default value 10 will be used.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tgtLifetimeHours")]
    pub tgt_lifetime_hours: Option<f64>,
    /// The Cloud Storage URI of the truststore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub truststore: Option<String>,
    /// The Cloud Storage URI of a KMS encrypted file containing the password to the user provided truststore. For the self-signed certificate, this password is generated by Dataproc.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "truststorePassword")]
    pub truststore_password: Option<String>,
}

/// The config settings for software inside the cluster.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateInitProviderPlacementManagedClusterConfigSoftwareConfig {
    /// The version of software inside the cluster. It must be one of the supported Dataproc Versions, such as "1.2" (including a subminor version, such as "1.2.29"), or the "preview" version. If unspecified, it defaults to the latest Debian version.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "imageVersion")]
    pub image_version: Option<String>,
    /// The set of components to activate on the cluster.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "optionalComponents")]
    pub optional_components: Option<Vec<String>>,
    /// A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub properties: Option<HashMap<String, String>>,
}

/// The Compute Engine config settings for additional worker instances in a cluster.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateInitProviderPlacementManagedClusterConfigWorkerConfig {
    /// The Compute Engine accelerator configuration for these instances.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub accelerators: Option<Vec<WorkflowTemplateInitProviderPlacementManagedClusterConfigWorkerConfigAccelerators>>,
    /// Disk option config settings.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "diskConfig")]
    pub disk_config: Option<WorkflowTemplateInitProviderPlacementManagedClusterConfigWorkerConfigDiskConfig>,
    /// The Compute Engine image resource used for cluster instances. The URI can represent an image or image family. Image examples: * https://www.googleapis.com/compute/beta/projects/ If the URI is unspecified, it will be inferred from SoftwareConfig.image_version or the system default.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub image: Option<String>,
    /// The Compute Engine machine type used for cluster instances. A full URL, partial URI, or short name are valid. Examples: * https://www.googleapis.com/compute/v1/projects/(https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the machine type resource, for example, n1-standard-2`.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "machineType")]
    pub machine_type: Option<String>,
    /// Specifies the minimum cpu platform for the Instance Group. See (https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "minCpuPlatform")]
    pub min_cpu_platform: Option<String>,
    /// The number of VM instances in the instance group. For master instance groups, must be set to 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "numInstances")]
    pub num_instances: Option<f64>,
    /// Specifies the preemptibility of the instance group. The default value for master and worker groups is NON_PREEMPTIBLE. This default cannot be changed. The default value for secondary instances is PREEMPTIBLE. Possible values: PREEMPTIBILITY_UNSPECIFIED, NON_PREEMPTIBLE, PREEMPTIBLE
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub preemptibility: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateInitProviderPlacementManagedClusterConfigWorkerConfigAccelerators {
    /// The number of the accelerator cards of this type exposed to this instance.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "acceleratorCount")]
    pub accelerator_count: Option<f64>,
    /// Full URL, partial URI, or short name of the accelerator type resource to expose to this instance. See (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the accelerator type resource, for example, nvidia-tesla-k80.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "acceleratorType")]
    pub accelerator_type: Option<String>,
}

/// Disk option config settings.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateInitProviderPlacementManagedClusterConfigWorkerConfigDiskConfig {
    /// Size in GB of the boot disk (default is 500GB).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bootDiskSizeGb")]
    pub boot_disk_size_gb: Option<f64>,
    /// Type of the boot disk (default is "pd-standard"). Valid values: "pd-ssd" (Persistent Disk Solid State Drive) or "pd-standard" (Persistent Disk Hard Disk Drive).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bootDiskType")]
    pub boot_disk_type: Option<String>,
    /// Number of attached SSDs, from 0 to 4 (default is 0). If SSDs are not attached, the boot disk is used to store runtime logs and (https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data. If one or more SSDs are attached, this runtime bulk data is spread across them, and the boot disk contains only basic config and installed binaries.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "numLocalSsds")]
    pub num_local_ssds: Option<f64>,
}

/// ProviderConfigReference specifies how the provider that will be used to
/// create, observe, update, and delete this managed resource should be
/// configured.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateProviderConfigRef {
    /// Name of the referenced object.
    pub name: String,
    /// Policies for referencing.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub policy: Option<WorkflowTemplateProviderConfigRefPolicy>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateProviderConfigRefPolicy {
    /// Resolution specifies whether resolution of this reference is required.
    /// The default is 'Required', which means the reconcile will fail if the
    /// reference cannot be resolved. 'Optional' means this reference will be
    /// a no-op if it cannot be resolved.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolution: Option<WorkflowTemplateProviderConfigRefPolicyResolution>,
    /// Resolve specifies when this reference should be resolved. The default
    /// is 'IfNotPresent', which will attempt to resolve the reference only when
    /// the corresponding field is not present. Use 'Always' to resolve the
    /// reference on every reconcile.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolve: Option<WorkflowTemplateProviderConfigRefPolicyResolve>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum WorkflowTemplateProviderConfigRefPolicyResolution {
    Required,
    Optional,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum WorkflowTemplateProviderConfigRefPolicyResolve {
    Always,
    IfNotPresent,
}

/// PublishConnectionDetailsTo specifies the connection secret config which
/// contains a name, metadata and a reference to secret store config to
/// which any connection details for this managed resource should be written.
/// Connection details frequently include the endpoint, username,
/// and password required to connect to the managed resource.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplatePublishConnectionDetailsTo {
    /// SecretStoreConfigRef specifies which secret store config should be used
    /// for this ConnectionSecret.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configRef")]
    pub config_ref: Option<WorkflowTemplatePublishConnectionDetailsToConfigRef>,
    /// Metadata is the metadata for connection secret.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub metadata: Option<WorkflowTemplatePublishConnectionDetailsToMetadata>,
    /// Name is the name of the connection secret.
    pub name: String,
}

/// SecretStoreConfigRef specifies which secret store config should be used
/// for this ConnectionSecret.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplatePublishConnectionDetailsToConfigRef {
    /// Name of the referenced object.
    pub name: String,
    /// Policies for referencing.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub policy: Option<WorkflowTemplatePublishConnectionDetailsToConfigRefPolicy>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplatePublishConnectionDetailsToConfigRefPolicy {
    /// Resolution specifies whether resolution of this reference is required.
    /// The default is 'Required', which means the reconcile will fail if the
    /// reference cannot be resolved. 'Optional' means this reference will be
    /// a no-op if it cannot be resolved.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolution: Option<WorkflowTemplatePublishConnectionDetailsToConfigRefPolicyResolution>,
    /// Resolve specifies when this reference should be resolved. The default
    /// is 'IfNotPresent', which will attempt to resolve the reference only when
    /// the corresponding field is not present. Use 'Always' to resolve the
    /// reference on every reconcile.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolve: Option<WorkflowTemplatePublishConnectionDetailsToConfigRefPolicyResolve>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum WorkflowTemplatePublishConnectionDetailsToConfigRefPolicyResolution {
    Required,
    Optional,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum WorkflowTemplatePublishConnectionDetailsToConfigRefPolicyResolve {
    Always,
    IfNotPresent,
}

/// Metadata is the metadata for connection secret.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplatePublishConnectionDetailsToMetadata {
    /// Annotations are the annotations to be added to connection secret.
    /// - For Kubernetes secrets, this will be used as "metadata.annotations".
    /// - It is up to Secret Store implementation for others store types.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub annotations: Option<HashMap<String, String>>,
    /// Labels are the labels/tags to be added to connection secret.
    /// - For Kubernetes secrets, this will be used as "metadata.labels".
    /// - It is up to Secret Store implementation for others store types.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub labels: Option<HashMap<String, String>>,
    /// Type is the SecretType for the connection secret.
    /// - Only valid for Kubernetes Secret Stores.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<String>,
}

/// WriteConnectionSecretToReference specifies the namespace and name of a
/// Secret to which any connection details for this managed resource should
/// be written. Connection details frequently include the endpoint, username,
/// and password required to connect to the managed resource.
/// This field is planned to be replaced in a future release in favor of
/// PublishConnectionDetailsTo. Currently, both could be set independently
/// and connection details would be published to both without affecting
/// each other.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateWriteConnectionSecretToRef {
    /// Name of the secret.
    pub name: String,
    /// Namespace of the secret.
    pub namespace: String,
}

/// WorkflowTemplateStatus defines the observed state of WorkflowTemplate.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatus {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "atProvider")]
    pub at_provider: Option<WorkflowTemplateStatusAtProvider>,
    /// Conditions of the resource.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub conditions: Option<Vec<Condition>>,
    /// ObservedGeneration is the latest metadata.generation
    /// which resulted in either a ready state, or stalled due to error
    /// it can not recover from without human intervention.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "observedGeneration")]
    pub observed_generation: Option<i64>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProvider {
    /// Output only. The time template was created.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "createTime")]
    pub create_time: Option<String>,
    /// (Beta only) Optional. Timeout duration for the DAG of jobs. You can use "s", "m", "h", and "d" suffixes for second, minute, hour, and day duration values, respectively. The timeout duration must be from 10 minutes ("10m") to 24 hours ("24h" or "1d"). The timer begins when the first job is submitted. If the workflow is running at the end of the timeout period, any remaining jobs are cancelled, the workflow is ended, and if the workflow was running on a (/dataproc/docs/concepts/workflows/using-workflows#configuring_or_selecting_a_cluster), the cluster is deleted.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dagTimeout")]
    pub dag_timeout: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "effectiveLabels")]
    pub effective_labels: Option<HashMap<String, String>>,
    /// an identifier for the resource with format projects/{{project}}/locations/{{location}}/workflowTemplates/{{name}}
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub id: Option<String>,
    /// Required. The Directed Acyclic Graph of Jobs to submit.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub jobs: Option<Vec<WorkflowTemplateStatusAtProviderJobs>>,
    /// The labels to associate with this cluster. Label keys must be between 1 and 63 characters long, and must conform to the following PCRE regular expression: {0,63} No more than 32 labels can be associated with a given cluster.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub labels: Option<HashMap<String, String>>,
    /// The location for the resource
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub location: Option<String>,
    /// Template parameters whose values are substituted into the template. Values for parameters must be provided when the template is instantiated.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub parameters: Option<Vec<WorkflowTemplateStatusAtProviderParameters>>,
    /// Required. WorkflowTemplate scheduling information.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub placement: Option<WorkflowTemplateStatusAtProviderPlacement>,
    /// The project for the resource
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub project: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terraformLabels")]
    pub terraform_labels: Option<HashMap<String, String>>,
    /// Output only. The time template was last updated.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "updateTime")]
    pub update_time: Option<String>,
    /// Used to perform a consistent read-modify-write. This field should be left blank for a CreateWorkflowTemplate request. It is required for an UpdateWorkflowTemplate request, and must match the current server version. A typical update template flow would fetch the current template with a GetWorkflowTemplate request, which will return the current template with the version field filled in with the current server version. The user updates other fields in the template, then returns it as part of the UpdateWorkflowTemplate request.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub version: Option<f64>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderJobs {
    /// Job is a Hadoop job.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hadoopJob")]
    pub hadoop_job: Option<WorkflowTemplateStatusAtProviderJobsHadoopJob>,
    /// Job is a Hive job.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hiveJob")]
    pub hive_job: Option<WorkflowTemplateStatusAtProviderJobsHiveJob>,
    /// The labels to associate with this job. Label keys must be between 1 and 63 characters long, and must conform to the following regular expression: {0,63} No more than 32 labels can be associated with a given job.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub labels: Option<HashMap<String, String>>,
    /// Job is a Pig job.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "pigJob")]
    pub pig_job: Option<WorkflowTemplateStatusAtProviderJobsPigJob>,
    /// The optional list of prerequisite job step_ids. If not specified, the job will start at the beginning of workflow.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "prerequisiteStepIds")]
    pub prerequisite_step_ids: Option<Vec<String>>,
    /// Job is a Presto job.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "prestoJob")]
    pub presto_job: Option<WorkflowTemplateStatusAtProviderJobsPrestoJob>,
    /// Job is a PySpark job.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "pysparkJob")]
    pub pyspark_job: Option<WorkflowTemplateStatusAtProviderJobsPysparkJob>,
    /// Job scheduling configuration.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub scheduling: Option<WorkflowTemplateStatusAtProviderJobsScheduling>,
    /// Job is a Spark job.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sparkJob")]
    pub spark_job: Option<WorkflowTemplateStatusAtProviderJobsSparkJob>,
    /// Job is a SparkR job.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sparkRJob")]
    pub spark_r_job: Option<WorkflowTemplateStatusAtProviderJobsSparkRJob>,
    /// Job is a SparkSql job.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sparkSqlJob")]
    pub spark_sql_job: Option<WorkflowTemplateStatusAtProviderJobsSparkSqlJob>,
    /// Required. The step id. The id must be unique among all jobs within the template. The step id is used as prefix for job id, as job goog-dataproc-workflow-step-id label, and in field from other steps. The id must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), and hyphens (-). Cannot begin or end with underscore or hyphen. Must consist of between 3 and 50 characters.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "stepId")]
    pub step_id: Option<String>,
}

/// Job is a Hadoop job.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderJobsHadoopJob {
    /// HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "archiveUris")]
    pub archive_uris: Option<Vec<String>>,
    /// The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub args: Option<Vec<String>>,
    /// HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fileUris")]
    pub file_uris: Option<Vec<String>>,
    /// HCFS URIs of jar files to be added to the Spark CLASSPATH.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jarFileUris")]
    pub jar_file_uris: Option<Vec<String>>,
    /// The runtime log config for job execution.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "loggingConfig")]
    pub logging_config: Option<WorkflowTemplateStatusAtProviderJobsHadoopJobLoggingConfig>,
    /// The name of the driver's main class. The jar file that contains the class must be in the default CLASSPATH or specified in jar_file_uris.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mainClass")]
    pub main_class: Option<String>,
    /// The HCFS URI of the jar file that contains the main class.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mainJarFileUri")]
    pub main_jar_file_uri: Option<String>,
    /// A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub properties: Option<HashMap<String, String>>,
}

/// The runtime log config for job execution.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderJobsHadoopJobLoggingConfig {
    /// The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "driverLogLevels")]
    pub driver_log_levels: Option<HashMap<String, String>>,
}

/// Job is a Hive job.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderJobsHiveJob {
    /// Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "continueOnFailure")]
    pub continue_on_failure: Option<bool>,
    /// HCFS URIs of jar files to be added to the Spark CLASSPATH.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jarFileUris")]
    pub jar_file_uris: Option<Vec<String>>,
    /// A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub properties: Option<HashMap<String, String>>,
    /// The HCFS URI of the script that contains SQL queries.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryFileUri")]
    pub query_file_uri: Option<String>,
    /// A list of queries.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryList")]
    pub query_list: Option<WorkflowTemplateStatusAtProviderJobsHiveJobQueryList>,
    /// Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "scriptVariables")]
    pub script_variables: Option<HashMap<String, String>>,
}

/// A list of queries.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderJobsHiveJobQueryList {
    /// Required. The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": } }
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub queries: Option<Vec<String>>,
}

/// Job is a Pig job.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderJobsPigJob {
    /// Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "continueOnFailure")]
    pub continue_on_failure: Option<bool>,
    /// HCFS URIs of jar files to be added to the Spark CLASSPATH.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jarFileUris")]
    pub jar_file_uris: Option<Vec<String>>,
    /// The runtime log config for job execution.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "loggingConfig")]
    pub logging_config: Option<WorkflowTemplateStatusAtProviderJobsPigJobLoggingConfig>,
    /// A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub properties: Option<HashMap<String, String>>,
    /// The HCFS URI of the script that contains SQL queries.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryFileUri")]
    pub query_file_uri: Option<String>,
    /// A list of queries.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryList")]
    pub query_list: Option<WorkflowTemplateStatusAtProviderJobsPigJobQueryList>,
    /// Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "scriptVariables")]
    pub script_variables: Option<HashMap<String, String>>,
}

/// The runtime log config for job execution.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderJobsPigJobLoggingConfig {
    /// The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "driverLogLevels")]
    pub driver_log_levels: Option<HashMap<String, String>>,
}

/// A list of queries.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderJobsPigJobQueryList {
    /// Required. The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": } }
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub queries: Option<Vec<String>>,
}

/// Job is a Presto job.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderJobsPrestoJob {
    /// Presto client tags to attach to this query
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clientTags")]
    pub client_tags: Option<Vec<String>>,
    /// Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "continueOnFailure")]
    pub continue_on_failure: Option<bool>,
    /// The runtime log config for job execution.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "loggingConfig")]
    pub logging_config: Option<WorkflowTemplateStatusAtProviderJobsPrestoJobLoggingConfig>,
    /// The format in which query output will be displayed. See the Presto documentation for supported output formats
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "outputFormat")]
    pub output_format: Option<String>,
    /// A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub properties: Option<HashMap<String, String>>,
    /// The HCFS URI of the script that contains SQL queries.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryFileUri")]
    pub query_file_uri: Option<String>,
    /// A list of queries.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryList")]
    pub query_list: Option<WorkflowTemplateStatusAtProviderJobsPrestoJobQueryList>,
}

/// The runtime log config for job execution.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderJobsPrestoJobLoggingConfig {
    /// The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "driverLogLevels")]
    pub driver_log_levels: Option<HashMap<String, String>>,
}

/// A list of queries.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderJobsPrestoJobQueryList {
    /// Required. The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": } }
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub queries: Option<Vec<String>>,
}

/// Job is a PySpark job.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderJobsPysparkJob {
    /// HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "archiveUris")]
    pub archive_uris: Option<Vec<String>>,
    /// The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub args: Option<Vec<String>>,
    /// HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fileUris")]
    pub file_uris: Option<Vec<String>>,
    /// HCFS URIs of jar files to be added to the Spark CLASSPATH.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jarFileUris")]
    pub jar_file_uris: Option<Vec<String>>,
    /// The runtime log config for job execution.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "loggingConfig")]
    pub logging_config: Option<WorkflowTemplateStatusAtProviderJobsPysparkJobLoggingConfig>,
    /// Required. The HCFS URI of the main Python file to use as the driver. Must be a .py file.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mainPythonFileUri")]
    pub main_python_file_uri: Option<String>,
    /// A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub properties: Option<HashMap<String, String>>,
    /// HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "pythonFileUris")]
    pub python_file_uris: Option<Vec<String>>,
}

/// The runtime log config for job execution.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderJobsPysparkJobLoggingConfig {
    /// The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "driverLogLevels")]
    pub driver_log_levels: Option<HashMap<String, String>>,
}

/// Job scheduling configuration.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderJobsScheduling {
    /// Maximum number of times per hour a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed. A job may be reported as thrashing if driver exits with non-zero code 4 times within 10 minute window. Maximum value is 10.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxFailuresPerHour")]
    pub max_failures_per_hour: Option<f64>,
    /// Maximum number of times in total a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed. Maximum value is 240
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxFailuresTotal")]
    pub max_failures_total: Option<f64>,
}

/// Job is a Spark job.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderJobsSparkJob {
    /// HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "archiveUris")]
    pub archive_uris: Option<Vec<String>>,
    /// The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub args: Option<Vec<String>>,
    /// HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fileUris")]
    pub file_uris: Option<Vec<String>>,
    /// HCFS URIs of jar files to be added to the Spark CLASSPATH.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jarFileUris")]
    pub jar_file_uris: Option<Vec<String>>,
    /// The runtime log config for job execution.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "loggingConfig")]
    pub logging_config: Option<WorkflowTemplateStatusAtProviderJobsSparkJobLoggingConfig>,
    /// The name of the driver's main class. The jar file that contains the class must be in the default CLASSPATH or specified in jar_file_uris.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mainClass")]
    pub main_class: Option<String>,
    /// The HCFS URI of the jar file that contains the main class.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mainJarFileUri")]
    pub main_jar_file_uri: Option<String>,
    /// A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub properties: Option<HashMap<String, String>>,
}

/// The runtime log config for job execution.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderJobsSparkJobLoggingConfig {
    /// The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "driverLogLevels")]
    pub driver_log_levels: Option<HashMap<String, String>>,
}

/// Job is a SparkR job.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderJobsSparkRJob {
    /// HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "archiveUris")]
    pub archive_uris: Option<Vec<String>>,
    /// The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub args: Option<Vec<String>>,
    /// HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fileUris")]
    pub file_uris: Option<Vec<String>>,
    /// The runtime log config for job execution.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "loggingConfig")]
    pub logging_config: Option<WorkflowTemplateStatusAtProviderJobsSparkRJobLoggingConfig>,
    /// Required. The HCFS URI of the main R file to use as the driver. Must be a .R file.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mainRFileUri")]
    pub main_r_file_uri: Option<String>,
    /// A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub properties: Option<HashMap<String, String>>,
}

/// The runtime log config for job execution.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderJobsSparkRJobLoggingConfig {
    /// The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "driverLogLevels")]
    pub driver_log_levels: Option<HashMap<String, String>>,
}

/// Job is a SparkSql job.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderJobsSparkSqlJob {
    /// HCFS URIs of jar files to be added to the Spark CLASSPATH.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jarFileUris")]
    pub jar_file_uris: Option<Vec<String>>,
    /// The runtime log config for job execution.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "loggingConfig")]
    pub logging_config: Option<WorkflowTemplateStatusAtProviderJobsSparkSqlJobLoggingConfig>,
    /// A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub properties: Option<HashMap<String, String>>,
    /// The HCFS URI of the script that contains SQL queries.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryFileUri")]
    pub query_file_uri: Option<String>,
    /// A list of queries.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryList")]
    pub query_list: Option<WorkflowTemplateStatusAtProviderJobsSparkSqlJobQueryList>,
    /// Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "scriptVariables")]
    pub script_variables: Option<HashMap<String, String>>,
}

/// The runtime log config for job execution.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderJobsSparkSqlJobLoggingConfig {
    /// The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "driverLogLevels")]
    pub driver_log_levels: Option<HashMap<String, String>>,
}

/// A list of queries.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderJobsSparkSqlJobQueryList {
    /// Required. The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": } }
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub queries: Option<Vec<String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderParameters {
    /// Brief description of the parameter. Must not exceed 1024 characters.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub description: Option<String>,
    /// Required. Paths to all fields that the parameter replaces. A field is allowed to appear in at most one parameter's list of field paths. A field path is similar in syntax to a .sparkJob.args
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub fields: Option<Vec<String>>,
    /// Required. Parameter name. The parameter name is used as the key, and paired with the parameter value, which are passed to the template when the template is instantiated. The name must contain only capital letters (A-Z), numbers (0-9), and underscores (_), and must not start with a number. The maximum length is 40 characters.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Validation rules to be applied to this parameter's value.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub validation: Option<WorkflowTemplateStatusAtProviderParametersValidation>,
}

/// Validation rules to be applied to this parameter's value.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderParametersValidation {
    /// Validation based on regular expressions.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub regex: Option<WorkflowTemplateStatusAtProviderParametersValidationRegex>,
    /// Required. List of allowed values for the parameter.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<WorkflowTemplateStatusAtProviderParametersValidationValues>,
}

/// Validation based on regular expressions.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderParametersValidationRegex {
    /// Required. RE2 regular expressions used to validate the parameter's value. The value must match the regex in its entirety (substring matches are not sufficient).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub regexes: Option<Vec<String>>,
}

/// Required. List of allowed values for the parameter.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderParametersValidationValues {
    /// Required. List of allowed values for the parameter.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// Required. WorkflowTemplate scheduling information.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderPlacement {
    /// A selector that chooses target cluster for jobs based on metadata. The selector is evaluated at the time each job is submitted.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clusterSelector")]
    pub cluster_selector: Option<WorkflowTemplateStatusAtProviderPlacementClusterSelector>,
    /// A cluster that is managed by the workflow.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "managedCluster")]
    pub managed_cluster: Option<WorkflowTemplateStatusAtProviderPlacementManagedCluster>,
}

/// A selector that chooses target cluster for jobs based on metadata. The selector is evaluated at the time each job is submitted.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderPlacementClusterSelector {
    /// Required. The cluster labels. Cluster must have all labels to match.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clusterLabels")]
    pub cluster_labels: Option<HashMap<String, String>>,
    /// The zone where the Compute Engine cluster will be located. On a create request, it is required in the "global" region. If omitted in a non-global Dataproc region, the service will pick a zone in the corresponding Compute Engine region. On a get request, zone will always be present. A full URL, partial URI, or short name are valid. Examples: * https://www.googleapis.com/compute/v1/projects/ * us-central1-f
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub zone: Option<String>,
}

/// A cluster that is managed by the workflow.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderPlacementManagedCluster {
    /// Required. The cluster name prefix. A unique cluster name will be formed by appending a random suffix. The name must contain only lower-case letters (a-z), numbers (0-9), and hyphens (-). Must begin with a letter. Cannot begin or end with hyphen. Must consist of between 2 and 35 characters.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clusterName")]
    pub cluster_name: Option<String>,
    /// Required. The cluster configuration.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub config: Option<WorkflowTemplateStatusAtProviderPlacementManagedClusterConfig>,
    /// The labels to associate with this cluster. Label keys must be between 1 and 63 characters long, and must conform to the following PCRE regular expression: {0,63} No more than 32 labels can be associated with a given cluster.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub labels: Option<HashMap<String, String>>,
}

/// Required. The cluster configuration.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderPlacementManagedClusterConfig {
    /// Autoscaling config for the policy associated with the cluster. Cluster does not autoscale if this field is unset.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "autoscalingConfig")]
    pub autoscaling_config: Option<WorkflowTemplateStatusAtProviderPlacementManagedClusterConfigAutoscalingConfig>,
    /// Encryption settings for the cluster.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "encryptionConfig")]
    pub encryption_config: Option<WorkflowTemplateStatusAtProviderPlacementManagedClusterConfigEncryptionConfig>,
    /// Port/endpoint configuration for this cluster
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "endpointConfig")]
    pub endpoint_config: Option<WorkflowTemplateStatusAtProviderPlacementManagedClusterConfigEndpointConfig>,
    /// The shared Compute Engine config settings for all instances in a cluster.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gceClusterConfig")]
    pub gce_cluster_config: Option<WorkflowTemplateStatusAtProviderPlacementManagedClusterConfigGceClusterConfig>,
    /// Commands to execute on each node after config is completed. By default, executables are run on master and all worker nodes. You can test a node's role metadata to run an executable on a master or worker node, as shown below using curl (you can also use wget): ROLE=$(curl -H Metadata-Flavor:Google http://metadata/computeMetadata/v1/instance/attributes/dataproc-role) if ; then ... master specific actions ... else ... worker specific actions ... fi
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initializationActions")]
    pub initialization_actions: Option<Vec<WorkflowTemplateStatusAtProviderPlacementManagedClusterConfigInitializationActions>>,
    /// Lifecycle setting for the cluster.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "lifecycleConfig")]
    pub lifecycle_config: Option<WorkflowTemplateStatusAtProviderPlacementManagedClusterConfigLifecycleConfig>,
    /// The Compute Engine config settings for additional worker instances in a cluster.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "masterConfig")]
    pub master_config: Option<WorkflowTemplateStatusAtProviderPlacementManagedClusterConfigMasterConfig>,
    /// The Compute Engine config settings for additional worker instances in a cluster.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "secondaryWorkerConfig")]
    pub secondary_worker_config: Option<WorkflowTemplateStatusAtProviderPlacementManagedClusterConfigSecondaryWorkerConfig>,
    /// Security settings for the cluster.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "securityConfig")]
    pub security_config: Option<WorkflowTemplateStatusAtProviderPlacementManagedClusterConfigSecurityConfig>,
    /// The config settings for software inside the cluster.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "softwareConfig")]
    pub software_config: Option<WorkflowTemplateStatusAtProviderPlacementManagedClusterConfigSoftwareConfig>,
    /// A Cloud Storage bucket used to stage job dependencies, config files, and job driver console output. If you do not specify a staging bucket, Cloud Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's staging bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket (see (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "stagingBucket")]
    pub staging_bucket: Option<String>,
    /// A Cloud Storage bucket used to store ephemeral cluster and jobs data, such as Spark and MapReduce history files. If you do not specify a temp bucket, Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's temp bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket. The default bucket has a TTL of 90 days, but you can use any TTL (or none) if you specify a bucket.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tempBucket")]
    pub temp_bucket: Option<String>,
    /// The Compute Engine config settings for additional worker instances in a cluster.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "workerConfig")]
    pub worker_config: Option<WorkflowTemplateStatusAtProviderPlacementManagedClusterConfigWorkerConfig>,
}

/// Autoscaling config for the policy associated with the cluster. Cluster does not autoscale if this field is unset.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderPlacementManagedClusterConfigAutoscalingConfig {
    /// The autoscaling policy used by the cluster. Only resource names including projectid and location (region) are valid. Examples: * https://www.googleapis.com/compute/v1/projects/ Note that the policy must be in the same project and Dataproc region.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub policy: Option<String>,
}

/// Encryption settings for the cluster.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderPlacementManagedClusterConfigEncryptionConfig {
    /// The Cloud KMS key name to use for PD disk encryption for all instances in the cluster.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gcePdKmsKeyName")]
    pub gce_pd_kms_key_name: Option<String>,
}

/// Port/endpoint configuration for this cluster
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderPlacementManagedClusterConfigEndpointConfig {
    /// If true, enable http access to specific ports on the cluster from external sources. Defaults to false.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableHttpPortAccess")]
    pub enable_http_port_access: Option<bool>,
    /// Output only. The map of port descriptions to URLs. Will only be populated if enable_http_port_access is true.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpPorts")]
    pub http_ports: Option<HashMap<String, String>>,
}

/// The shared Compute Engine config settings for all instances in a cluster.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderPlacementManagedClusterConfigGceClusterConfig {
    /// If true, all instances in the cluster will only have internal IP addresses. By default, clusters are not restricted to internal IP addresses, and will have ephemeral external IP addresses assigned to each instance. This internal_ip_only restriction can only be enabled for subnetwork enabled networks, and all off-cluster dependencies must be configured to be accessible without external IP addresses.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "internalIpOnly")]
    pub internal_ip_only: Option<bool>,
    /// The Compute Engine metadata entries to add to all instances (see (https://cloud.google.com/compute/docs/storing-retrieving-metadata#project_and_instance_metadata)).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub metadata: Option<HashMap<String, String>>,
    /// The Compute Engine network to be used for machine communications. Cannot be specified with subnetwork_uri. If neither network_uri nor subnetwork_uri is specified, the "default" network of the project is used, if it exists. Cannot be a "Custom Subnet Network" (see /regions/global/default*default`
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub network: Option<String>,
    /// Node Group Affinity for sole-tenant clusters.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodeGroupAffinity")]
    pub node_group_affinity: Option<WorkflowTemplateStatusAtProviderPlacementManagedClusterConfigGceClusterConfigNodeGroupAffinity>,
    /// The type of IPv6 access for a cluster. Possible values: PRIVATE_IPV6_GOOGLE_ACCESS_UNSPECIFIED, INHERIT_FROM_SUBNETWORK, OUTBOUND, BIDIRECTIONAL
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "privateIpv6GoogleAccess")]
    pub private_ipv6_google_access: Option<String>,
    /// Reservation Affinity for consuming Zonal reservation.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "reservationAffinity")]
    pub reservation_affinity: Option<WorkflowTemplateStatusAtProviderPlacementManagedClusterConfigGceClusterConfigReservationAffinity>,
    /// The (https://cloud.google.com/compute/docs/access/service-accounts#default_service_account) is used.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceAccount")]
    pub service_account: Option<String>,
    /// The URIs of service account scopes to be included in Compute Engine instances. The following base set of scopes is always included: * https://www.googleapis.com/auth/cloud.useraccounts.readonly * https://www.googleapis.com/auth/devstorage.read_write * https://www.googleapis.com/auth/logging.write If no scopes are specified, the following defaults are also provided: * https://www.googleapis.com/auth/bigquery * https://www.googleapis.com/auth/bigtable.admin.table * https://www.googleapis.com/auth/bigtable.data * https://www.googleapis.com/auth/devstorage.full_control
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceAccountScopes")]
    pub service_account_scopes: Option<Vec<String>>,
    /// Shielded Instance Config for clusters using Compute Engine Shielded VMs. Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "shieldedInstanceConfig")]
    pub shielded_instance_config: Option<WorkflowTemplateStatusAtProviderPlacementManagedClusterConfigGceClusterConfigShieldedInstanceConfig>,
    /// The Compute Engine subnetwork to be used for machine communications. Cannot be specified with network_uri. A full URL, partial URI, or short name are valid. Examples: * https://www.googleapis.com/compute/v1/projects//regions/us-east1/subnetworks/sub0 * sub0
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub subnetwork: Option<String>,
    /// The Compute Engine tags to add to all instances (see (https://cloud.google.com/compute/docs/label-or-tag-resources#tags)).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tags: Option<Vec<String>>,
    /// The zone where the Compute Engine cluster will be located. On a create request, it is required in the "global" region. If omitted in a non-global Dataproc region, the service will pick a zone in the corresponding Compute Engine region. On a get request, zone will always be present. A full URL, partial URI, or short name are valid. Examples: * https://www.googleapis.com/compute/v1/projects/ * us-central1-f
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub zone: Option<String>,
}

/// Node Group Affinity for sole-tenant clusters.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderPlacementManagedClusterConfigGceClusterConfigNodeGroupAffinity {
    /// Required. The URI of a sole-tenant /zones/us-central1-a/nodeGroups/node-group-1*node-group-1`
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodeGroup")]
    pub node_group: Option<String>,
}

/// Reservation Affinity for consuming Zonal reservation.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderPlacementManagedClusterConfigGceClusterConfigReservationAffinity {
    /// Type of reservation to consume Possible values: TYPE_UNSPECIFIED, NO_RESERVATION, ANY_RESERVATION, SPECIFIC_RESERVATION
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "consumeReservationType")]
    pub consume_reservation_type: Option<String>,
    /// Corresponds to the label key of reservation resource.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub key: Option<String>,
    /// Required. List of allowed values for the parameter.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// Shielded Instance Config for clusters using Compute Engine Shielded VMs. Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderPlacementManagedClusterConfigGceClusterConfigShieldedInstanceConfig {
    /// Defines whether instances have Integrity Monitoring enabled.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableIntegrityMonitoring")]
    pub enable_integrity_monitoring: Option<bool>,
    /// Defines whether instances have Secure Boot enabled.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableSecureBoot")]
    pub enable_secure_boot: Option<bool>,
    /// Defines whether instances have the vTPM enabled.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableVtpm")]
    pub enable_vtpm: Option<bool>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderPlacementManagedClusterConfigInitializationActions {
    /// Required. Cloud Storage URI of executable file.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "executableFile")]
    pub executable_file: Option<String>,
    /// Amount of time executable has to complete. Default is 10 minutes (see JSON representation of (https://developers.google.com/protocol-buffers/docs/proto3#json)). Cluster creation fails with an explanatory error message (the name of the executable that caused the error and the exceeded timeout period) if the executable is not completed at end of the timeout period.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "executionTimeout")]
    pub execution_timeout: Option<String>,
}

/// Lifecycle setting for the cluster.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderPlacementManagedClusterConfigLifecycleConfig {
    /// The time when cluster will be auto-deleted (see JSON representation of (https://developers.google.com/protocol-buffers/docs/proto3#json)).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "autoDeleteTime")]
    pub auto_delete_time: Option<String>,
    /// The lifetime duration of cluster. The cluster will be auto-deleted at the end of this period. Minimum value is 10 minutes; maximum value is 14 days (see JSON representation of (https://developers.google.com/protocol-buffers/docs/proto3#json)).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "autoDeleteTtl")]
    pub auto_delete_ttl: Option<String>,
    /// The duration to keep the cluster alive while idling (when no jobs are running). Passing this threshold will cause the cluster to be deleted. Minimum value is 5 minutes; maximum value is 14 days (see JSON representation of (https://developers.google.com/protocol-buffers/docs/proto3#json).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "idleDeleteTtl")]
    pub idle_delete_ttl: Option<String>,
    /// Output only. The time when cluster became idle (most recent job finished) and became eligible for deletion due to idleness (see JSON representation of (https://developers.google.com/protocol-buffers/docs/proto3#json)).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "idleStartTime")]
    pub idle_start_time: Option<String>,
}

/// The Compute Engine config settings for additional worker instances in a cluster.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderPlacementManagedClusterConfigMasterConfig {
    /// The Compute Engine accelerator configuration for these instances.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub accelerators: Option<Vec<WorkflowTemplateStatusAtProviderPlacementManagedClusterConfigMasterConfigAccelerators>>,
    /// Disk option config settings.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "diskConfig")]
    pub disk_config: Option<WorkflowTemplateStatusAtProviderPlacementManagedClusterConfigMasterConfigDiskConfig>,
    /// The Compute Engine image resource used for cluster instances. The URI can represent an image or image family. Image examples: * https://www.googleapis.com/compute/beta/projects/ If the URI is unspecified, it will be inferred from SoftwareConfig.image_version or the system default.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub image: Option<String>,
    /// Output only. The list of instance names. Dataproc derives the names from cluster_name, num_instances, and the instance group.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "instanceNames")]
    pub instance_names: Option<Vec<String>>,
    /// Output only. Specifies that this instance group contains preemptible instances.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "isPreemptible")]
    pub is_preemptible: Option<bool>,
    /// The Compute Engine machine type used for cluster instances. A full URL, partial URI, or short name are valid. Examples: * https://www.googleapis.com/compute/v1/projects/(https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the machine type resource, for example, n1-standard-2`.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "machineType")]
    pub machine_type: Option<String>,
    /// Output only. The config for Compute Engine Instance Group Manager that manages this group. This is only used for preemptible instance groups.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "managedGroupConfig")]
    pub managed_group_config: Option<Vec<WorkflowTemplateStatusAtProviderPlacementManagedClusterConfigMasterConfigManagedGroupConfig>>,
    /// Specifies the minimum cpu platform for the Instance Group. See (https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "minCpuPlatform")]
    pub min_cpu_platform: Option<String>,
    /// The number of VM instances in the instance group. For master instance groups, must be set to 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "numInstances")]
    pub num_instances: Option<f64>,
    /// Specifies the preemptibility of the instance group. The default value for master and worker groups is NON_PREEMPTIBLE. This default cannot be changed. The default value for secondary instances is PREEMPTIBLE. Possible values: PREEMPTIBILITY_UNSPECIFIED, NON_PREEMPTIBLE, PREEMPTIBLE
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub preemptibility: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderPlacementManagedClusterConfigMasterConfigAccelerators {
    /// The number of the accelerator cards of this type exposed to this instance.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "acceleratorCount")]
    pub accelerator_count: Option<f64>,
    /// Full URL, partial URI, or short name of the accelerator type resource to expose to this instance. See (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the accelerator type resource, for example, nvidia-tesla-k80.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "acceleratorType")]
    pub accelerator_type: Option<String>,
}

/// Disk option config settings.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderPlacementManagedClusterConfigMasterConfigDiskConfig {
    /// Size in GB of the boot disk (default is 500GB).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bootDiskSizeGb")]
    pub boot_disk_size_gb: Option<f64>,
    /// Type of the boot disk (default is "pd-standard"). Valid values: "pd-ssd" (Persistent Disk Solid State Drive) or "pd-standard" (Persistent Disk Hard Disk Drive).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bootDiskType")]
    pub boot_disk_type: Option<String>,
    /// Number of attached SSDs, from 0 to 4 (default is 0). If SSDs are not attached, the boot disk is used to store runtime logs and (https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data. If one or more SSDs are attached, this runtime bulk data is spread across them, and the boot disk contains only basic config and installed binaries.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "numLocalSsds")]
    pub num_local_ssds: Option<f64>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderPlacementManagedClusterConfigMasterConfigManagedGroupConfig {
    /// Output only. The resource name of the workflow template, as described in https://cloud.google.com/apis/design/resource_names. * For projects.regions.workflowTemplates, the resource name of the template has the following format: projects/{project_id}/regions/{region}/workflowTemplates/{template_id} * For projects.locations.workflowTemplates, the resource name of the template has the following format: projects/{project_id}/locations/{location}/workflowTemplates/{template_id}
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "instanceGroupManagerName")]
    pub instance_group_manager_name: Option<String>,
    /// Output only. The resource name of the workflow template, as described in https://cloud.google.com/apis/design/resource_names. * For projects.regions.workflowTemplates, the resource name of the template has the following format: projects/{project_id}/regions/{region}/workflowTemplates/{template_id} * For projects.locations.workflowTemplates, the resource name of the template has the following format: projects/{project_id}/locations/{location}/workflowTemplates/{template_id}
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "instanceTemplateName")]
    pub instance_template_name: Option<String>,
}

/// The Compute Engine config settings for additional worker instances in a cluster.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderPlacementManagedClusterConfigSecondaryWorkerConfig {
    /// The Compute Engine accelerator configuration for these instances.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub accelerators: Option<Vec<WorkflowTemplateStatusAtProviderPlacementManagedClusterConfigSecondaryWorkerConfigAccelerators>>,
    /// Disk option config settings.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "diskConfig")]
    pub disk_config: Option<WorkflowTemplateStatusAtProviderPlacementManagedClusterConfigSecondaryWorkerConfigDiskConfig>,
    /// The Compute Engine image resource used for cluster instances. The URI can represent an image or image family. Image examples: * https://www.googleapis.com/compute/beta/projects/ If the URI is unspecified, it will be inferred from SoftwareConfig.image_version or the system default.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub image: Option<String>,
    /// Output only. The list of instance names. Dataproc derives the names from cluster_name, num_instances, and the instance group.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "instanceNames")]
    pub instance_names: Option<Vec<String>>,
    /// Output only. Specifies that this instance group contains preemptible instances.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "isPreemptible")]
    pub is_preemptible: Option<bool>,
    /// The Compute Engine machine type used for cluster instances. A full URL, partial URI, or short name are valid. Examples: * https://www.googleapis.com/compute/v1/projects/(https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the machine type resource, for example, n1-standard-2`.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "machineType")]
    pub machine_type: Option<String>,
    /// Output only. The config for Compute Engine Instance Group Manager that manages this group. This is only used for preemptible instance groups.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "managedGroupConfig")]
    pub managed_group_config: Option<Vec<WorkflowTemplateStatusAtProviderPlacementManagedClusterConfigSecondaryWorkerConfigManagedGroupConfig>>,
    /// Specifies the minimum cpu platform for the Instance Group. See (https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "minCpuPlatform")]
    pub min_cpu_platform: Option<String>,
    /// The number of VM instances in the instance group. For master instance groups, must be set to 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "numInstances")]
    pub num_instances: Option<f64>,
    /// Specifies the preemptibility of the instance group. The default value for master and worker groups is NON_PREEMPTIBLE. This default cannot be changed. The default value for secondary instances is PREEMPTIBLE. Possible values: PREEMPTIBILITY_UNSPECIFIED, NON_PREEMPTIBLE, PREEMPTIBLE
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub preemptibility: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderPlacementManagedClusterConfigSecondaryWorkerConfigAccelerators {
    /// The number of the accelerator cards of this type exposed to this instance.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "acceleratorCount")]
    pub accelerator_count: Option<f64>,
    /// Full URL, partial URI, or short name of the accelerator type resource to expose to this instance. See (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the accelerator type resource, for example, nvidia-tesla-k80.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "acceleratorType")]
    pub accelerator_type: Option<String>,
}

/// Disk option config settings.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderPlacementManagedClusterConfigSecondaryWorkerConfigDiskConfig {
    /// Size in GB of the boot disk (default is 500GB).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bootDiskSizeGb")]
    pub boot_disk_size_gb: Option<f64>,
    /// Type of the boot disk (default is "pd-standard"). Valid values: "pd-ssd" (Persistent Disk Solid State Drive) or "pd-standard" (Persistent Disk Hard Disk Drive).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bootDiskType")]
    pub boot_disk_type: Option<String>,
    /// Number of attached SSDs, from 0 to 4 (default is 0). If SSDs are not attached, the boot disk is used to store runtime logs and (https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data. If one or more SSDs are attached, this runtime bulk data is spread across them, and the boot disk contains only basic config and installed binaries.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "numLocalSsds")]
    pub num_local_ssds: Option<f64>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderPlacementManagedClusterConfigSecondaryWorkerConfigManagedGroupConfig {
    /// Output only. The resource name of the workflow template, as described in https://cloud.google.com/apis/design/resource_names. * For projects.regions.workflowTemplates, the resource name of the template has the following format: projects/{project_id}/regions/{region}/workflowTemplates/{template_id} * For projects.locations.workflowTemplates, the resource name of the template has the following format: projects/{project_id}/locations/{location}/workflowTemplates/{template_id}
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "instanceGroupManagerName")]
    pub instance_group_manager_name: Option<String>,
    /// Output only. The resource name of the workflow template, as described in https://cloud.google.com/apis/design/resource_names. * For projects.regions.workflowTemplates, the resource name of the template has the following format: projects/{project_id}/regions/{region}/workflowTemplates/{template_id} * For projects.locations.workflowTemplates, the resource name of the template has the following format: projects/{project_id}/locations/{location}/workflowTemplates/{template_id}
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "instanceTemplateName")]
    pub instance_template_name: Option<String>,
}

/// Security settings for the cluster.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderPlacementManagedClusterConfigSecurityConfig {
    /// Kerberos related configuration.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kerberosConfig")]
    pub kerberos_config: Option<WorkflowTemplateStatusAtProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig>,
}

/// Kerberos related configuration.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderPlacementManagedClusterConfigSecurityConfigKerberosConfig {
    /// The admin server (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "crossRealmTrustAdminServer")]
    pub cross_realm_trust_admin_server: Option<String>,
    /// The KDC (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "crossRealmTrustKdc")]
    pub cross_realm_trust_kdc: Option<String>,
    /// The remote realm the Dataproc on-cluster KDC will trust, should the user enable cross realm trust.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "crossRealmTrustRealm")]
    pub cross_realm_trust_realm: Option<String>,
    /// The Cloud Storage URI of a KMS encrypted file containing the shared password between the on-cluster Kerberos realm and the remote trusted realm, in a cross realm trust relationship.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "crossRealmTrustSharedPassword")]
    pub cross_realm_trust_shared_password: Option<String>,
    /// Flag to indicate whether to Kerberize the cluster (default: false). Set this field to true to enable Kerberos on a cluster.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableKerberos")]
    pub enable_kerberos: Option<bool>,
    /// The Cloud Storage URI of a KMS encrypted file containing the master key of the KDC database.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kdcDbKey")]
    pub kdc_db_key: Option<String>,
    /// The Cloud Storage URI of a KMS encrypted file containing the password to the user provided key. For the self-signed certificate, this password is generated by Dataproc.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "keyPassword")]
    pub key_password: Option<String>,
    /// The Cloud Storage URI of the keystore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub keystore: Option<String>,
    /// The Cloud Storage URI of a KMS encrypted file containing the password to the user provided keystore. For the self-signed certificate, this password is generated by Dataproc.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "keystorePassword")]
    pub keystore_password: Option<String>,
    /// The uri of the KMS key used to encrypt various sensitive files.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kmsKey")]
    pub kms_key: Option<String>,
    /// The name of the on-cluster Kerberos realm. If not specified, the uppercased domain of hostnames will be the realm.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub realm: Option<String>,
    /// The Cloud Storage URI of a KMS encrypted file containing the root principal password.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "rootPrincipalPassword")]
    pub root_principal_password: Option<String>,
    /// The lifetime of the ticket granting ticket, in hours. If not specified, or user specifies 0, then default value 10 will be used.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tgtLifetimeHours")]
    pub tgt_lifetime_hours: Option<f64>,
    /// The Cloud Storage URI of the truststore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub truststore: Option<String>,
    /// The Cloud Storage URI of a KMS encrypted file containing the password to the user provided truststore. For the self-signed certificate, this password is generated by Dataproc.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "truststorePassword")]
    pub truststore_password: Option<String>,
}

/// The config settings for software inside the cluster.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderPlacementManagedClusterConfigSoftwareConfig {
    /// The version of software inside the cluster. It must be one of the supported Dataproc Versions, such as "1.2" (including a subminor version, such as "1.2.29"), or the "preview" version. If unspecified, it defaults to the latest Debian version.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "imageVersion")]
    pub image_version: Option<String>,
    /// The set of components to activate on the cluster.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "optionalComponents")]
    pub optional_components: Option<Vec<String>>,
    /// A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub properties: Option<HashMap<String, String>>,
}

/// The Compute Engine config settings for additional worker instances in a cluster.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderPlacementManagedClusterConfigWorkerConfig {
    /// The Compute Engine accelerator configuration for these instances.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub accelerators: Option<Vec<WorkflowTemplateStatusAtProviderPlacementManagedClusterConfigWorkerConfigAccelerators>>,
    /// Disk option config settings.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "diskConfig")]
    pub disk_config: Option<WorkflowTemplateStatusAtProviderPlacementManagedClusterConfigWorkerConfigDiskConfig>,
    /// The Compute Engine image resource used for cluster instances. The URI can represent an image or image family. Image examples: * https://www.googleapis.com/compute/beta/projects/ If the URI is unspecified, it will be inferred from SoftwareConfig.image_version or the system default.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub image: Option<String>,
    /// Output only. The list of instance names. Dataproc derives the names from cluster_name, num_instances, and the instance group.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "instanceNames")]
    pub instance_names: Option<Vec<String>>,
    /// Output only. Specifies that this instance group contains preemptible instances.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "isPreemptible")]
    pub is_preemptible: Option<bool>,
    /// The Compute Engine machine type used for cluster instances. A full URL, partial URI, or short name are valid. Examples: * https://www.googleapis.com/compute/v1/projects/(https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the machine type resource, for example, n1-standard-2`.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "machineType")]
    pub machine_type: Option<String>,
    /// Output only. The config for Compute Engine Instance Group Manager that manages this group. This is only used for preemptible instance groups.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "managedGroupConfig")]
    pub managed_group_config: Option<Vec<WorkflowTemplateStatusAtProviderPlacementManagedClusterConfigWorkerConfigManagedGroupConfig>>,
    /// Specifies the minimum cpu platform for the Instance Group. See (https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "minCpuPlatform")]
    pub min_cpu_platform: Option<String>,
    /// The number of VM instances in the instance group. For master instance groups, must be set to 1.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "numInstances")]
    pub num_instances: Option<f64>,
    /// Specifies the preemptibility of the instance group. The default value for master and worker groups is NON_PREEMPTIBLE. This default cannot be changed. The default value for secondary instances is PREEMPTIBLE. Possible values: PREEMPTIBILITY_UNSPECIFIED, NON_PREEMPTIBLE, PREEMPTIBLE
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub preemptibility: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderPlacementManagedClusterConfigWorkerConfigAccelerators {
    /// The number of the accelerator cards of this type exposed to this instance.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "acceleratorCount")]
    pub accelerator_count: Option<f64>,
    /// Full URL, partial URI, or short name of the accelerator type resource to expose to this instance. See (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the accelerator type resource, for example, nvidia-tesla-k80.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "acceleratorType")]
    pub accelerator_type: Option<String>,
}

/// Disk option config settings.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderPlacementManagedClusterConfigWorkerConfigDiskConfig {
    /// Size in GB of the boot disk (default is 500GB).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bootDiskSizeGb")]
    pub boot_disk_size_gb: Option<f64>,
    /// Type of the boot disk (default is "pd-standard"). Valid values: "pd-ssd" (Persistent Disk Solid State Drive) or "pd-standard" (Persistent Disk Hard Disk Drive).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bootDiskType")]
    pub boot_disk_type: Option<String>,
    /// Number of attached SSDs, from 0 to 4 (default is 0). If SSDs are not attached, the boot disk is used to store runtime logs and (https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data. If one or more SSDs are attached, this runtime bulk data is spread across them, and the boot disk contains only basic config and installed binaries.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "numLocalSsds")]
    pub num_local_ssds: Option<f64>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct WorkflowTemplateStatusAtProviderPlacementManagedClusterConfigWorkerConfigManagedGroupConfig {
    /// Output only. The resource name of the workflow template, as described in https://cloud.google.com/apis/design/resource_names. * For projects.regions.workflowTemplates, the resource name of the template has the following format: projects/{project_id}/regions/{region}/workflowTemplates/{template_id} * For projects.locations.workflowTemplates, the resource name of the template has the following format: projects/{project_id}/locations/{location}/workflowTemplates/{template_id}
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "instanceGroupManagerName")]
    pub instance_group_manager_name: Option<String>,
    /// Output only. The resource name of the workflow template, as described in https://cloud.google.com/apis/design/resource_names. * For projects.regions.workflowTemplates, the resource name of the template has the following format: projects/{project_id}/regions/{region}/workflowTemplates/{template_id} * For projects.locations.workflowTemplates, the resource name of the template has the following format: projects/{project_id}/locations/{location}/workflowTemplates/{template_id}
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "instanceTemplateName")]
    pub instance_template_name: Option<String>,
}

