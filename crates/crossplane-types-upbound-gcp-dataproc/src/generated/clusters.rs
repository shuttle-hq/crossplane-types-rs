// WARNING: generated by kopium - manual changes will be overwritten
// kopium version: 0.21.2

#[allow(unused_imports)]
mod prelude {
    pub use kube::CustomResource;
    pub use schemars::JsonSchema;
    pub use serde::{Serialize, Deserialize};
    pub use std::collections::HashMap;
    pub use k8s_openapi::apimachinery::pkg::apis::meta::v1::Condition;
}
use self::prelude::*;

/// ClusterSpec defines the desired state of Cluster
#[derive(CustomResource, Serialize, Deserialize, Clone, Debug, JsonSchema)]
#[kube(group = "dataproc.gcp.upbound.io", version = "v1beta2", kind = "Cluster", plural = "clusters")]
#[kube(status = "ClusterStatus")]
pub struct ClusterSpec {
    /// DeletionPolicy specifies what will happen to the underlying external
    /// when this managed resource is deleted - either "Delete" or "Orphan" the
    /// external resource.
    /// This field is planned to be deprecated in favor of the ManagementPolicies
    /// field in a future release. Currently, both could be set independently and
    /// non-default values would be honored if the feature flag is enabled.
    /// See the design doc for more information: https://github.com/crossplane/crossplane/blob/499895a25d1a1a0ba1604944ef98ac7a1a71f197/design/design-doc-observe-only-resources.md?plain=1#L223
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "deletionPolicy")]
    pub deletion_policy: Option<ClusterDeletionPolicy>,
    #[serde(rename = "forProvider")]
    pub for_provider: ClusterForProvider,
    /// THIS IS A BETA FIELD. It will be honored
    /// unless the Management Policies feature flag is disabled.
    /// InitProvider holds the same fields as ForProvider, with the exception
    /// of Identifier and other resource reference fields. The fields that are
    /// in InitProvider are merged into ForProvider when the resource is created.
    /// The same fields are also added to the terraform ignore_changes hook, to
    /// avoid updating them after creation. This is useful for fields that are
    /// required on creation, but we do not desire to update them after creation,
    /// for example because of an external controller is managing them, like an
    /// autoscaler.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initProvider")]
    pub init_provider: Option<ClusterInitProvider>,
    /// THIS IS A BETA FIELD. It is on by default but can be opted out
    /// through a Crossplane feature flag.
    /// ManagementPolicies specify the array of actions Crossplane is allowed to
    /// take on the managed and external resources.
    /// This field is planned to replace the DeletionPolicy field in a future
    /// release. Currently, both could be set independently and non-default
    /// values would be honored if the feature flag is enabled. If both are
    /// custom, the DeletionPolicy field will be ignored.
    /// See the design doc for more information: https://github.com/crossplane/crossplane/blob/499895a25d1a1a0ba1604944ef98ac7a1a71f197/design/design-doc-observe-only-resources.md?plain=1#L223
    /// and this one: https://github.com/crossplane/crossplane/blob/444267e84783136daa93568b364a5f01228cacbe/design/one-pager-ignore-changes.md
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "managementPolicies")]
    pub management_policies: Option<Vec<String>>,
    /// ProviderConfigReference specifies how the provider that will be used to
    /// create, observe, update, and delete this managed resource should be
    /// configured.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "providerConfigRef")]
    pub provider_config_ref: Option<ClusterProviderConfigRef>,
    /// PublishConnectionDetailsTo specifies the connection secret config which
    /// contains a name, metadata and a reference to secret store config to
    /// which any connection details for this managed resource should be written.
    /// Connection details frequently include the endpoint, username,
    /// and password required to connect to the managed resource.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "publishConnectionDetailsTo")]
    pub publish_connection_details_to: Option<ClusterPublishConnectionDetailsTo>,
    /// WriteConnectionSecretToReference specifies the namespace and name of a
    /// Secret to which any connection details for this managed resource should
    /// be written. Connection details frequently include the endpoint, username,
    /// and password required to connect to the managed resource.
    /// This field is planned to be replaced in a future release in favor of
    /// PublishConnectionDetailsTo. Currently, both could be set independently
    /// and connection details would be published to both without affecting
    /// each other.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "writeConnectionSecretToRef")]
    pub write_connection_secret_to_ref: Option<ClusterWriteConnectionSecretToRef>,
}

/// ClusterSpec defines the desired state of Cluster
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum ClusterDeletionPolicy {
    Orphan,
    Delete,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterForProvider {
    /// Allows you to configure various aspects of the cluster.
    /// Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clusterConfig")]
    pub cluster_config: Option<ClusterForProviderClusterConfig>,
    /// Does not affect auto scaling decomissioning from an autoscaling policy.
    /// Graceful decommissioning allows removing nodes from the cluster without interrupting jobs in progress.
    /// Timeout specifies how long to wait for jobs in progress to finish before forcefully removing nodes (and potentially interrupting jobs).
    /// Default timeout is 0 (for forceful decommission), and the maximum allowed timeout is 1 day. (see JSON representation of
    /// Duration).
    /// Only supported on Dataproc image versions 1.2 and higher.
    /// For more context see the docs
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gracefulDecommissionTimeout")]
    pub graceful_decommission_timeout: Option<String>,
    /// Note: This field is non-authoritative, and will only manage the labels present in your configuration. Please refer to the field effective_labels for all of the labels present on the resource.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub labels: Option<HashMap<String, String>>,
    /// The name of the cluster, unique within the project and
    /// zone.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// The ID of the project in which the cluster will exist. If it
    /// is not provided, the provider project is used.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub project: Option<String>,
    /// The region in which the cluster and associated nodes will be created in.
    /// Defaults to global.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub region: Option<String>,
    /// Allows you to configure a virtual Dataproc on GKE cluster.
    /// Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "virtualClusterConfig")]
    pub virtual_cluster_config: Option<ClusterForProviderVirtualClusterConfig>,
}

/// Allows you to configure various aspects of the cluster.
/// Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterForProviderClusterConfig {
    /// The autoscaling policy config associated with the cluster.
    /// Note that once set, if autoscaling_config is the only field set in cluster_config, it can
    /// only be removed by setting policy_uri = "", rather than removing the whole block.
    /// Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "autoscalingConfig")]
    pub autoscaling_config: Option<ClusterForProviderClusterConfigAutoscalingConfig>,
    /// A Dataproc NodeGroup resource is a group of Dataproc cluster nodes that execute an assigned role.
    /// Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "auxiliaryNodeGroups")]
    pub auxiliary_node_groups: Option<Vec<ClusterForProviderClusterConfigAuxiliaryNodeGroups>>,
    /// The Compute Engine accelerator (GPU) configuration for these instances. Can be specified multiple times.
    /// Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dataprocMetricConfig")]
    pub dataproc_metric_config: Option<ClusterForProviderClusterConfigDataprocMetricConfig>,
    /// The Customer managed encryption keys settings for the cluster.
    /// Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "encryptionConfig")]
    pub encryption_config: Option<ClusterForProviderClusterConfigEncryptionConfig>,
    /// The config settings for port access on the cluster.
    /// Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "endpointConfig")]
    pub endpoint_config: Option<ClusterForProviderClusterConfigEndpointConfig>,
    /// Common config settings for resources of Google Compute Engine cluster
    /// instances, applicable to all instances in the cluster. Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gceClusterConfig")]
    pub gce_cluster_config: Option<ClusterForProviderClusterConfigGceClusterConfig>,
    /// Commands to execute on each node after config is completed.
    /// You can specify multiple versions of these. Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initializationAction")]
    pub initialization_action: Option<Vec<ClusterForProviderClusterConfigInitializationAction>>,
    /// The settings for auto deletion cluster schedule.
    /// Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "lifecycleConfig")]
    pub lifecycle_config: Option<ClusterForProviderClusterConfigLifecycleConfig>,
    /// The Google Compute Engine config settings for the master instances
    /// in a cluster. Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "masterConfig")]
    pub master_config: Option<ClusterForProviderClusterConfigMasterConfig>,
    /// The config setting for metastore service with the cluster.
    /// Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "metastoreConfig")]
    pub metastore_config: Option<ClusterForProviderClusterConfigMetastoreConfig>,
    /// The Google Compute Engine config settings for the additional
    /// instances in a cluster. Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "preemptibleWorkerConfig")]
    pub preemptible_worker_config: Option<ClusterForProviderClusterConfigPreemptibleWorkerConfig>,
    /// Security related configuration. Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "securityConfig")]
    pub security_config: Option<ClusterForProviderClusterConfigSecurityConfig>,
    /// The config settings for software inside the cluster.
    /// Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "softwareConfig")]
    pub software_config: Option<ClusterForProviderClusterConfigSoftwareConfig>,
    /// The Cloud Storage staging bucket used to stage files,
    /// such as Hadoop jars, between client machines and the cluster.
    /// Note: If you don't explicitly specify a staging_bucket
    /// then GCP will auto create / assign one for you. However, you are not guaranteed
    /// an auto generated bucket which is solely dedicated to your cluster; it may be shared
    /// with other clusters in the same region/zone also choosing to use the auto generation
    /// option.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "stagingBucket")]
    pub staging_bucket: Option<String>,
    /// The Cloud Storage temp bucket used to store ephemeral cluster
    /// and jobs data, such as Spark and MapReduce history files.
    /// Note: If you don't explicitly specify a temp_bucket then GCP will auto create / assign one for you.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tempBucket")]
    pub temp_bucket: Option<String>,
    /// The Google Compute Engine config settings for the worker instances
    /// in a cluster. Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "workerConfig")]
    pub worker_config: Option<ClusterForProviderClusterConfigWorkerConfig>,
}

/// The autoscaling policy config associated with the cluster.
/// Note that once set, if autoscaling_config is the only field set in cluster_config, it can
/// only be removed by setting policy_uri = "", rather than removing the whole block.
/// Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterForProviderClusterConfigAutoscalingConfig {
    /// The autoscaling policy used by the cluster.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "policyUri")]
    pub policy_uri: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterForProviderClusterConfigAuxiliaryNodeGroups {
    /// Node group configuration.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodeGroup")]
    pub node_group: Option<Vec<ClusterForProviderClusterConfigAuxiliaryNodeGroupsNodeGroup>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodeGroupId")]
    pub node_group_id: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterForProviderClusterConfigAuxiliaryNodeGroupsNodeGroup {
    /// The node group instance group configuration.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodeGroupConfig")]
    pub node_group_config: Option<ClusterForProviderClusterConfigAuxiliaryNodeGroupsNodeGroupNodeGroupConfig>,
    /// The roles associated with the GKE node pool.
    /// One of "DEFAULT", "CONTROLLER", "SPARK_DRIVER" or "SPARK_EXECUTOR".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub roles: Option<Vec<String>>,
}

/// The node group instance group configuration.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterForProviderClusterConfigAuxiliaryNodeGroupsNodeGroupNodeGroupConfig {
    /// The Compute Engine accelerator configuration for these instances. Can be specified multiple times.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub accelerators: Option<Vec<ClusterForProviderClusterConfigAuxiliaryNodeGroupsNodeGroupNodeGroupConfigAccelerators>>,
    /// Disk Config
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "diskConfig")]
    pub disk_config: Option<ClusterForProviderClusterConfigAuxiliaryNodeGroupsNodeGroupNodeGroupConfigDiskConfig>,
    /// The name of a Compute Engine machine type.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "machineType")]
    pub machine_type: Option<String>,
    /// Minimum CPU platform to be used by this instance.
    /// The instance may be scheduled on the specified or a newer CPU platform.
    /// Specify the friendly names of CPU platforms, such as "Intel Haswell" or "Intel Sandy Bridge".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "minCpuPlatform")]
    pub min_cpu_platform: Option<String>,
    /// Specifies the number of master nodes to create.
    /// Please set a number greater than 0. Node Group must have at least 1 instance.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "numInstances")]
    pub num_instances: Option<f64>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterForProviderClusterConfigAuxiliaryNodeGroupsNodeGroupNodeGroupConfigAccelerators {
    /// The number of the accelerator cards of this type exposed to this instance. Often restricted to one of 1, 2, 4, or 8.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "acceleratorCount")]
    pub accelerator_count: Option<f64>,
    /// The short name of the accelerator type to expose to this instance. For example, nvidia-tesla-k80.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "acceleratorType")]
    pub accelerator_type: Option<String>,
}

/// Disk Config
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterForProviderClusterConfigAuxiliaryNodeGroupsNodeGroupNodeGroupConfigDiskConfig {
    /// Size of the primary disk attached to each node, specified
    /// in GB. The primary disk contains the boot volume and system libraries, and the
    /// smallest allowed disk size is 10GB. GCP will default to a predetermined
    /// computed value if not set (currently 500GB). Note: If SSDs are not
    /// attached, it also contains the HDFS data blocks and Hadoop working directories.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bootDiskSizeGb")]
    pub boot_disk_size_gb: Option<f64>,
    /// The disk type of the primary disk attached to each node.
    /// One of "pd-ssd" or "pd-standard". Defaults to "pd-standard".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bootDiskType")]
    pub boot_disk_type: Option<String>,
    /// Optional. Interface type of local SSDs (default is "scsi").
    /// Valid values: "scsi" (Small Computer System Interface), "nvme" (Non-Volatile
    /// Memory Express). See
    /// local SSD performance.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "localSsdInterface")]
    pub local_ssd_interface: Option<String>,
    /// The amount of local SSD disks that will be
    /// attached to each master cluster node. Defaults to 0.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "numLocalSsds")]
    pub num_local_ssds: Option<f64>,
}

/// The Compute Engine accelerator (GPU) configuration for these instances. Can be specified multiple times.
/// Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterForProviderClusterConfigDataprocMetricConfig {
    /// Metrics sources to enable.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub metrics: Option<Vec<ClusterForProviderClusterConfigDataprocMetricConfigMetrics>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterForProviderClusterConfigDataprocMetricConfigMetrics {
    /// One or more [available OSS metrics] (https://cloud.google.com/dataproc/docs/guides/monitoring#available_oss_metrics) to collect for the metric course.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "metricOverrides")]
    pub metric_overrides: Option<Vec<String>>,
    /// A source for the collection of Dataproc OSS metrics (see available OSS metrics).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "metricSource")]
    pub metric_source: Option<String>,
}

/// The Customer managed encryption keys settings for the cluster.
/// Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterForProviderClusterConfigEncryptionConfig {
    /// The Cloud KMS key name to use for PD disk encryption for
    /// all instances in the cluster.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kmsKeyName")]
    pub kms_key_name: Option<String>,
}

/// The config settings for port access on the cluster.
/// Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterForProviderClusterConfigEndpointConfig {
    /// The flag to enable http access to specific ports
    /// on the cluster from external sources (aka Component Gateway). Defaults to false.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableHttpPortAccess")]
    pub enable_http_port_access: Option<bool>,
}

/// Common config settings for resources of Google Compute Engine cluster
/// instances, applicable to all instances in the cluster. Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterForProviderClusterConfigGceClusterConfig {
    /// By default, clusters are not restricted to internal IP addresses,
    /// and will have ephemeral external IP addresses assigned to each instance. If set to true, all
    /// instances in the cluster will only have internal IP addresses. Note: Private Google Access
    /// (also known as privateIpGoogleAccess) must be enabled on the subnetwork that the cluster
    /// will be launched in.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "internalIpOnly")]
    pub internal_ip_only: Option<bool>,
    /// A map of the Compute Engine metadata entries to add to all instances
    /// (see Project and instance metadata).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub metadata: Option<HashMap<String, String>>,
    /// The name or self_link of the Google Compute Engine
    /// network to the cluster will be part of. Conflicts with subnetwork.
    /// If neither is specified, this defaults to the "default" network.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub network: Option<String>,
    /// Node Group Affinity for sole-tenant clusters.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodeGroupAffinity")]
    pub node_group_affinity: Option<ClusterForProviderClusterConfigGceClusterConfigNodeGroupAffinity>,
    /// Reservation Affinity for consuming zonal reservation.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "reservationAffinity")]
    pub reservation_affinity: Option<ClusterForProviderClusterConfigGceClusterConfigReservationAffinity>,
    /// The service account to be used by the Node VMs.
    /// If not specified, the "default" service account is used.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceAccount")]
    pub service_account: Option<String>,
    /// Reference to a ServiceAccount in cloudplatform to populate serviceAccount.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceAccountRef")]
    pub service_account_ref: Option<ClusterForProviderClusterConfigGceClusterConfigServiceAccountRef>,
    /// The set of Google API scopes
    /// to be made available on all of the node VMs under the service_account
    /// specified. Both OAuth2 URLs and gcloud
    /// short names are supported. To allow full access to all Cloud APIs, use the
    /// cloud-platform scope. See a complete list of scopes here.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceAccountScopes")]
    pub service_account_scopes: Option<Vec<String>>,
    /// Selector for a ServiceAccount in cloudplatform to populate serviceAccount.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceAccountSelector")]
    pub service_account_selector: Option<ClusterForProviderClusterConfigGceClusterConfigServiceAccountSelector>,
    /// Shielded Instance Config for clusters using Compute Engine Shielded VMs.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "shieldedInstanceConfig")]
    pub shielded_instance_config: Option<ClusterForProviderClusterConfigGceClusterConfigShieldedInstanceConfig>,
    /// The name or self_link of the Google Compute Engine
    /// subnetwork the cluster will be part of. Conflicts with network.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub subnetwork: Option<String>,
    /// The list of instance tags applied to instances in the cluster.
    /// Tags are used to identify valid sources or targets for network firewalls.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tags: Option<Vec<String>>,
    /// The GCP zone where your data is stored and used (i.e. where
    /// the master and the worker nodes will be created in). If region is set to 'global' (default)
    /// then zone is mandatory, otherwise GCP is able to make use of Auto Zone Placement
    /// to determine this automatically for you.
    /// Note: This setting additionally determines and restricts
    /// which computing resources are available for use with other configs such as
    /// cluster_config.master_config.machine_type and cluster_config.worker_config.machine_type.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub zone: Option<String>,
}

/// Node Group Affinity for sole-tenant clusters.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterForProviderClusterConfigGceClusterConfigNodeGroupAffinity {
    /// The URI of a sole-tenant node group resource that the cluster will be created on.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodeGroupUri")]
    pub node_group_uri: Option<String>,
}

/// Reservation Affinity for consuming zonal reservation.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterForProviderClusterConfigGceClusterConfigReservationAffinity {
    /// Corresponds to the type of reservation consumption.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "consumeReservationType")]
    pub consume_reservation_type: Option<String>,
    /// Corresponds to the label key of reservation resource.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub key: Option<String>,
    /// Corresponds to the label values of reservation resource.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// Reference to a ServiceAccount in cloudplatform to populate serviceAccount.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterForProviderClusterConfigGceClusterConfigServiceAccountRef {
    /// Name of the referenced object.
    pub name: String,
    /// Policies for referencing.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub policy: Option<ClusterForProviderClusterConfigGceClusterConfigServiceAccountRefPolicy>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterForProviderClusterConfigGceClusterConfigServiceAccountRefPolicy {
    /// Resolution specifies whether resolution of this reference is required.
    /// The default is 'Required', which means the reconcile will fail if the
    /// reference cannot be resolved. 'Optional' means this reference will be
    /// a no-op if it cannot be resolved.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolution: Option<ClusterForProviderClusterConfigGceClusterConfigServiceAccountRefPolicyResolution>,
    /// Resolve specifies when this reference should be resolved. The default
    /// is 'IfNotPresent', which will attempt to resolve the reference only when
    /// the corresponding field is not present. Use 'Always' to resolve the
    /// reference on every reconcile.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolve: Option<ClusterForProviderClusterConfigGceClusterConfigServiceAccountRefPolicyResolve>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum ClusterForProviderClusterConfigGceClusterConfigServiceAccountRefPolicyResolution {
    Required,
    Optional,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum ClusterForProviderClusterConfigGceClusterConfigServiceAccountRefPolicyResolve {
    Always,
    IfNotPresent,
}

/// Selector for a ServiceAccount in cloudplatform to populate serviceAccount.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterForProviderClusterConfigGceClusterConfigServiceAccountSelector {
    /// MatchControllerRef ensures an object with the same controller reference
    /// as the selecting object is selected.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchControllerRef")]
    pub match_controller_ref: Option<bool>,
    /// MatchLabels ensures an object with matching labels is selected.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<HashMap<String, String>>,
    /// Policies for selection.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub policy: Option<ClusterForProviderClusterConfigGceClusterConfigServiceAccountSelectorPolicy>,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterForProviderClusterConfigGceClusterConfigServiceAccountSelectorPolicy {
    /// Resolution specifies whether resolution of this reference is required.
    /// The default is 'Required', which means the reconcile will fail if the
    /// reference cannot be resolved. 'Optional' means this reference will be
    /// a no-op if it cannot be resolved.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolution: Option<ClusterForProviderClusterConfigGceClusterConfigServiceAccountSelectorPolicyResolution>,
    /// Resolve specifies when this reference should be resolved. The default
    /// is 'IfNotPresent', which will attempt to resolve the reference only when
    /// the corresponding field is not present. Use 'Always' to resolve the
    /// reference on every reconcile.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolve: Option<ClusterForProviderClusterConfigGceClusterConfigServiceAccountSelectorPolicyResolve>,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum ClusterForProviderClusterConfigGceClusterConfigServiceAccountSelectorPolicyResolution {
    Required,
    Optional,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum ClusterForProviderClusterConfigGceClusterConfigServiceAccountSelectorPolicyResolve {
    Always,
    IfNotPresent,
}

/// Shielded Instance Config for clusters using Compute Engine Shielded VMs.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterForProviderClusterConfigGceClusterConfigShieldedInstanceConfig {
    /// Defines whether instances have integrity monitoring enabled.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableIntegrityMonitoring")]
    pub enable_integrity_monitoring: Option<bool>,
    /// Defines whether instances have Secure Boot enabled.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableSecureBoot")]
    pub enable_secure_boot: Option<bool>,
    /// Defines whether instances have the vTPM enabled.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableVtpm")]
    pub enable_vtpm: Option<bool>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterForProviderClusterConfigInitializationAction {
    /// The script to be executed during initialization of the cluster.
    /// The script must be a GCS file with a gs:// prefix.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub script: Option<String>,
    /// The maximum duration (in seconds) which script is
    /// allowed to take to execute its action. GCP will default to a predetermined
    /// computed value if not set (currently 300).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "timeoutSec")]
    pub timeout_sec: Option<f64>,
}

/// The settings for auto deletion cluster schedule.
/// Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterForProviderClusterConfigLifecycleConfig {
    /// The time when cluster will be auto-deleted.
    /// A timestamp in RFC3339 UTC "Zulu" format, accurate to nanoseconds.
    /// Example: "2014-10-02T15:01:23.045123456Z".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "autoDeleteTime")]
    pub auto_delete_time: Option<String>,
    /// The duration to keep the cluster alive while idling
    /// (no jobs running). After this TTL, the cluster will be deleted. Valid range: [10m, 14d].
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "idleDeleteTtl")]
    pub idle_delete_ttl: Option<String>,
}

/// The Google Compute Engine config settings for the master instances
/// in a cluster. Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterForProviderClusterConfigMasterConfig {
    /// The Compute Engine accelerator (GPU) configuration for these instances. Can be specified multiple times.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub accelerators: Option<Vec<ClusterForProviderClusterConfigMasterConfigAccelerators>>,
    /// Disk Config
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "diskConfig")]
    pub disk_config: Option<ClusterForProviderClusterConfigMasterConfigDiskConfig>,
    /// The URI for the image to use for this worker.  See the guide
    /// for more information.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "imageUri")]
    pub image_uri: Option<String>,
    /// The name of a Google Compute Engine machine type
    /// to create for the master. If not specified, GCP will default to a predetermined
    /// computed value (currently n1-standard-4).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "machineType")]
    pub machine_type: Option<String>,
    /// The name of a minimum generation of CPU family
    /// for the master. If not specified, GCP will default to a predetermined computed value
    /// for each zone. See the guide
    /// for details about which CPU families are available (and defaulted) for each zone.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "minCpuPlatform")]
    pub min_cpu_platform: Option<String>,
    /// Specifies the number of master nodes to create.
    /// If not specified, GCP will default to a predetermined computed value (currently 1).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "numInstances")]
    pub num_instances: Option<f64>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterForProviderClusterConfigMasterConfigAccelerators {
    /// The number of the accelerator cards of this type exposed to this instance. Often restricted to one of 1, 2, 4, or 8.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "acceleratorCount")]
    pub accelerator_count: Option<f64>,
    /// The short name of the accelerator type to expose to this instance. For example, nvidia-tesla-k80.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "acceleratorType")]
    pub accelerator_type: Option<String>,
}

/// Disk Config
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterForProviderClusterConfigMasterConfigDiskConfig {
    /// Size of the primary disk attached to each node, specified
    /// in GB. The primary disk contains the boot volume and system libraries, and the
    /// smallest allowed disk size is 10GB. GCP will default to a predetermined
    /// computed value if not set (currently 500GB). Note: If SSDs are not
    /// attached, it also contains the HDFS data blocks and Hadoop working directories.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bootDiskSizeGb")]
    pub boot_disk_size_gb: Option<f64>,
    /// The disk type of the primary disk attached to each node.
    /// One of "pd-ssd" or "pd-standard". Defaults to "pd-standard".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bootDiskType")]
    pub boot_disk_type: Option<String>,
    /// Optional. Interface type of local SSDs (default is "scsi").
    /// Valid values: "scsi" (Small Computer System Interface), "nvme" (Non-Volatile
    /// Memory Express). See
    /// local SSD performance.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "localSsdInterface")]
    pub local_ssd_interface: Option<String>,
    /// The amount of local SSD disks that will be
    /// attached to each master cluster node. Defaults to 0.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "numLocalSsds")]
    pub num_local_ssds: Option<f64>,
}

/// The config setting for metastore service with the cluster.
/// Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterForProviderClusterConfigMetastoreConfig {
    /// Resource name of an existing Dataproc Metastore service.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dataprocMetastoreService")]
    pub dataproc_metastore_service: Option<String>,
}

/// The Google Compute Engine config settings for the additional
/// instances in a cluster. Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterForProviderClusterConfigPreemptibleWorkerConfig {
    /// Disk Config
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "diskConfig")]
    pub disk_config: Option<ClusterForProviderClusterConfigPreemptibleWorkerConfigDiskConfig>,
    /// Instance flexibility Policy allowing a mixture of VM shapes and provisioning models.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "instanceFlexibilityPolicy")]
    pub instance_flexibility_policy: Option<ClusterForProviderClusterConfigPreemptibleWorkerConfigInstanceFlexibilityPolicy>,
    /// Specifies the number of preemptible nodes to create.
    /// Defaults to 0.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "numInstances")]
    pub num_instances: Option<f64>,
    /// Specifies the preemptibility of the secondary workers. The default value is PREEMPTIBLE
    /// Accepted values are:
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub preemptibility: Option<String>,
}

/// Disk Config
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterForProviderClusterConfigPreemptibleWorkerConfigDiskConfig {
    /// Size of the primary disk attached to each node, specified
    /// in GB. The primary disk contains the boot volume and system libraries, and the
    /// smallest allowed disk size is 10GB. GCP will default to a predetermined
    /// computed value if not set (currently 500GB). Note: If SSDs are not
    /// attached, it also contains the HDFS data blocks and Hadoop working directories.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bootDiskSizeGb")]
    pub boot_disk_size_gb: Option<f64>,
    /// The disk type of the primary disk attached to each node.
    /// One of "pd-ssd" or "pd-standard". Defaults to "pd-standard".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bootDiskType")]
    pub boot_disk_type: Option<String>,
    /// Optional. Interface type of local SSDs (default is "scsi").
    /// Valid values: "scsi" (Small Computer System Interface), "nvme" (Non-Volatile
    /// Memory Express). See
    /// local SSD performance.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "localSsdInterface")]
    pub local_ssd_interface: Option<String>,
    /// The amount of local SSD disks that will be
    /// attached to each master cluster node. Defaults to 0.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "numLocalSsds")]
    pub num_local_ssds: Option<f64>,
}

/// Instance flexibility Policy allowing a mixture of VM shapes and provisioning models.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterForProviderClusterConfigPreemptibleWorkerConfigInstanceFlexibilityPolicy {
    /// List of instance selection options that the group will use when creating new VMs.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "instanceSelectionList")]
    pub instance_selection_list: Option<Vec<ClusterForProviderClusterConfigPreemptibleWorkerConfigInstanceFlexibilityPolicyInstanceSelectionList>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterForProviderClusterConfigPreemptibleWorkerConfigInstanceFlexibilityPolicyInstanceSelectionList {
    /// Full machine-type names, e.g. "n1-standard-16".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "machineTypes")]
    pub machine_types: Option<Vec<String>>,
    /// Preference of this instance selection. A lower number means higher preference. Dataproc will first try to create a VM based on the machine-type with priority rank and fallback to next rank based on availability. Machine types and instance selections with the same priority have the same preference.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub rank: Option<f64>,
}

/// Security related configuration. Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterForProviderClusterConfigSecurityConfig {
    /// Kerberos Configuration
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kerberosConfig")]
    pub kerberos_config: Option<ClusterForProviderClusterConfigSecurityConfigKerberosConfig>,
}

/// Kerberos Configuration
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterForProviderClusterConfigSecurityConfigKerberosConfig {
    /// The admin server (IP or hostname) for the
    /// remote trusted realm in a cross realm trust relationship.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "crossRealmTrustAdminServer")]
    pub cross_realm_trust_admin_server: Option<String>,
    /// The KDC (IP or hostname) for the
    /// remote trusted realm in a cross realm trust relationship.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "crossRealmTrustKdc")]
    pub cross_realm_trust_kdc: Option<String>,
    /// The remote realm the Dataproc on-cluster KDC will
    /// trust, should the user enable cross realm trust.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "crossRealmTrustRealm")]
    pub cross_realm_trust_realm: Option<String>,
    /// The Cloud Storage URI of a KMS
    /// encrypted file containing the shared password between the on-cluster Kerberos realm
    /// and the remote trusted realm, in a cross realm trust relationship.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "crossRealmTrustSharedPasswordUri")]
    pub cross_realm_trust_shared_password_uri: Option<String>,
    /// Flag to indicate whether to Kerberize the cluster.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableKerberos")]
    pub enable_kerberos: Option<bool>,
    /// The Cloud Storage URI of a KMS encrypted file containing
    /// the master key of the KDC database.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kdcDbKeyUri")]
    pub kdc_db_key_uri: Option<String>,
    /// The Cloud Storage URI of a KMS encrypted file containing
    /// the password to the user provided key. For the self-signed certificate, this password
    /// is generated by Dataproc.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "keyPasswordUri")]
    pub key_password_uri: Option<String>,
    /// The Cloud Storage URI of a KMS encrypted file containing
    /// the password to the user provided keystore. For the self-signed certificated, the password
    /// is generated by Dataproc.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "keystorePasswordUri")]
    pub keystore_password_uri: Option<String>,
    /// The Cloud Storage URI of the keystore file used for SSL encryption.
    /// If not provided, Dataproc will provide a self-signed certificate.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "keystoreUri")]
    pub keystore_uri: Option<String>,
    /// The URI of the KMS key used to encrypt various sensitive files.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kmsKeyUri")]
    pub kms_key_uri: Option<String>,
    /// The name of the on-cluster Kerberos realm. If not specified, the
    /// uppercased domain of hostnames will be the realm.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub realm: Option<String>,
    /// The Cloud Storage URI of a KMS encrypted file
    /// containing the root principal password.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "rootPrincipalPasswordUri")]
    pub root_principal_password_uri: Option<String>,
    /// The lifetime of the ticket granting ticket, in hours.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tgtLifetimeHours")]
    pub tgt_lifetime_hours: Option<f64>,
    /// The Cloud Storage URI of a KMS encrypted file
    /// containing the password to the user provided truststore. For the self-signed
    /// certificate, this password is generated by Dataproc.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "truststorePasswordUri")]
    pub truststore_password_uri: Option<String>,
    /// The Cloud Storage URI of the truststore file used for
    /// SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "truststoreUri")]
    pub truststore_uri: Option<String>,
}

/// The config settings for software inside the cluster.
/// Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterForProviderClusterConfigSoftwareConfig {
    /// The Cloud Dataproc image version to use
    /// for the cluster - this controls the sets of software versions
    /// installed onto the nodes when you create clusters. If not specified, defaults to the
    /// latest version. For a list of valid versions see
    /// Cloud Dataproc versions
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "imageVersion")]
    pub image_version: Option<String>,
    /// The set of optional components to activate on the cluster. See Available Optional Components.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "optionalComponents")]
    pub optional_components: Option<Vec<String>>,
    /// A list of override and additional properties (key/value pairs)
    /// used to modify various aspects of the common configuration files used when creating
    /// a cluster. For a list of valid properties please see
    /// Cluster properties
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "overrideProperties")]
    pub override_properties: Option<HashMap<String, String>>,
}

/// The Google Compute Engine config settings for the worker instances
/// in a cluster. Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterForProviderClusterConfigWorkerConfig {
    /// The Compute Engine accelerator configuration for these instances. Can be specified multiple times.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub accelerators: Option<Vec<ClusterForProviderClusterConfigWorkerConfigAccelerators>>,
    /// Disk Config
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "diskConfig")]
    pub disk_config: Option<ClusterForProviderClusterConfigWorkerConfigDiskConfig>,
    /// The URI for the image to use for this worker.  See the guide
    /// for more information.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "imageUri")]
    pub image_uri: Option<String>,
    /// The name of a Google Compute Engine machine type
    /// to create for the worker nodes. If not specified, GCP will default to a predetermined
    /// computed value (currently n1-standard-4).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "machineType")]
    pub machine_type: Option<String>,
    /// The name of a minimum generation of CPU family
    /// for the master. If not specified, GCP will default to a predetermined computed value
    /// for each zone. See the guide
    /// for details about which CPU families are available (and defaulted) for each zone.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "minCpuPlatform")]
    pub min_cpu_platform: Option<String>,
    /// The minimum number of primary worker instances to create.  If min_num_instances is set, cluster creation will succeed if the number of primary workers created is at least equal to the min_num_instances number.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "minNumInstances")]
    pub min_num_instances: Option<f64>,
    /// Specifies the number of worker nodes to create.
    /// If not specified, GCP will default to a predetermined computed value (currently 2).
    /// There is currently a beta feature which allows you to run a
    /// Single Node Cluster.
    /// In order to take advantage of this you need to set
    /// "dataproc:dataproc.allow.zero.workers" = "true" in
    /// cluster_config.software_config.properties
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "numInstances")]
    pub num_instances: Option<f64>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterForProviderClusterConfigWorkerConfigAccelerators {
    /// The number of the accelerator cards of this type exposed to this instance. Often restricted to one of 1, 2, 4, or 8.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "acceleratorCount")]
    pub accelerator_count: Option<f64>,
    /// The short name of the accelerator type to expose to this instance. For example, nvidia-tesla-k80.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "acceleratorType")]
    pub accelerator_type: Option<String>,
}

/// Disk Config
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterForProviderClusterConfigWorkerConfigDiskConfig {
    /// Size of the primary disk attached to each node, specified
    /// in GB. The primary disk contains the boot volume and system libraries, and the
    /// smallest allowed disk size is 10GB. GCP will default to a predetermined
    /// computed value if not set (currently 500GB). Note: If SSDs are not
    /// attached, it also contains the HDFS data blocks and Hadoop working directories.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bootDiskSizeGb")]
    pub boot_disk_size_gb: Option<f64>,
    /// The disk type of the primary disk attached to each node.
    /// One of "pd-ssd" or "pd-standard". Defaults to "pd-standard".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bootDiskType")]
    pub boot_disk_type: Option<String>,
    /// Optional. Interface type of local SSDs (default is "scsi").
    /// Valid values: "scsi" (Small Computer System Interface), "nvme" (Non-Volatile
    /// Memory Express). See
    /// local SSD performance.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "localSsdInterface")]
    pub local_ssd_interface: Option<String>,
    /// The amount of local SSD disks that will be
    /// attached to each master cluster node. Defaults to 0.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "numLocalSsds")]
    pub num_local_ssds: Option<f64>,
}

/// Allows you to configure a virtual Dataproc on GKE cluster.
/// Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterForProviderVirtualClusterConfig {
    /// Configuration of auxiliary services used by this cluster.
    /// Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "auxiliaryServicesConfig")]
    pub auxiliary_services_config: Option<ClusterForProviderVirtualClusterConfigAuxiliaryServicesConfig>,
    /// The configuration for running the Dataproc cluster on Kubernetes.
    /// Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kubernetesClusterConfig")]
    pub kubernetes_cluster_config: Option<ClusterForProviderVirtualClusterConfigKubernetesClusterConfig>,
    /// The Cloud Storage staging bucket used to stage files,
    /// such as Hadoop jars, between client machines and the cluster.
    /// Note: If you don't explicitly specify a staging_bucket
    /// then GCP will auto create / assign one for you. However, you are not guaranteed
    /// an auto generated bucket which is solely dedicated to your cluster; it may be shared
    /// with other clusters in the same region/zone also choosing to use the auto generation
    /// option.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "stagingBucket")]
    pub staging_bucket: Option<String>,
}

/// Configuration of auxiliary services used by this cluster.
/// Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterForProviderVirtualClusterConfigAuxiliaryServicesConfig {
    /// The config setting for metastore service with the cluster.
    /// Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "metastoreConfig")]
    pub metastore_config: Option<ClusterForProviderVirtualClusterConfigAuxiliaryServicesConfigMetastoreConfig>,
    /// The Spark History Server configuration for the workload.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sparkHistoryServerConfig")]
    pub spark_history_server_config: Option<ClusterForProviderVirtualClusterConfigAuxiliaryServicesConfigSparkHistoryServerConfig>,
}

/// The config setting for metastore service with the cluster.
/// Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterForProviderVirtualClusterConfigAuxiliaryServicesConfigMetastoreConfig {
    /// Resource name of an existing Dataproc Metastore service.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dataprocMetastoreService")]
    pub dataproc_metastore_service: Option<String>,
}

/// The Spark History Server configuration for the workload.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterForProviderVirtualClusterConfigAuxiliaryServicesConfigSparkHistoryServerConfig {
    /// Resource name of an existing Dataproc Cluster to act as a Spark History Server for the workload.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dataprocCluster")]
    pub dataproc_cluster: Option<String>,
}

/// The configuration for running the Dataproc cluster on Kubernetes.
/// Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterForProviderVirtualClusterConfigKubernetesClusterConfig {
    /// The configuration for running the Dataproc cluster on GKE.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gkeClusterConfig")]
    pub gke_cluster_config: Option<ClusterForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfig>,
    /// A namespace within the Kubernetes cluster to deploy into.
    /// If this namespace does not exist, it is created.
    /// If it  exists, Dataproc verifies that another Dataproc VirtualCluster is not installed into it.
    /// If not specified, the name of the Dataproc Cluster is used.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kubernetesNamespace")]
    pub kubernetes_namespace: Option<String>,
    /// The software configuration for this Dataproc cluster running on Kubernetes.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kubernetesSoftwareConfig")]
    pub kubernetes_software_config: Option<ClusterForProviderVirtualClusterConfigKubernetesClusterConfigKubernetesSoftwareConfig>,
}

/// The configuration for running the Dataproc cluster on GKE.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfig {
    /// A target GKE cluster to deploy to. It must be in the same project and region as the Dataproc cluster
    /// (the GKE cluster can be zonal or regional)
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gkeClusterTarget")]
    pub gke_cluster_target: Option<String>,
    /// GKE node pools where workloads will be scheduled. At least one node pool must be assigned the DEFAULT
    /// GkeNodePoolTarget.Role. If a GkeNodePoolTarget is not specified, Dataproc constructs a DEFAULT GkeNodePoolTarget.
    /// Each role can be given to only one GkeNodePoolTarget. All node pools must have the same location settings.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodePoolTarget")]
    pub node_pool_target: Option<Vec<ClusterForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTarget>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTarget {
    /// The target GKE node pool.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodePool")]
    pub node_pool: Option<String>,
    /// (Input only) The configuration for the GKE node pool.
    /// If specified, Dataproc attempts to create a node pool with the specified shape.
    /// If one with the same name already exists, it is verified against all specified fields.
    /// If a field differs, the virtual cluster creation will fail.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodePoolConfig")]
    pub node_pool_config: Option<ClusterForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfig>,
    /// The roles associated with the GKE node pool.
    /// One of "DEFAULT", "CONTROLLER", "SPARK_DRIVER" or "SPARK_EXECUTOR".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub roles: Option<Vec<String>>,
}

/// (Input only) The configuration for the GKE node pool.
/// If specified, Dataproc attempts to create a node pool with the specified shape.
/// If one with the same name already exists, it is verified against all specified fields.
/// If a field differs, the virtual cluster creation will fail.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfig {
    /// The autoscaler configuration for this node pool.
    /// The autoscaler is enabled only when a valid configuration is present.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub autoscaling: Option<ClusterForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigAutoscaling>,
    /// The node pool configuration.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub config: Option<ClusterForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfig>,
    /// The list of Compute Engine zones where node pool nodes associated
    /// with a Dataproc on GKE virtual cluster will be located.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub locations: Option<Vec<String>>,
}

/// The autoscaler configuration for this node pool.
/// The autoscaler is enabled only when a valid configuration is present.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigAutoscaling {
    /// The maximum number of nodes in the node pool. Must be >= minNodeCount, and must be > 0.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxNodeCount")]
    pub max_node_count: Option<f64>,
    /// The minimum number of nodes in the node pool. Must be >= 0 and <= maxNodeCount.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "minNodeCount")]
    pub min_node_count: Option<f64>,
}

/// The node pool configuration.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterForProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfig {
    /// The number of local SSD disks to attach to the node,
    /// which is limited by the maximum number of disks allowable per zone.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "localSsdCount")]
    pub local_ssd_count: Option<f64>,
    /// The name of a Compute Engine machine type.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "machineType")]
    pub machine_type: Option<String>,
    /// Minimum CPU platform to be used by this instance.
    /// The instance may be scheduled on the specified or a newer CPU platform.
    /// Specify the friendly names of CPU platforms, such as "Intel Haswell" or "Intel Sandy Bridge".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "minCpuPlatform")]
    pub min_cpu_platform: Option<String>,
    /// Whether the nodes are created as preemptible VM instances.
    /// Preemptible nodes cannot be used in a node pool with the CONTROLLER role or in the DEFAULT node pool if the
    /// CONTROLLER role is not assigned (the DEFAULT node pool will assume the CONTROLLER role).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub preemptible: Option<bool>,
    /// Spot flag for enabling Spot VM, which is a rebrand of the existing preemptible flag.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub spot: Option<bool>,
}

/// The software configuration for this Dataproc cluster running on Kubernetes.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterForProviderVirtualClusterConfigKubernetesClusterConfigKubernetesSoftwareConfig {
    /// The components that should be installed in this Dataproc cluster. The key must be a string from the
    /// KubernetesComponent enumeration. The value is the version of the software to be installed. At least one entry must be specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "componentVersion")]
    pub component_version: Option<HashMap<String, String>>,
    /// The properties to set on daemon config files. Property keys are specified in prefix:property format,
    /// for example spark:spark.kubernetes.container.image.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub properties: Option<HashMap<String, String>>,
}

/// THIS IS A BETA FIELD. It will be honored
/// unless the Management Policies feature flag is disabled.
/// InitProvider holds the same fields as ForProvider, with the exception
/// of Identifier and other resource reference fields. The fields that are
/// in InitProvider are merged into ForProvider when the resource is created.
/// The same fields are also added to the terraform ignore_changes hook, to
/// avoid updating them after creation. This is useful for fields that are
/// required on creation, but we do not desire to update them after creation,
/// for example because of an external controller is managing them, like an
/// autoscaler.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterInitProvider {
    /// Allows you to configure various aspects of the cluster.
    /// Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clusterConfig")]
    pub cluster_config: Option<ClusterInitProviderClusterConfig>,
    /// Does not affect auto scaling decomissioning from an autoscaling policy.
    /// Graceful decommissioning allows removing nodes from the cluster without interrupting jobs in progress.
    /// Timeout specifies how long to wait for jobs in progress to finish before forcefully removing nodes (and potentially interrupting jobs).
    /// Default timeout is 0 (for forceful decommission), and the maximum allowed timeout is 1 day. (see JSON representation of
    /// Duration).
    /// Only supported on Dataproc image versions 1.2 and higher.
    /// For more context see the docs
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gracefulDecommissionTimeout")]
    pub graceful_decommission_timeout: Option<String>,
    /// Note: This field is non-authoritative, and will only manage the labels present in your configuration. Please refer to the field effective_labels for all of the labels present on the resource.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub labels: Option<HashMap<String, String>>,
    /// The name of the cluster, unique within the project and
    /// zone.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// The ID of the project in which the cluster will exist. If it
    /// is not provided, the provider project is used.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub project: Option<String>,
    /// The region in which the cluster and associated nodes will be created in.
    /// Defaults to global.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub region: Option<String>,
    /// Allows you to configure a virtual Dataproc on GKE cluster.
    /// Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "virtualClusterConfig")]
    pub virtual_cluster_config: Option<ClusterInitProviderVirtualClusterConfig>,
}

/// Allows you to configure various aspects of the cluster.
/// Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterInitProviderClusterConfig {
    /// The autoscaling policy config associated with the cluster.
    /// Note that once set, if autoscaling_config is the only field set in cluster_config, it can
    /// only be removed by setting policy_uri = "", rather than removing the whole block.
    /// Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "autoscalingConfig")]
    pub autoscaling_config: Option<ClusterInitProviderClusterConfigAutoscalingConfig>,
    /// A Dataproc NodeGroup resource is a group of Dataproc cluster nodes that execute an assigned role.
    /// Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "auxiliaryNodeGroups")]
    pub auxiliary_node_groups: Option<Vec<ClusterInitProviderClusterConfigAuxiliaryNodeGroups>>,
    /// The Compute Engine accelerator (GPU) configuration for these instances. Can be specified multiple times.
    /// Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dataprocMetricConfig")]
    pub dataproc_metric_config: Option<ClusterInitProviderClusterConfigDataprocMetricConfig>,
    /// The Customer managed encryption keys settings for the cluster.
    /// Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "encryptionConfig")]
    pub encryption_config: Option<ClusterInitProviderClusterConfigEncryptionConfig>,
    /// The config settings for port access on the cluster.
    /// Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "endpointConfig")]
    pub endpoint_config: Option<ClusterInitProviderClusterConfigEndpointConfig>,
    /// Common config settings for resources of Google Compute Engine cluster
    /// instances, applicable to all instances in the cluster. Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gceClusterConfig")]
    pub gce_cluster_config: Option<ClusterInitProviderClusterConfigGceClusterConfig>,
    /// Commands to execute on each node after config is completed.
    /// You can specify multiple versions of these. Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initializationAction")]
    pub initialization_action: Option<Vec<ClusterInitProviderClusterConfigInitializationAction>>,
    /// The settings for auto deletion cluster schedule.
    /// Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "lifecycleConfig")]
    pub lifecycle_config: Option<ClusterInitProviderClusterConfigLifecycleConfig>,
    /// The Google Compute Engine config settings for the master instances
    /// in a cluster. Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "masterConfig")]
    pub master_config: Option<ClusterInitProviderClusterConfigMasterConfig>,
    /// The config setting for metastore service with the cluster.
    /// Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "metastoreConfig")]
    pub metastore_config: Option<ClusterInitProviderClusterConfigMetastoreConfig>,
    /// The Google Compute Engine config settings for the additional
    /// instances in a cluster. Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "preemptibleWorkerConfig")]
    pub preemptible_worker_config: Option<ClusterInitProviderClusterConfigPreemptibleWorkerConfig>,
    /// Security related configuration. Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "securityConfig")]
    pub security_config: Option<ClusterInitProviderClusterConfigSecurityConfig>,
    /// The config settings for software inside the cluster.
    /// Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "softwareConfig")]
    pub software_config: Option<ClusterInitProviderClusterConfigSoftwareConfig>,
    /// The Cloud Storage staging bucket used to stage files,
    /// such as Hadoop jars, between client machines and the cluster.
    /// Note: If you don't explicitly specify a staging_bucket
    /// then GCP will auto create / assign one for you. However, you are not guaranteed
    /// an auto generated bucket which is solely dedicated to your cluster; it may be shared
    /// with other clusters in the same region/zone also choosing to use the auto generation
    /// option.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "stagingBucket")]
    pub staging_bucket: Option<String>,
    /// The Cloud Storage temp bucket used to store ephemeral cluster
    /// and jobs data, such as Spark and MapReduce history files.
    /// Note: If you don't explicitly specify a temp_bucket then GCP will auto create / assign one for you.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tempBucket")]
    pub temp_bucket: Option<String>,
    /// The Google Compute Engine config settings for the worker instances
    /// in a cluster. Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "workerConfig")]
    pub worker_config: Option<ClusterInitProviderClusterConfigWorkerConfig>,
}

/// The autoscaling policy config associated with the cluster.
/// Note that once set, if autoscaling_config is the only field set in cluster_config, it can
/// only be removed by setting policy_uri = "", rather than removing the whole block.
/// Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterInitProviderClusterConfigAutoscalingConfig {
    /// The autoscaling policy used by the cluster.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "policyUri")]
    pub policy_uri: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterInitProviderClusterConfigAuxiliaryNodeGroups {
    /// Node group configuration.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodeGroup")]
    pub node_group: Option<Vec<ClusterInitProviderClusterConfigAuxiliaryNodeGroupsNodeGroup>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodeGroupId")]
    pub node_group_id: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterInitProviderClusterConfigAuxiliaryNodeGroupsNodeGroup {
    /// The node group instance group configuration.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodeGroupConfig")]
    pub node_group_config: Option<ClusterInitProviderClusterConfigAuxiliaryNodeGroupsNodeGroupNodeGroupConfig>,
    /// The roles associated with the GKE node pool.
    /// One of "DEFAULT", "CONTROLLER", "SPARK_DRIVER" or "SPARK_EXECUTOR".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub roles: Option<Vec<String>>,
}

/// The node group instance group configuration.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterInitProviderClusterConfigAuxiliaryNodeGroupsNodeGroupNodeGroupConfig {
    /// The Compute Engine accelerator configuration for these instances. Can be specified multiple times.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub accelerators: Option<Vec<ClusterInitProviderClusterConfigAuxiliaryNodeGroupsNodeGroupNodeGroupConfigAccelerators>>,
    /// Disk Config
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "diskConfig")]
    pub disk_config: Option<ClusterInitProviderClusterConfigAuxiliaryNodeGroupsNodeGroupNodeGroupConfigDiskConfig>,
    /// The name of a Compute Engine machine type.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "machineType")]
    pub machine_type: Option<String>,
    /// Minimum CPU platform to be used by this instance.
    /// The instance may be scheduled on the specified or a newer CPU platform.
    /// Specify the friendly names of CPU platforms, such as "Intel Haswell" or "Intel Sandy Bridge".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "minCpuPlatform")]
    pub min_cpu_platform: Option<String>,
    /// Specifies the number of master nodes to create.
    /// Please set a number greater than 0. Node Group must have at least 1 instance.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "numInstances")]
    pub num_instances: Option<f64>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterInitProviderClusterConfigAuxiliaryNodeGroupsNodeGroupNodeGroupConfigAccelerators {
    /// The number of the accelerator cards of this type exposed to this instance. Often restricted to one of 1, 2, 4, or 8.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "acceleratorCount")]
    pub accelerator_count: Option<f64>,
    /// The short name of the accelerator type to expose to this instance. For example, nvidia-tesla-k80.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "acceleratorType")]
    pub accelerator_type: Option<String>,
}

/// Disk Config
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterInitProviderClusterConfigAuxiliaryNodeGroupsNodeGroupNodeGroupConfigDiskConfig {
    /// Size of the primary disk attached to each node, specified
    /// in GB. The primary disk contains the boot volume and system libraries, and the
    /// smallest allowed disk size is 10GB. GCP will default to a predetermined
    /// computed value if not set (currently 500GB). Note: If SSDs are not
    /// attached, it also contains the HDFS data blocks and Hadoop working directories.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bootDiskSizeGb")]
    pub boot_disk_size_gb: Option<f64>,
    /// The disk type of the primary disk attached to each node.
    /// One of "pd-ssd" or "pd-standard". Defaults to "pd-standard".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bootDiskType")]
    pub boot_disk_type: Option<String>,
    /// Optional. Interface type of local SSDs (default is "scsi").
    /// Valid values: "scsi" (Small Computer System Interface), "nvme" (Non-Volatile
    /// Memory Express). See
    /// local SSD performance.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "localSsdInterface")]
    pub local_ssd_interface: Option<String>,
    /// The amount of local SSD disks that will be
    /// attached to each master cluster node. Defaults to 0.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "numLocalSsds")]
    pub num_local_ssds: Option<f64>,
}

/// The Compute Engine accelerator (GPU) configuration for these instances. Can be specified multiple times.
/// Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterInitProviderClusterConfigDataprocMetricConfig {
    /// Metrics sources to enable.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub metrics: Option<Vec<ClusterInitProviderClusterConfigDataprocMetricConfigMetrics>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterInitProviderClusterConfigDataprocMetricConfigMetrics {
    /// One or more [available OSS metrics] (https://cloud.google.com/dataproc/docs/guides/monitoring#available_oss_metrics) to collect for the metric course.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "metricOverrides")]
    pub metric_overrides: Option<Vec<String>>,
    /// A source for the collection of Dataproc OSS metrics (see available OSS metrics).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "metricSource")]
    pub metric_source: Option<String>,
}

/// The Customer managed encryption keys settings for the cluster.
/// Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterInitProviderClusterConfigEncryptionConfig {
    /// The Cloud KMS key name to use for PD disk encryption for
    /// all instances in the cluster.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kmsKeyName")]
    pub kms_key_name: Option<String>,
}

/// The config settings for port access on the cluster.
/// Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterInitProviderClusterConfigEndpointConfig {
    /// The flag to enable http access to specific ports
    /// on the cluster from external sources (aka Component Gateway). Defaults to false.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableHttpPortAccess")]
    pub enable_http_port_access: Option<bool>,
}

/// Common config settings for resources of Google Compute Engine cluster
/// instances, applicable to all instances in the cluster. Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterInitProviderClusterConfigGceClusterConfig {
    /// By default, clusters are not restricted to internal IP addresses,
    /// and will have ephemeral external IP addresses assigned to each instance. If set to true, all
    /// instances in the cluster will only have internal IP addresses. Note: Private Google Access
    /// (also known as privateIpGoogleAccess) must be enabled on the subnetwork that the cluster
    /// will be launched in.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "internalIpOnly")]
    pub internal_ip_only: Option<bool>,
    /// A map of the Compute Engine metadata entries to add to all instances
    /// (see Project and instance metadata).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub metadata: Option<HashMap<String, String>>,
    /// The name or self_link of the Google Compute Engine
    /// network to the cluster will be part of. Conflicts with subnetwork.
    /// If neither is specified, this defaults to the "default" network.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub network: Option<String>,
    /// Node Group Affinity for sole-tenant clusters.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodeGroupAffinity")]
    pub node_group_affinity: Option<ClusterInitProviderClusterConfigGceClusterConfigNodeGroupAffinity>,
    /// Reservation Affinity for consuming zonal reservation.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "reservationAffinity")]
    pub reservation_affinity: Option<ClusterInitProviderClusterConfigGceClusterConfigReservationAffinity>,
    /// The service account to be used by the Node VMs.
    /// If not specified, the "default" service account is used.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceAccount")]
    pub service_account: Option<String>,
    /// Reference to a ServiceAccount in cloudplatform to populate serviceAccount.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceAccountRef")]
    pub service_account_ref: Option<ClusterInitProviderClusterConfigGceClusterConfigServiceAccountRef>,
    /// The set of Google API scopes
    /// to be made available on all of the node VMs under the service_account
    /// specified. Both OAuth2 URLs and gcloud
    /// short names are supported. To allow full access to all Cloud APIs, use the
    /// cloud-platform scope. See a complete list of scopes here.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceAccountScopes")]
    pub service_account_scopes: Option<Vec<String>>,
    /// Selector for a ServiceAccount in cloudplatform to populate serviceAccount.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceAccountSelector")]
    pub service_account_selector: Option<ClusterInitProviderClusterConfigGceClusterConfigServiceAccountSelector>,
    /// Shielded Instance Config for clusters using Compute Engine Shielded VMs.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "shieldedInstanceConfig")]
    pub shielded_instance_config: Option<ClusterInitProviderClusterConfigGceClusterConfigShieldedInstanceConfig>,
    /// The name or self_link of the Google Compute Engine
    /// subnetwork the cluster will be part of. Conflicts with network.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub subnetwork: Option<String>,
    /// The list of instance tags applied to instances in the cluster.
    /// Tags are used to identify valid sources or targets for network firewalls.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tags: Option<Vec<String>>,
    /// The GCP zone where your data is stored and used (i.e. where
    /// the master and the worker nodes will be created in). If region is set to 'global' (default)
    /// then zone is mandatory, otherwise GCP is able to make use of Auto Zone Placement
    /// to determine this automatically for you.
    /// Note: This setting additionally determines and restricts
    /// which computing resources are available for use with other configs such as
    /// cluster_config.master_config.machine_type and cluster_config.worker_config.machine_type.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub zone: Option<String>,
}

/// Node Group Affinity for sole-tenant clusters.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterInitProviderClusterConfigGceClusterConfigNodeGroupAffinity {
    /// The URI of a sole-tenant node group resource that the cluster will be created on.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodeGroupUri")]
    pub node_group_uri: Option<String>,
}

/// Reservation Affinity for consuming zonal reservation.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterInitProviderClusterConfigGceClusterConfigReservationAffinity {
    /// Corresponds to the type of reservation consumption.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "consumeReservationType")]
    pub consume_reservation_type: Option<String>,
    /// Corresponds to the label key of reservation resource.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub key: Option<String>,
    /// Corresponds to the label values of reservation resource.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// Reference to a ServiceAccount in cloudplatform to populate serviceAccount.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterInitProviderClusterConfigGceClusterConfigServiceAccountRef {
    /// Name of the referenced object.
    pub name: String,
    /// Policies for referencing.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub policy: Option<ClusterInitProviderClusterConfigGceClusterConfigServiceAccountRefPolicy>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterInitProviderClusterConfigGceClusterConfigServiceAccountRefPolicy {
    /// Resolution specifies whether resolution of this reference is required.
    /// The default is 'Required', which means the reconcile will fail if the
    /// reference cannot be resolved. 'Optional' means this reference will be
    /// a no-op if it cannot be resolved.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolution: Option<ClusterInitProviderClusterConfigGceClusterConfigServiceAccountRefPolicyResolution>,
    /// Resolve specifies when this reference should be resolved. The default
    /// is 'IfNotPresent', which will attempt to resolve the reference only when
    /// the corresponding field is not present. Use 'Always' to resolve the
    /// reference on every reconcile.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolve: Option<ClusterInitProviderClusterConfigGceClusterConfigServiceAccountRefPolicyResolve>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum ClusterInitProviderClusterConfigGceClusterConfigServiceAccountRefPolicyResolution {
    Required,
    Optional,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum ClusterInitProviderClusterConfigGceClusterConfigServiceAccountRefPolicyResolve {
    Always,
    IfNotPresent,
}

/// Selector for a ServiceAccount in cloudplatform to populate serviceAccount.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterInitProviderClusterConfigGceClusterConfigServiceAccountSelector {
    /// MatchControllerRef ensures an object with the same controller reference
    /// as the selecting object is selected.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchControllerRef")]
    pub match_controller_ref: Option<bool>,
    /// MatchLabels ensures an object with matching labels is selected.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    pub match_labels: Option<HashMap<String, String>>,
    /// Policies for selection.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub policy: Option<ClusterInitProviderClusterConfigGceClusterConfigServiceAccountSelectorPolicy>,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterInitProviderClusterConfigGceClusterConfigServiceAccountSelectorPolicy {
    /// Resolution specifies whether resolution of this reference is required.
    /// The default is 'Required', which means the reconcile will fail if the
    /// reference cannot be resolved. 'Optional' means this reference will be
    /// a no-op if it cannot be resolved.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolution: Option<ClusterInitProviderClusterConfigGceClusterConfigServiceAccountSelectorPolicyResolution>,
    /// Resolve specifies when this reference should be resolved. The default
    /// is 'IfNotPresent', which will attempt to resolve the reference only when
    /// the corresponding field is not present. Use 'Always' to resolve the
    /// reference on every reconcile.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolve: Option<ClusterInitProviderClusterConfigGceClusterConfigServiceAccountSelectorPolicyResolve>,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum ClusterInitProviderClusterConfigGceClusterConfigServiceAccountSelectorPolicyResolution {
    Required,
    Optional,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum ClusterInitProviderClusterConfigGceClusterConfigServiceAccountSelectorPolicyResolve {
    Always,
    IfNotPresent,
}

/// Shielded Instance Config for clusters using Compute Engine Shielded VMs.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterInitProviderClusterConfigGceClusterConfigShieldedInstanceConfig {
    /// Defines whether instances have integrity monitoring enabled.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableIntegrityMonitoring")]
    pub enable_integrity_monitoring: Option<bool>,
    /// Defines whether instances have Secure Boot enabled.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableSecureBoot")]
    pub enable_secure_boot: Option<bool>,
    /// Defines whether instances have the vTPM enabled.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableVtpm")]
    pub enable_vtpm: Option<bool>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterInitProviderClusterConfigInitializationAction {
    /// The script to be executed during initialization of the cluster.
    /// The script must be a GCS file with a gs:// prefix.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub script: Option<String>,
    /// The maximum duration (in seconds) which script is
    /// allowed to take to execute its action. GCP will default to a predetermined
    /// computed value if not set (currently 300).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "timeoutSec")]
    pub timeout_sec: Option<f64>,
}

/// The settings for auto deletion cluster schedule.
/// Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterInitProviderClusterConfigLifecycleConfig {
    /// The time when cluster will be auto-deleted.
    /// A timestamp in RFC3339 UTC "Zulu" format, accurate to nanoseconds.
    /// Example: "2014-10-02T15:01:23.045123456Z".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "autoDeleteTime")]
    pub auto_delete_time: Option<String>,
    /// The duration to keep the cluster alive while idling
    /// (no jobs running). After this TTL, the cluster will be deleted. Valid range: [10m, 14d].
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "idleDeleteTtl")]
    pub idle_delete_ttl: Option<String>,
}

/// The Google Compute Engine config settings for the master instances
/// in a cluster. Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterInitProviderClusterConfigMasterConfig {
    /// The Compute Engine accelerator (GPU) configuration for these instances. Can be specified multiple times.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub accelerators: Option<Vec<ClusterInitProviderClusterConfigMasterConfigAccelerators>>,
    /// Disk Config
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "diskConfig")]
    pub disk_config: Option<ClusterInitProviderClusterConfigMasterConfigDiskConfig>,
    /// The URI for the image to use for this worker.  See the guide
    /// for more information.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "imageUri")]
    pub image_uri: Option<String>,
    /// The name of a Google Compute Engine machine type
    /// to create for the master. If not specified, GCP will default to a predetermined
    /// computed value (currently n1-standard-4).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "machineType")]
    pub machine_type: Option<String>,
    /// The name of a minimum generation of CPU family
    /// for the master. If not specified, GCP will default to a predetermined computed value
    /// for each zone. See the guide
    /// for details about which CPU families are available (and defaulted) for each zone.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "minCpuPlatform")]
    pub min_cpu_platform: Option<String>,
    /// Specifies the number of master nodes to create.
    /// If not specified, GCP will default to a predetermined computed value (currently 1).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "numInstances")]
    pub num_instances: Option<f64>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterInitProviderClusterConfigMasterConfigAccelerators {
    /// The number of the accelerator cards of this type exposed to this instance. Often restricted to one of 1, 2, 4, or 8.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "acceleratorCount")]
    pub accelerator_count: Option<f64>,
    /// The short name of the accelerator type to expose to this instance. For example, nvidia-tesla-k80.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "acceleratorType")]
    pub accelerator_type: Option<String>,
}

/// Disk Config
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterInitProviderClusterConfigMasterConfigDiskConfig {
    /// Size of the primary disk attached to each node, specified
    /// in GB. The primary disk contains the boot volume and system libraries, and the
    /// smallest allowed disk size is 10GB. GCP will default to a predetermined
    /// computed value if not set (currently 500GB). Note: If SSDs are not
    /// attached, it also contains the HDFS data blocks and Hadoop working directories.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bootDiskSizeGb")]
    pub boot_disk_size_gb: Option<f64>,
    /// The disk type of the primary disk attached to each node.
    /// One of "pd-ssd" or "pd-standard". Defaults to "pd-standard".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bootDiskType")]
    pub boot_disk_type: Option<String>,
    /// Optional. Interface type of local SSDs (default is "scsi").
    /// Valid values: "scsi" (Small Computer System Interface), "nvme" (Non-Volatile
    /// Memory Express). See
    /// local SSD performance.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "localSsdInterface")]
    pub local_ssd_interface: Option<String>,
    /// The amount of local SSD disks that will be
    /// attached to each master cluster node. Defaults to 0.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "numLocalSsds")]
    pub num_local_ssds: Option<f64>,
}

/// The config setting for metastore service with the cluster.
/// Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterInitProviderClusterConfigMetastoreConfig {
    /// Resource name of an existing Dataproc Metastore service.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dataprocMetastoreService")]
    pub dataproc_metastore_service: Option<String>,
}

/// The Google Compute Engine config settings for the additional
/// instances in a cluster. Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterInitProviderClusterConfigPreemptibleWorkerConfig {
    /// Disk Config
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "diskConfig")]
    pub disk_config: Option<ClusterInitProviderClusterConfigPreemptibleWorkerConfigDiskConfig>,
    /// Instance flexibility Policy allowing a mixture of VM shapes and provisioning models.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "instanceFlexibilityPolicy")]
    pub instance_flexibility_policy: Option<ClusterInitProviderClusterConfigPreemptibleWorkerConfigInstanceFlexibilityPolicy>,
    /// Specifies the number of preemptible nodes to create.
    /// Defaults to 0.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "numInstances")]
    pub num_instances: Option<f64>,
    /// Specifies the preemptibility of the secondary workers. The default value is PREEMPTIBLE
    /// Accepted values are:
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub preemptibility: Option<String>,
}

/// Disk Config
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterInitProviderClusterConfigPreemptibleWorkerConfigDiskConfig {
    /// Size of the primary disk attached to each node, specified
    /// in GB. The primary disk contains the boot volume and system libraries, and the
    /// smallest allowed disk size is 10GB. GCP will default to a predetermined
    /// computed value if not set (currently 500GB). Note: If SSDs are not
    /// attached, it also contains the HDFS data blocks and Hadoop working directories.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bootDiskSizeGb")]
    pub boot_disk_size_gb: Option<f64>,
    /// The disk type of the primary disk attached to each node.
    /// One of "pd-ssd" or "pd-standard". Defaults to "pd-standard".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bootDiskType")]
    pub boot_disk_type: Option<String>,
    /// Optional. Interface type of local SSDs (default is "scsi").
    /// Valid values: "scsi" (Small Computer System Interface), "nvme" (Non-Volatile
    /// Memory Express). See
    /// local SSD performance.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "localSsdInterface")]
    pub local_ssd_interface: Option<String>,
    /// The amount of local SSD disks that will be
    /// attached to each master cluster node. Defaults to 0.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "numLocalSsds")]
    pub num_local_ssds: Option<f64>,
}

/// Instance flexibility Policy allowing a mixture of VM shapes and provisioning models.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterInitProviderClusterConfigPreemptibleWorkerConfigInstanceFlexibilityPolicy {
    /// List of instance selection options that the group will use when creating new VMs.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "instanceSelectionList")]
    pub instance_selection_list: Option<Vec<ClusterInitProviderClusterConfigPreemptibleWorkerConfigInstanceFlexibilityPolicyInstanceSelectionList>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterInitProviderClusterConfigPreemptibleWorkerConfigInstanceFlexibilityPolicyInstanceSelectionList {
    /// Full machine-type names, e.g. "n1-standard-16".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "machineTypes")]
    pub machine_types: Option<Vec<String>>,
    /// Preference of this instance selection. A lower number means higher preference. Dataproc will first try to create a VM based on the machine-type with priority rank and fallback to next rank based on availability. Machine types and instance selections with the same priority have the same preference.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub rank: Option<f64>,
}

/// Security related configuration. Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterInitProviderClusterConfigSecurityConfig {
    /// Kerberos Configuration
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kerberosConfig")]
    pub kerberos_config: Option<ClusterInitProviderClusterConfigSecurityConfigKerberosConfig>,
}

/// Kerberos Configuration
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterInitProviderClusterConfigSecurityConfigKerberosConfig {
    /// The admin server (IP or hostname) for the
    /// remote trusted realm in a cross realm trust relationship.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "crossRealmTrustAdminServer")]
    pub cross_realm_trust_admin_server: Option<String>,
    /// The KDC (IP or hostname) for the
    /// remote trusted realm in a cross realm trust relationship.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "crossRealmTrustKdc")]
    pub cross_realm_trust_kdc: Option<String>,
    /// The remote realm the Dataproc on-cluster KDC will
    /// trust, should the user enable cross realm trust.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "crossRealmTrustRealm")]
    pub cross_realm_trust_realm: Option<String>,
    /// The Cloud Storage URI of a KMS
    /// encrypted file containing the shared password between the on-cluster Kerberos realm
    /// and the remote trusted realm, in a cross realm trust relationship.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "crossRealmTrustSharedPasswordUri")]
    pub cross_realm_trust_shared_password_uri: Option<String>,
    /// Flag to indicate whether to Kerberize the cluster.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableKerberos")]
    pub enable_kerberos: Option<bool>,
    /// The Cloud Storage URI of a KMS encrypted file containing
    /// the master key of the KDC database.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kdcDbKeyUri")]
    pub kdc_db_key_uri: Option<String>,
    /// The Cloud Storage URI of a KMS encrypted file containing
    /// the password to the user provided key. For the self-signed certificate, this password
    /// is generated by Dataproc.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "keyPasswordUri")]
    pub key_password_uri: Option<String>,
    /// The Cloud Storage URI of a KMS encrypted file containing
    /// the password to the user provided keystore. For the self-signed certificated, the password
    /// is generated by Dataproc.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "keystorePasswordUri")]
    pub keystore_password_uri: Option<String>,
    /// The Cloud Storage URI of the keystore file used for SSL encryption.
    /// If not provided, Dataproc will provide a self-signed certificate.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "keystoreUri")]
    pub keystore_uri: Option<String>,
    /// The URI of the KMS key used to encrypt various sensitive files.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kmsKeyUri")]
    pub kms_key_uri: Option<String>,
    /// The name of the on-cluster Kerberos realm. If not specified, the
    /// uppercased domain of hostnames will be the realm.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub realm: Option<String>,
    /// The Cloud Storage URI of a KMS encrypted file
    /// containing the root principal password.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "rootPrincipalPasswordUri")]
    pub root_principal_password_uri: Option<String>,
    /// The lifetime of the ticket granting ticket, in hours.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tgtLifetimeHours")]
    pub tgt_lifetime_hours: Option<f64>,
    /// The Cloud Storage URI of a KMS encrypted file
    /// containing the password to the user provided truststore. For the self-signed
    /// certificate, this password is generated by Dataproc.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "truststorePasswordUri")]
    pub truststore_password_uri: Option<String>,
    /// The Cloud Storage URI of the truststore file used for
    /// SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "truststoreUri")]
    pub truststore_uri: Option<String>,
}

/// The config settings for software inside the cluster.
/// Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterInitProviderClusterConfigSoftwareConfig {
    /// The Cloud Dataproc image version to use
    /// for the cluster - this controls the sets of software versions
    /// installed onto the nodes when you create clusters. If not specified, defaults to the
    /// latest version. For a list of valid versions see
    /// Cloud Dataproc versions
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "imageVersion")]
    pub image_version: Option<String>,
    /// The set of optional components to activate on the cluster. See Available Optional Components.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "optionalComponents")]
    pub optional_components: Option<Vec<String>>,
    /// A list of override and additional properties (key/value pairs)
    /// used to modify various aspects of the common configuration files used when creating
    /// a cluster. For a list of valid properties please see
    /// Cluster properties
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "overrideProperties")]
    pub override_properties: Option<HashMap<String, String>>,
}

/// The Google Compute Engine config settings for the worker instances
/// in a cluster. Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterInitProviderClusterConfigWorkerConfig {
    /// The Compute Engine accelerator configuration for these instances. Can be specified multiple times.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub accelerators: Option<Vec<ClusterInitProviderClusterConfigWorkerConfigAccelerators>>,
    /// Disk Config
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "diskConfig")]
    pub disk_config: Option<ClusterInitProviderClusterConfigWorkerConfigDiskConfig>,
    /// The URI for the image to use for this worker.  See the guide
    /// for more information.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "imageUri")]
    pub image_uri: Option<String>,
    /// The name of a Google Compute Engine machine type
    /// to create for the worker nodes. If not specified, GCP will default to a predetermined
    /// computed value (currently n1-standard-4).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "machineType")]
    pub machine_type: Option<String>,
    /// The name of a minimum generation of CPU family
    /// for the master. If not specified, GCP will default to a predetermined computed value
    /// for each zone. See the guide
    /// for details about which CPU families are available (and defaulted) for each zone.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "minCpuPlatform")]
    pub min_cpu_platform: Option<String>,
    /// The minimum number of primary worker instances to create.  If min_num_instances is set, cluster creation will succeed if the number of primary workers created is at least equal to the min_num_instances number.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "minNumInstances")]
    pub min_num_instances: Option<f64>,
    /// Specifies the number of worker nodes to create.
    /// If not specified, GCP will default to a predetermined computed value (currently 2).
    /// There is currently a beta feature which allows you to run a
    /// Single Node Cluster.
    /// In order to take advantage of this you need to set
    /// "dataproc:dataproc.allow.zero.workers" = "true" in
    /// cluster_config.software_config.properties
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "numInstances")]
    pub num_instances: Option<f64>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterInitProviderClusterConfigWorkerConfigAccelerators {
    /// The number of the accelerator cards of this type exposed to this instance. Often restricted to one of 1, 2, 4, or 8.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "acceleratorCount")]
    pub accelerator_count: Option<f64>,
    /// The short name of the accelerator type to expose to this instance. For example, nvidia-tesla-k80.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "acceleratorType")]
    pub accelerator_type: Option<String>,
}

/// Disk Config
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterInitProviderClusterConfigWorkerConfigDiskConfig {
    /// Size of the primary disk attached to each node, specified
    /// in GB. The primary disk contains the boot volume and system libraries, and the
    /// smallest allowed disk size is 10GB. GCP will default to a predetermined
    /// computed value if not set (currently 500GB). Note: If SSDs are not
    /// attached, it also contains the HDFS data blocks and Hadoop working directories.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bootDiskSizeGb")]
    pub boot_disk_size_gb: Option<f64>,
    /// The disk type of the primary disk attached to each node.
    /// One of "pd-ssd" or "pd-standard". Defaults to "pd-standard".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bootDiskType")]
    pub boot_disk_type: Option<String>,
    /// Optional. Interface type of local SSDs (default is "scsi").
    /// Valid values: "scsi" (Small Computer System Interface), "nvme" (Non-Volatile
    /// Memory Express). See
    /// local SSD performance.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "localSsdInterface")]
    pub local_ssd_interface: Option<String>,
    /// The amount of local SSD disks that will be
    /// attached to each master cluster node. Defaults to 0.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "numLocalSsds")]
    pub num_local_ssds: Option<f64>,
}

/// Allows you to configure a virtual Dataproc on GKE cluster.
/// Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterInitProviderVirtualClusterConfig {
    /// Configuration of auxiliary services used by this cluster.
    /// Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "auxiliaryServicesConfig")]
    pub auxiliary_services_config: Option<ClusterInitProviderVirtualClusterConfigAuxiliaryServicesConfig>,
    /// The configuration for running the Dataproc cluster on Kubernetes.
    /// Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kubernetesClusterConfig")]
    pub kubernetes_cluster_config: Option<ClusterInitProviderVirtualClusterConfigKubernetesClusterConfig>,
    /// The Cloud Storage staging bucket used to stage files,
    /// such as Hadoop jars, between client machines and the cluster.
    /// Note: If you don't explicitly specify a staging_bucket
    /// then GCP will auto create / assign one for you. However, you are not guaranteed
    /// an auto generated bucket which is solely dedicated to your cluster; it may be shared
    /// with other clusters in the same region/zone also choosing to use the auto generation
    /// option.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "stagingBucket")]
    pub staging_bucket: Option<String>,
}

/// Configuration of auxiliary services used by this cluster.
/// Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterInitProviderVirtualClusterConfigAuxiliaryServicesConfig {
    /// The config setting for metastore service with the cluster.
    /// Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "metastoreConfig")]
    pub metastore_config: Option<ClusterInitProviderVirtualClusterConfigAuxiliaryServicesConfigMetastoreConfig>,
    /// The Spark History Server configuration for the workload.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sparkHistoryServerConfig")]
    pub spark_history_server_config: Option<ClusterInitProviderVirtualClusterConfigAuxiliaryServicesConfigSparkHistoryServerConfig>,
}

/// The config setting for metastore service with the cluster.
/// Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterInitProviderVirtualClusterConfigAuxiliaryServicesConfigMetastoreConfig {
    /// Resource name of an existing Dataproc Metastore service.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dataprocMetastoreService")]
    pub dataproc_metastore_service: Option<String>,
}

/// The Spark History Server configuration for the workload.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterInitProviderVirtualClusterConfigAuxiliaryServicesConfigSparkHistoryServerConfig {
    /// Resource name of an existing Dataproc Cluster to act as a Spark History Server for the workload.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dataprocCluster")]
    pub dataproc_cluster: Option<String>,
}

/// The configuration for running the Dataproc cluster on Kubernetes.
/// Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterInitProviderVirtualClusterConfigKubernetesClusterConfig {
    /// The configuration for running the Dataproc cluster on GKE.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gkeClusterConfig")]
    pub gke_cluster_config: Option<ClusterInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfig>,
    /// A namespace within the Kubernetes cluster to deploy into.
    /// If this namespace does not exist, it is created.
    /// If it  exists, Dataproc verifies that another Dataproc VirtualCluster is not installed into it.
    /// If not specified, the name of the Dataproc Cluster is used.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kubernetesNamespace")]
    pub kubernetes_namespace: Option<String>,
    /// The software configuration for this Dataproc cluster running on Kubernetes.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kubernetesSoftwareConfig")]
    pub kubernetes_software_config: Option<ClusterInitProviderVirtualClusterConfigKubernetesClusterConfigKubernetesSoftwareConfig>,
}

/// The configuration for running the Dataproc cluster on GKE.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfig {
    /// A target GKE cluster to deploy to. It must be in the same project and region as the Dataproc cluster
    /// (the GKE cluster can be zonal or regional)
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gkeClusterTarget")]
    pub gke_cluster_target: Option<String>,
    /// GKE node pools where workloads will be scheduled. At least one node pool must be assigned the DEFAULT
    /// GkeNodePoolTarget.Role. If a GkeNodePoolTarget is not specified, Dataproc constructs a DEFAULT GkeNodePoolTarget.
    /// Each role can be given to only one GkeNodePoolTarget. All node pools must have the same location settings.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodePoolTarget")]
    pub node_pool_target: Option<Vec<ClusterInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTarget>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTarget {
    /// The target GKE node pool.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodePool")]
    pub node_pool: Option<String>,
    /// (Input only) The configuration for the GKE node pool.
    /// If specified, Dataproc attempts to create a node pool with the specified shape.
    /// If one with the same name already exists, it is verified against all specified fields.
    /// If a field differs, the virtual cluster creation will fail.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodePoolConfig")]
    pub node_pool_config: Option<ClusterInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfig>,
    /// The roles associated with the GKE node pool.
    /// One of "DEFAULT", "CONTROLLER", "SPARK_DRIVER" or "SPARK_EXECUTOR".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub roles: Option<Vec<String>>,
}

/// (Input only) The configuration for the GKE node pool.
/// If specified, Dataproc attempts to create a node pool with the specified shape.
/// If one with the same name already exists, it is verified against all specified fields.
/// If a field differs, the virtual cluster creation will fail.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfig {
    /// The autoscaler configuration for this node pool.
    /// The autoscaler is enabled only when a valid configuration is present.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub autoscaling: Option<ClusterInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigAutoscaling>,
    /// The node pool configuration.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub config: Option<ClusterInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfig>,
    /// The list of Compute Engine zones where node pool nodes associated
    /// with a Dataproc on GKE virtual cluster will be located.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub locations: Option<Vec<String>>,
}

/// The autoscaler configuration for this node pool.
/// The autoscaler is enabled only when a valid configuration is present.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigAutoscaling {
    /// The maximum number of nodes in the node pool. Must be >= minNodeCount, and must be > 0.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxNodeCount")]
    pub max_node_count: Option<f64>,
    /// The minimum number of nodes in the node pool. Must be >= 0 and <= maxNodeCount.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "minNodeCount")]
    pub min_node_count: Option<f64>,
}

/// The node pool configuration.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterInitProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfig {
    /// The number of local SSD disks to attach to the node,
    /// which is limited by the maximum number of disks allowable per zone.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "localSsdCount")]
    pub local_ssd_count: Option<f64>,
    /// The name of a Compute Engine machine type.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "machineType")]
    pub machine_type: Option<String>,
    /// Minimum CPU platform to be used by this instance.
    /// The instance may be scheduled on the specified or a newer CPU platform.
    /// Specify the friendly names of CPU platforms, such as "Intel Haswell" or "Intel Sandy Bridge".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "minCpuPlatform")]
    pub min_cpu_platform: Option<String>,
    /// Whether the nodes are created as preemptible VM instances.
    /// Preemptible nodes cannot be used in a node pool with the CONTROLLER role or in the DEFAULT node pool if the
    /// CONTROLLER role is not assigned (the DEFAULT node pool will assume the CONTROLLER role).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub preemptible: Option<bool>,
    /// Spot flag for enabling Spot VM, which is a rebrand of the existing preemptible flag.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub spot: Option<bool>,
}

/// The software configuration for this Dataproc cluster running on Kubernetes.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterInitProviderVirtualClusterConfigKubernetesClusterConfigKubernetesSoftwareConfig {
    /// The components that should be installed in this Dataproc cluster. The key must be a string from the
    /// KubernetesComponent enumeration. The value is the version of the software to be installed. At least one entry must be specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "componentVersion")]
    pub component_version: Option<HashMap<String, String>>,
    /// The properties to set on daemon config files. Property keys are specified in prefix:property format,
    /// for example spark:spark.kubernetes.container.image.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub properties: Option<HashMap<String, String>>,
}

/// ProviderConfigReference specifies how the provider that will be used to
/// create, observe, update, and delete this managed resource should be
/// configured.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterProviderConfigRef {
    /// Name of the referenced object.
    pub name: String,
    /// Policies for referencing.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub policy: Option<ClusterProviderConfigRefPolicy>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterProviderConfigRefPolicy {
    /// Resolution specifies whether resolution of this reference is required.
    /// The default is 'Required', which means the reconcile will fail if the
    /// reference cannot be resolved. 'Optional' means this reference will be
    /// a no-op if it cannot be resolved.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolution: Option<ClusterProviderConfigRefPolicyResolution>,
    /// Resolve specifies when this reference should be resolved. The default
    /// is 'IfNotPresent', which will attempt to resolve the reference only when
    /// the corresponding field is not present. Use 'Always' to resolve the
    /// reference on every reconcile.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolve: Option<ClusterProviderConfigRefPolicyResolve>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum ClusterProviderConfigRefPolicyResolution {
    Required,
    Optional,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum ClusterProviderConfigRefPolicyResolve {
    Always,
    IfNotPresent,
}

/// PublishConnectionDetailsTo specifies the connection secret config which
/// contains a name, metadata and a reference to secret store config to
/// which any connection details for this managed resource should be written.
/// Connection details frequently include the endpoint, username,
/// and password required to connect to the managed resource.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterPublishConnectionDetailsTo {
    /// SecretStoreConfigRef specifies which secret store config should be used
    /// for this ConnectionSecret.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configRef")]
    pub config_ref: Option<ClusterPublishConnectionDetailsToConfigRef>,
    /// Metadata is the metadata for connection secret.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub metadata: Option<ClusterPublishConnectionDetailsToMetadata>,
    /// Name is the name of the connection secret.
    pub name: String,
}

/// SecretStoreConfigRef specifies which secret store config should be used
/// for this ConnectionSecret.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterPublishConnectionDetailsToConfigRef {
    /// Name of the referenced object.
    pub name: String,
    /// Policies for referencing.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub policy: Option<ClusterPublishConnectionDetailsToConfigRefPolicy>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterPublishConnectionDetailsToConfigRefPolicy {
    /// Resolution specifies whether resolution of this reference is required.
    /// The default is 'Required', which means the reconcile will fail if the
    /// reference cannot be resolved. 'Optional' means this reference will be
    /// a no-op if it cannot be resolved.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolution: Option<ClusterPublishConnectionDetailsToConfigRefPolicyResolution>,
    /// Resolve specifies when this reference should be resolved. The default
    /// is 'IfNotPresent', which will attempt to resolve the reference only when
    /// the corresponding field is not present. Use 'Always' to resolve the
    /// reference on every reconcile.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub resolve: Option<ClusterPublishConnectionDetailsToConfigRefPolicyResolve>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum ClusterPublishConnectionDetailsToConfigRefPolicyResolution {
    Required,
    Optional,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum ClusterPublishConnectionDetailsToConfigRefPolicyResolve {
    Always,
    IfNotPresent,
}

/// Metadata is the metadata for connection secret.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterPublishConnectionDetailsToMetadata {
    /// Annotations are the annotations to be added to connection secret.
    /// - For Kubernetes secrets, this will be used as "metadata.annotations".
    /// - It is up to Secret Store implementation for others store types.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub annotations: Option<HashMap<String, String>>,
    /// Labels are the labels/tags to be added to connection secret.
    /// - For Kubernetes secrets, this will be used as "metadata.labels".
    /// - It is up to Secret Store implementation for others store types.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub labels: Option<HashMap<String, String>>,
    /// Type is the SecretType for the connection secret.
    /// - Only valid for Kubernetes Secret Stores.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    pub r#type: Option<String>,
}

/// WriteConnectionSecretToReference specifies the namespace and name of a
/// Secret to which any connection details for this managed resource should
/// be written. Connection details frequently include the endpoint, username,
/// and password required to connect to the managed resource.
/// This field is planned to be replaced in a future release in favor of
/// PublishConnectionDetailsTo. Currently, both could be set independently
/// and connection details would be published to both without affecting
/// each other.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterWriteConnectionSecretToRef {
    /// Name of the secret.
    pub name: String,
    /// Namespace of the secret.
    pub namespace: String,
}

/// ClusterStatus defines the observed state of Cluster.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterStatus {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "atProvider")]
    pub at_provider: Option<ClusterStatusAtProvider>,
    /// Conditions of the resource.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub conditions: Option<Vec<Condition>>,
    /// ObservedGeneration is the latest metadata.generation
    /// which resulted in either a ready state, or stalled due to error
    /// it can not recover from without human intervention.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "observedGeneration")]
    pub observed_generation: Option<i64>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterStatusAtProvider {
    /// Allows you to configure various aspects of the cluster.
    /// Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clusterConfig")]
    pub cluster_config: Option<ClusterStatusAtProviderClusterConfig>,
    /// (Computed) The list of labels (key/value pairs) to be applied to
    /// instances in the cluster. GCP generates some itself including goog-dataproc-cluster-name
    /// which is the name of the cluster.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "effectiveLabels")]
    pub effective_labels: Option<HashMap<String, String>>,
    /// Does not affect auto scaling decomissioning from an autoscaling policy.
    /// Graceful decommissioning allows removing nodes from the cluster without interrupting jobs in progress.
    /// Timeout specifies how long to wait for jobs in progress to finish before forcefully removing nodes (and potentially interrupting jobs).
    /// Default timeout is 0 (for forceful decommission), and the maximum allowed timeout is 1 day. (see JSON representation of
    /// Duration).
    /// Only supported on Dataproc image versions 1.2 and higher.
    /// For more context see the docs
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gracefulDecommissionTimeout")]
    pub graceful_decommission_timeout: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub id: Option<String>,
    /// Note: This field is non-authoritative, and will only manage the labels present in your configuration. Please refer to the field effective_labels for all of the labels present on the resource.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub labels: Option<HashMap<String, String>>,
    /// The name of the cluster, unique within the project and
    /// zone.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// The ID of the project in which the cluster will exist. If it
    /// is not provided, the provider project is used.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub project: Option<String>,
    /// The region in which the cluster and associated nodes will be created in.
    /// Defaults to global.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub region: Option<String>,
    /// The combination of labels configured directly on the resource and default labels configured on the provider.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terraformLabels")]
    pub terraform_labels: Option<HashMap<String, String>>,
    /// Allows you to configure a virtual Dataproc on GKE cluster.
    /// Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "virtualClusterConfig")]
    pub virtual_cluster_config: Option<ClusterStatusAtProviderVirtualClusterConfig>,
}

/// Allows you to configure various aspects of the cluster.
/// Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterStatusAtProviderClusterConfig {
    /// The autoscaling policy config associated with the cluster.
    /// Note that once set, if autoscaling_config is the only field set in cluster_config, it can
    /// only be removed by setting policy_uri = "", rather than removing the whole block.
    /// Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "autoscalingConfig")]
    pub autoscaling_config: Option<ClusterStatusAtProviderClusterConfigAutoscalingConfig>,
    /// A Dataproc NodeGroup resource is a group of Dataproc cluster nodes that execute an assigned role.
    /// Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "auxiliaryNodeGroups")]
    pub auxiliary_node_groups: Option<Vec<ClusterStatusAtProviderClusterConfigAuxiliaryNodeGroups>>,
    /// The name of the cloud storage bucket ultimately used to house the staging data
    /// for the cluster. If staging_bucket is specified, it will contain this value, otherwise
    /// it will be the auto generated name.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub bucket: Option<String>,
    /// The Compute Engine accelerator (GPU) configuration for these instances. Can be specified multiple times.
    /// Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dataprocMetricConfig")]
    pub dataproc_metric_config: Option<ClusterStatusAtProviderClusterConfigDataprocMetricConfig>,
    /// The Customer managed encryption keys settings for the cluster.
    /// Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "encryptionConfig")]
    pub encryption_config: Option<ClusterStatusAtProviderClusterConfigEncryptionConfig>,
    /// The config settings for port access on the cluster.
    /// Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "endpointConfig")]
    pub endpoint_config: Option<ClusterStatusAtProviderClusterConfigEndpointConfig>,
    /// Common config settings for resources of Google Compute Engine cluster
    /// instances, applicable to all instances in the cluster. Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gceClusterConfig")]
    pub gce_cluster_config: Option<ClusterStatusAtProviderClusterConfigGceClusterConfig>,
    /// Commands to execute on each node after config is completed.
    /// You can specify multiple versions of these. Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initializationAction")]
    pub initialization_action: Option<Vec<ClusterStatusAtProviderClusterConfigInitializationAction>>,
    /// The settings for auto deletion cluster schedule.
    /// Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "lifecycleConfig")]
    pub lifecycle_config: Option<ClusterStatusAtProviderClusterConfigLifecycleConfig>,
    /// The Google Compute Engine config settings for the master instances
    /// in a cluster. Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "masterConfig")]
    pub master_config: Option<ClusterStatusAtProviderClusterConfigMasterConfig>,
    /// The config setting for metastore service with the cluster.
    /// Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "metastoreConfig")]
    pub metastore_config: Option<ClusterStatusAtProviderClusterConfigMetastoreConfig>,
    /// The Google Compute Engine config settings for the additional
    /// instances in a cluster. Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "preemptibleWorkerConfig")]
    pub preemptible_worker_config: Option<ClusterStatusAtProviderClusterConfigPreemptibleWorkerConfig>,
    /// Security related configuration. Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "securityConfig")]
    pub security_config: Option<ClusterStatusAtProviderClusterConfigSecurityConfig>,
    /// The config settings for software inside the cluster.
    /// Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "softwareConfig")]
    pub software_config: Option<ClusterStatusAtProviderClusterConfigSoftwareConfig>,
    /// The Cloud Storage staging bucket used to stage files,
    /// such as Hadoop jars, between client machines and the cluster.
    /// Note: If you don't explicitly specify a staging_bucket
    /// then GCP will auto create / assign one for you. However, you are not guaranteed
    /// an auto generated bucket which is solely dedicated to your cluster; it may be shared
    /// with other clusters in the same region/zone also choosing to use the auto generation
    /// option.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "stagingBucket")]
    pub staging_bucket: Option<String>,
    /// The Cloud Storage temp bucket used to store ephemeral cluster
    /// and jobs data, such as Spark and MapReduce history files.
    /// Note: If you don't explicitly specify a temp_bucket then GCP will auto create / assign one for you.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tempBucket")]
    pub temp_bucket: Option<String>,
    /// The Google Compute Engine config settings for the worker instances
    /// in a cluster. Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "workerConfig")]
    pub worker_config: Option<ClusterStatusAtProviderClusterConfigWorkerConfig>,
}

/// The autoscaling policy config associated with the cluster.
/// Note that once set, if autoscaling_config is the only field set in cluster_config, it can
/// only be removed by setting policy_uri = "", rather than removing the whole block.
/// Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterStatusAtProviderClusterConfigAutoscalingConfig {
    /// The autoscaling policy used by the cluster.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "policyUri")]
    pub policy_uri: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterStatusAtProviderClusterConfigAuxiliaryNodeGroups {
    /// Node group configuration.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodeGroup")]
    pub node_group: Option<Vec<ClusterStatusAtProviderClusterConfigAuxiliaryNodeGroupsNodeGroup>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodeGroupId")]
    pub node_group_id: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterStatusAtProviderClusterConfigAuxiliaryNodeGroupsNodeGroup {
    /// The name of the cluster, unique within the project and
    /// zone.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// The node group instance group configuration.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodeGroupConfig")]
    pub node_group_config: Option<ClusterStatusAtProviderClusterConfigAuxiliaryNodeGroupsNodeGroupNodeGroupConfig>,
    /// The roles associated with the GKE node pool.
    /// One of "DEFAULT", "CONTROLLER", "SPARK_DRIVER" or "SPARK_EXECUTOR".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub roles: Option<Vec<String>>,
}

/// The node group instance group configuration.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterStatusAtProviderClusterConfigAuxiliaryNodeGroupsNodeGroupNodeGroupConfig {
    /// The Compute Engine accelerator configuration for these instances. Can be specified multiple times.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub accelerators: Option<Vec<ClusterStatusAtProviderClusterConfigAuxiliaryNodeGroupsNodeGroupNodeGroupConfigAccelerators>>,
    /// Disk Config
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "diskConfig")]
    pub disk_config: Option<ClusterStatusAtProviderClusterConfigAuxiliaryNodeGroupsNodeGroupNodeGroupConfigDiskConfig>,
    /// List of worker instance names which have been assigned
    /// to the cluster.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "instanceNames")]
    pub instance_names: Option<Vec<String>>,
    /// The name of a Compute Engine machine type.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "machineType")]
    pub machine_type: Option<String>,
    /// Minimum CPU platform to be used by this instance.
    /// The instance may be scheduled on the specified or a newer CPU platform.
    /// Specify the friendly names of CPU platforms, such as "Intel Haswell" or "Intel Sandy Bridge".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "minCpuPlatform")]
    pub min_cpu_platform: Option<String>,
    /// Specifies the number of master nodes to create.
    /// Please set a number greater than 0. Node Group must have at least 1 instance.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "numInstances")]
    pub num_instances: Option<f64>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterStatusAtProviderClusterConfigAuxiliaryNodeGroupsNodeGroupNodeGroupConfigAccelerators {
    /// The number of the accelerator cards of this type exposed to this instance. Often restricted to one of 1, 2, 4, or 8.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "acceleratorCount")]
    pub accelerator_count: Option<f64>,
    /// The short name of the accelerator type to expose to this instance. For example, nvidia-tesla-k80.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "acceleratorType")]
    pub accelerator_type: Option<String>,
}

/// Disk Config
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterStatusAtProviderClusterConfigAuxiliaryNodeGroupsNodeGroupNodeGroupConfigDiskConfig {
    /// Size of the primary disk attached to each node, specified
    /// in GB. The primary disk contains the boot volume and system libraries, and the
    /// smallest allowed disk size is 10GB. GCP will default to a predetermined
    /// computed value if not set (currently 500GB). Note: If SSDs are not
    /// attached, it also contains the HDFS data blocks and Hadoop working directories.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bootDiskSizeGb")]
    pub boot_disk_size_gb: Option<f64>,
    /// The disk type of the primary disk attached to each node.
    /// One of "pd-ssd" or "pd-standard". Defaults to "pd-standard".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bootDiskType")]
    pub boot_disk_type: Option<String>,
    /// Optional. Interface type of local SSDs (default is "scsi").
    /// Valid values: "scsi" (Small Computer System Interface), "nvme" (Non-Volatile
    /// Memory Express). See
    /// local SSD performance.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "localSsdInterface")]
    pub local_ssd_interface: Option<String>,
    /// The amount of local SSD disks that will be
    /// attached to each master cluster node. Defaults to 0.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "numLocalSsds")]
    pub num_local_ssds: Option<f64>,
}

/// The Compute Engine accelerator (GPU) configuration for these instances. Can be specified multiple times.
/// Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterStatusAtProviderClusterConfigDataprocMetricConfig {
    /// Metrics sources to enable.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub metrics: Option<Vec<ClusterStatusAtProviderClusterConfigDataprocMetricConfigMetrics>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterStatusAtProviderClusterConfigDataprocMetricConfigMetrics {
    /// One or more [available OSS metrics] (https://cloud.google.com/dataproc/docs/guides/monitoring#available_oss_metrics) to collect for the metric course.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "metricOverrides")]
    pub metric_overrides: Option<Vec<String>>,
    /// A source for the collection of Dataproc OSS metrics (see available OSS metrics).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "metricSource")]
    pub metric_source: Option<String>,
}

/// The Customer managed encryption keys settings for the cluster.
/// Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterStatusAtProviderClusterConfigEncryptionConfig {
    /// The Cloud KMS key name to use for PD disk encryption for
    /// all instances in the cluster.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kmsKeyName")]
    pub kms_key_name: Option<String>,
}

/// The config settings for port access on the cluster.
/// Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterStatusAtProviderClusterConfigEndpointConfig {
    /// The flag to enable http access to specific ports
    /// on the cluster from external sources (aka Component Gateway). Defaults to false.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableHttpPortAccess")]
    pub enable_http_port_access: Option<bool>,
    /// The map of port descriptions to URLs. Will only be populated if
    /// enable_http_port_access is true.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "httpPorts")]
    pub http_ports: Option<HashMap<String, String>>,
}

/// Common config settings for resources of Google Compute Engine cluster
/// instances, applicable to all instances in the cluster. Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterStatusAtProviderClusterConfigGceClusterConfig {
    /// By default, clusters are not restricted to internal IP addresses,
    /// and will have ephemeral external IP addresses assigned to each instance. If set to true, all
    /// instances in the cluster will only have internal IP addresses. Note: Private Google Access
    /// (also known as privateIpGoogleAccess) must be enabled on the subnetwork that the cluster
    /// will be launched in.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "internalIpOnly")]
    pub internal_ip_only: Option<bool>,
    /// A map of the Compute Engine metadata entries to add to all instances
    /// (see Project and instance metadata).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub metadata: Option<HashMap<String, String>>,
    /// The name or self_link of the Google Compute Engine
    /// network to the cluster will be part of. Conflicts with subnetwork.
    /// If neither is specified, this defaults to the "default" network.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub network: Option<String>,
    /// Node Group Affinity for sole-tenant clusters.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodeGroupAffinity")]
    pub node_group_affinity: Option<ClusterStatusAtProviderClusterConfigGceClusterConfigNodeGroupAffinity>,
    /// Reservation Affinity for consuming zonal reservation.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "reservationAffinity")]
    pub reservation_affinity: Option<ClusterStatusAtProviderClusterConfigGceClusterConfigReservationAffinity>,
    /// The service account to be used by the Node VMs.
    /// If not specified, the "default" service account is used.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceAccount")]
    pub service_account: Option<String>,
    /// The set of Google API scopes
    /// to be made available on all of the node VMs under the service_account
    /// specified. Both OAuth2 URLs and gcloud
    /// short names are supported. To allow full access to all Cloud APIs, use the
    /// cloud-platform scope. See a complete list of scopes here.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceAccountScopes")]
    pub service_account_scopes: Option<Vec<String>>,
    /// Shielded Instance Config for clusters using Compute Engine Shielded VMs.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "shieldedInstanceConfig")]
    pub shielded_instance_config: Option<ClusterStatusAtProviderClusterConfigGceClusterConfigShieldedInstanceConfig>,
    /// The name or self_link of the Google Compute Engine
    /// subnetwork the cluster will be part of. Conflicts with network.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub subnetwork: Option<String>,
    /// The list of instance tags applied to instances in the cluster.
    /// Tags are used to identify valid sources or targets for network firewalls.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tags: Option<Vec<String>>,
    /// The GCP zone where your data is stored and used (i.e. where
    /// the master and the worker nodes will be created in). If region is set to 'global' (default)
    /// then zone is mandatory, otherwise GCP is able to make use of Auto Zone Placement
    /// to determine this automatically for you.
    /// Note: This setting additionally determines and restricts
    /// which computing resources are available for use with other configs such as
    /// cluster_config.master_config.machine_type and cluster_config.worker_config.machine_type.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub zone: Option<String>,
}

/// Node Group Affinity for sole-tenant clusters.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterStatusAtProviderClusterConfigGceClusterConfigNodeGroupAffinity {
    /// The URI of a sole-tenant node group resource that the cluster will be created on.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodeGroupUri")]
    pub node_group_uri: Option<String>,
}

/// Reservation Affinity for consuming zonal reservation.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterStatusAtProviderClusterConfigGceClusterConfigReservationAffinity {
    /// Corresponds to the type of reservation consumption.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "consumeReservationType")]
    pub consume_reservation_type: Option<String>,
    /// Corresponds to the label key of reservation resource.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub key: Option<String>,
    /// Corresponds to the label values of reservation resource.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub values: Option<Vec<String>>,
}

/// Shielded Instance Config for clusters using Compute Engine Shielded VMs.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterStatusAtProviderClusterConfigGceClusterConfigShieldedInstanceConfig {
    /// Defines whether instances have integrity monitoring enabled.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableIntegrityMonitoring")]
    pub enable_integrity_monitoring: Option<bool>,
    /// Defines whether instances have Secure Boot enabled.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableSecureBoot")]
    pub enable_secure_boot: Option<bool>,
    /// Defines whether instances have the vTPM enabled.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableVtpm")]
    pub enable_vtpm: Option<bool>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterStatusAtProviderClusterConfigInitializationAction {
    /// The script to be executed during initialization of the cluster.
    /// The script must be a GCS file with a gs:// prefix.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub script: Option<String>,
    /// The maximum duration (in seconds) which script is
    /// allowed to take to execute its action. GCP will default to a predetermined
    /// computed value if not set (currently 300).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "timeoutSec")]
    pub timeout_sec: Option<f64>,
}

/// The settings for auto deletion cluster schedule.
/// Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterStatusAtProviderClusterConfigLifecycleConfig {
    /// The time when cluster will be auto-deleted.
    /// A timestamp in RFC3339 UTC "Zulu" format, accurate to nanoseconds.
    /// Example: "2014-10-02T15:01:23.045123456Z".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "autoDeleteTime")]
    pub auto_delete_time: Option<String>,
    /// The duration to keep the cluster alive while idling
    /// (no jobs running). After this TTL, the cluster will be deleted. Valid range: [10m, 14d].
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "idleDeleteTtl")]
    pub idle_delete_ttl: Option<String>,
    /// Time when the cluster became idle
    /// (most recent job finished) and became eligible for deletion due to idleness.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "idleStartTime")]
    pub idle_start_time: Option<String>,
}

/// The Google Compute Engine config settings for the master instances
/// in a cluster. Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterStatusAtProviderClusterConfigMasterConfig {
    /// The Compute Engine accelerator (GPU) configuration for these instances. Can be specified multiple times.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub accelerators: Option<Vec<ClusterStatusAtProviderClusterConfigMasterConfigAccelerators>>,
    /// Disk Config
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "diskConfig")]
    pub disk_config: Option<ClusterStatusAtProviderClusterConfigMasterConfigDiskConfig>,
    /// The URI for the image to use for this worker.  See the guide
    /// for more information.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "imageUri")]
    pub image_uri: Option<String>,
    /// List of worker instance names which have been assigned
    /// to the cluster.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "instanceNames")]
    pub instance_names: Option<Vec<String>>,
    /// The name of a Google Compute Engine machine type
    /// to create for the master. If not specified, GCP will default to a predetermined
    /// computed value (currently n1-standard-4).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "machineType")]
    pub machine_type: Option<String>,
    /// The name of a minimum generation of CPU family
    /// for the master. If not specified, GCP will default to a predetermined computed value
    /// for each zone. See the guide
    /// for details about which CPU families are available (and defaulted) for each zone.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "minCpuPlatform")]
    pub min_cpu_platform: Option<String>,
    /// Specifies the number of master nodes to create.
    /// If not specified, GCP will default to a predetermined computed value (currently 1).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "numInstances")]
    pub num_instances: Option<f64>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterStatusAtProviderClusterConfigMasterConfigAccelerators {
    /// The number of the accelerator cards of this type exposed to this instance. Often restricted to one of 1, 2, 4, or 8.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "acceleratorCount")]
    pub accelerator_count: Option<f64>,
    /// The short name of the accelerator type to expose to this instance. For example, nvidia-tesla-k80.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "acceleratorType")]
    pub accelerator_type: Option<String>,
}

/// Disk Config
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterStatusAtProviderClusterConfigMasterConfigDiskConfig {
    /// Size of the primary disk attached to each node, specified
    /// in GB. The primary disk contains the boot volume and system libraries, and the
    /// smallest allowed disk size is 10GB. GCP will default to a predetermined
    /// computed value if not set (currently 500GB). Note: If SSDs are not
    /// attached, it also contains the HDFS data blocks and Hadoop working directories.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bootDiskSizeGb")]
    pub boot_disk_size_gb: Option<f64>,
    /// The disk type of the primary disk attached to each node.
    /// One of "pd-ssd" or "pd-standard". Defaults to "pd-standard".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bootDiskType")]
    pub boot_disk_type: Option<String>,
    /// Optional. Interface type of local SSDs (default is "scsi").
    /// Valid values: "scsi" (Small Computer System Interface), "nvme" (Non-Volatile
    /// Memory Express). See
    /// local SSD performance.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "localSsdInterface")]
    pub local_ssd_interface: Option<String>,
    /// The amount of local SSD disks that will be
    /// attached to each master cluster node. Defaults to 0.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "numLocalSsds")]
    pub num_local_ssds: Option<f64>,
}

/// The config setting for metastore service with the cluster.
/// Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterStatusAtProviderClusterConfigMetastoreConfig {
    /// Resource name of an existing Dataproc Metastore service.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dataprocMetastoreService")]
    pub dataproc_metastore_service: Option<String>,
}

/// The Google Compute Engine config settings for the additional
/// instances in a cluster. Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterStatusAtProviderClusterConfigPreemptibleWorkerConfig {
    /// Disk Config
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "diskConfig")]
    pub disk_config: Option<ClusterStatusAtProviderClusterConfigPreemptibleWorkerConfigDiskConfig>,
    /// Instance flexibility Policy allowing a mixture of VM shapes and provisioning models.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "instanceFlexibilityPolicy")]
    pub instance_flexibility_policy: Option<ClusterStatusAtProviderClusterConfigPreemptibleWorkerConfigInstanceFlexibilityPolicy>,
    /// List of worker instance names which have been assigned
    /// to the cluster.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "instanceNames")]
    pub instance_names: Option<Vec<String>>,
    /// Specifies the number of preemptible nodes to create.
    /// Defaults to 0.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "numInstances")]
    pub num_instances: Option<f64>,
    /// Specifies the preemptibility of the secondary workers. The default value is PREEMPTIBLE
    /// Accepted values are:
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub preemptibility: Option<String>,
}

/// Disk Config
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterStatusAtProviderClusterConfigPreemptibleWorkerConfigDiskConfig {
    /// Size of the primary disk attached to each node, specified
    /// in GB. The primary disk contains the boot volume and system libraries, and the
    /// smallest allowed disk size is 10GB. GCP will default to a predetermined
    /// computed value if not set (currently 500GB). Note: If SSDs are not
    /// attached, it also contains the HDFS data blocks and Hadoop working directories.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bootDiskSizeGb")]
    pub boot_disk_size_gb: Option<f64>,
    /// The disk type of the primary disk attached to each node.
    /// One of "pd-ssd" or "pd-standard". Defaults to "pd-standard".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bootDiskType")]
    pub boot_disk_type: Option<String>,
    /// Optional. Interface type of local SSDs (default is "scsi").
    /// Valid values: "scsi" (Small Computer System Interface), "nvme" (Non-Volatile
    /// Memory Express). See
    /// local SSD performance.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "localSsdInterface")]
    pub local_ssd_interface: Option<String>,
    /// The amount of local SSD disks that will be
    /// attached to each master cluster node. Defaults to 0.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "numLocalSsds")]
    pub num_local_ssds: Option<f64>,
}

/// Instance flexibility Policy allowing a mixture of VM shapes and provisioning models.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterStatusAtProviderClusterConfigPreemptibleWorkerConfigInstanceFlexibilityPolicy {
    /// List of instance selection options that the group will use when creating new VMs.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "instanceSelectionList")]
    pub instance_selection_list: Option<Vec<ClusterStatusAtProviderClusterConfigPreemptibleWorkerConfigInstanceFlexibilityPolicyInstanceSelectionList>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "instanceSelectionResults")]
    pub instance_selection_results: Option<Vec<ClusterStatusAtProviderClusterConfigPreemptibleWorkerConfigInstanceFlexibilityPolicyInstanceSelectionResults>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterStatusAtProviderClusterConfigPreemptibleWorkerConfigInstanceFlexibilityPolicyInstanceSelectionList {
    /// Full machine-type names, e.g. "n1-standard-16".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "machineTypes")]
    pub machine_types: Option<Vec<String>>,
    /// Preference of this instance selection. A lower number means higher preference. Dataproc will first try to create a VM based on the machine-type with priority rank and fallback to next rank based on availability. Machine types and instance selections with the same priority have the same preference.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub rank: Option<f64>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterStatusAtProviderClusterConfigPreemptibleWorkerConfigInstanceFlexibilityPolicyInstanceSelectionResults {
    /// The name of a Compute Engine machine type.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "machineType")]
    pub machine_type: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "vmCount")]
    pub vm_count: Option<f64>,
}

/// Security related configuration. Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterStatusAtProviderClusterConfigSecurityConfig {
    /// Kerberos Configuration
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kerberosConfig")]
    pub kerberos_config: Option<ClusterStatusAtProviderClusterConfigSecurityConfigKerberosConfig>,
}

/// Kerberos Configuration
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterStatusAtProviderClusterConfigSecurityConfigKerberosConfig {
    /// The admin server (IP or hostname) for the
    /// remote trusted realm in a cross realm trust relationship.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "crossRealmTrustAdminServer")]
    pub cross_realm_trust_admin_server: Option<String>,
    /// The KDC (IP or hostname) for the
    /// remote trusted realm in a cross realm trust relationship.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "crossRealmTrustKdc")]
    pub cross_realm_trust_kdc: Option<String>,
    /// The remote realm the Dataproc on-cluster KDC will
    /// trust, should the user enable cross realm trust.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "crossRealmTrustRealm")]
    pub cross_realm_trust_realm: Option<String>,
    /// The Cloud Storage URI of a KMS
    /// encrypted file containing the shared password between the on-cluster Kerberos realm
    /// and the remote trusted realm, in a cross realm trust relationship.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "crossRealmTrustSharedPasswordUri")]
    pub cross_realm_trust_shared_password_uri: Option<String>,
    /// Flag to indicate whether to Kerberize the cluster.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableKerberos")]
    pub enable_kerberos: Option<bool>,
    /// The Cloud Storage URI of a KMS encrypted file containing
    /// the master key of the KDC database.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kdcDbKeyUri")]
    pub kdc_db_key_uri: Option<String>,
    /// The Cloud Storage URI of a KMS encrypted file containing
    /// the password to the user provided key. For the self-signed certificate, this password
    /// is generated by Dataproc.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "keyPasswordUri")]
    pub key_password_uri: Option<String>,
    /// The Cloud Storage URI of a KMS encrypted file containing
    /// the password to the user provided keystore. For the self-signed certificated, the password
    /// is generated by Dataproc.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "keystorePasswordUri")]
    pub keystore_password_uri: Option<String>,
    /// The Cloud Storage URI of the keystore file used for SSL encryption.
    /// If not provided, Dataproc will provide a self-signed certificate.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "keystoreUri")]
    pub keystore_uri: Option<String>,
    /// The URI of the KMS key used to encrypt various sensitive files.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kmsKeyUri")]
    pub kms_key_uri: Option<String>,
    /// The name of the on-cluster Kerberos realm. If not specified, the
    /// uppercased domain of hostnames will be the realm.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub realm: Option<String>,
    /// The Cloud Storage URI of a KMS encrypted file
    /// containing the root principal password.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "rootPrincipalPasswordUri")]
    pub root_principal_password_uri: Option<String>,
    /// The lifetime of the ticket granting ticket, in hours.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tgtLifetimeHours")]
    pub tgt_lifetime_hours: Option<f64>,
    /// The Cloud Storage URI of a KMS encrypted file
    /// containing the password to the user provided truststore. For the self-signed
    /// certificate, this password is generated by Dataproc.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "truststorePasswordUri")]
    pub truststore_password_uri: Option<String>,
    /// The Cloud Storage URI of the truststore file used for
    /// SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "truststoreUri")]
    pub truststore_uri: Option<String>,
}

/// The config settings for software inside the cluster.
/// Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterStatusAtProviderClusterConfigSoftwareConfig {
    /// The Cloud Dataproc image version to use
    /// for the cluster - this controls the sets of software versions
    /// installed onto the nodes when you create clusters. If not specified, defaults to the
    /// latest version. For a list of valid versions see
    /// Cloud Dataproc versions
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "imageVersion")]
    pub image_version: Option<String>,
    /// The set of optional components to activate on the cluster. See Available Optional Components.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "optionalComponents")]
    pub optional_components: Option<Vec<String>>,
    /// A list of override and additional properties (key/value pairs)
    /// used to modify various aspects of the common configuration files used when creating
    /// a cluster. For a list of valid properties please see
    /// Cluster properties
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "overrideProperties")]
    pub override_properties: Option<HashMap<String, String>>,
    /// The properties to set on daemon config files. Property keys are specified in prefix:property format,
    /// for example spark:spark.kubernetes.container.image.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub properties: Option<HashMap<String, String>>,
}

/// The Google Compute Engine config settings for the worker instances
/// in a cluster. Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterStatusAtProviderClusterConfigWorkerConfig {
    /// The Compute Engine accelerator configuration for these instances. Can be specified multiple times.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub accelerators: Option<Vec<ClusterStatusAtProviderClusterConfigWorkerConfigAccelerators>>,
    /// Disk Config
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "diskConfig")]
    pub disk_config: Option<ClusterStatusAtProviderClusterConfigWorkerConfigDiskConfig>,
    /// The URI for the image to use for this worker.  See the guide
    /// for more information.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "imageUri")]
    pub image_uri: Option<String>,
    /// List of worker instance names which have been assigned
    /// to the cluster.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "instanceNames")]
    pub instance_names: Option<Vec<String>>,
    /// The name of a Google Compute Engine machine type
    /// to create for the worker nodes. If not specified, GCP will default to a predetermined
    /// computed value (currently n1-standard-4).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "machineType")]
    pub machine_type: Option<String>,
    /// The name of a minimum generation of CPU family
    /// for the master. If not specified, GCP will default to a predetermined computed value
    /// for each zone. See the guide
    /// for details about which CPU families are available (and defaulted) for each zone.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "minCpuPlatform")]
    pub min_cpu_platform: Option<String>,
    /// The minimum number of primary worker instances to create.  If min_num_instances is set, cluster creation will succeed if the number of primary workers created is at least equal to the min_num_instances number.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "minNumInstances")]
    pub min_num_instances: Option<f64>,
    /// Specifies the number of worker nodes to create.
    /// If not specified, GCP will default to a predetermined computed value (currently 2).
    /// There is currently a beta feature which allows you to run a
    /// Single Node Cluster.
    /// In order to take advantage of this you need to set
    /// "dataproc:dataproc.allow.zero.workers" = "true" in
    /// cluster_config.software_config.properties
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "numInstances")]
    pub num_instances: Option<f64>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterStatusAtProviderClusterConfigWorkerConfigAccelerators {
    /// The number of the accelerator cards of this type exposed to this instance. Often restricted to one of 1, 2, 4, or 8.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "acceleratorCount")]
    pub accelerator_count: Option<f64>,
    /// The short name of the accelerator type to expose to this instance. For example, nvidia-tesla-k80.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "acceleratorType")]
    pub accelerator_type: Option<String>,
}

/// Disk Config
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterStatusAtProviderClusterConfigWorkerConfigDiskConfig {
    /// Size of the primary disk attached to each node, specified
    /// in GB. The primary disk contains the boot volume and system libraries, and the
    /// smallest allowed disk size is 10GB. GCP will default to a predetermined
    /// computed value if not set (currently 500GB). Note: If SSDs are not
    /// attached, it also contains the HDFS data blocks and Hadoop working directories.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bootDiskSizeGb")]
    pub boot_disk_size_gb: Option<f64>,
    /// The disk type of the primary disk attached to each node.
    /// One of "pd-ssd" or "pd-standard". Defaults to "pd-standard".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bootDiskType")]
    pub boot_disk_type: Option<String>,
    /// Optional. Interface type of local SSDs (default is "scsi").
    /// Valid values: "scsi" (Small Computer System Interface), "nvme" (Non-Volatile
    /// Memory Express). See
    /// local SSD performance.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "localSsdInterface")]
    pub local_ssd_interface: Option<String>,
    /// The amount of local SSD disks that will be
    /// attached to each master cluster node. Defaults to 0.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "numLocalSsds")]
    pub num_local_ssds: Option<f64>,
}

/// Allows you to configure a virtual Dataproc on GKE cluster.
/// Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterStatusAtProviderVirtualClusterConfig {
    /// Configuration of auxiliary services used by this cluster.
    /// Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "auxiliaryServicesConfig")]
    pub auxiliary_services_config: Option<ClusterStatusAtProviderVirtualClusterConfigAuxiliaryServicesConfig>,
    /// The configuration for running the Dataproc cluster on Kubernetes.
    /// Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kubernetesClusterConfig")]
    pub kubernetes_cluster_config: Option<ClusterStatusAtProviderVirtualClusterConfigKubernetesClusterConfig>,
    /// The Cloud Storage staging bucket used to stage files,
    /// such as Hadoop jars, between client machines and the cluster.
    /// Note: If you don't explicitly specify a staging_bucket
    /// then GCP will auto create / assign one for you. However, you are not guaranteed
    /// an auto generated bucket which is solely dedicated to your cluster; it may be shared
    /// with other clusters in the same region/zone also choosing to use the auto generation
    /// option.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "stagingBucket")]
    pub staging_bucket: Option<String>,
}

/// Configuration of auxiliary services used by this cluster.
/// Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterStatusAtProviderVirtualClusterConfigAuxiliaryServicesConfig {
    /// The config setting for metastore service with the cluster.
    /// Structure defined below.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "metastoreConfig")]
    pub metastore_config: Option<ClusterStatusAtProviderVirtualClusterConfigAuxiliaryServicesConfigMetastoreConfig>,
    /// The Spark History Server configuration for the workload.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sparkHistoryServerConfig")]
    pub spark_history_server_config: Option<ClusterStatusAtProviderVirtualClusterConfigAuxiliaryServicesConfigSparkHistoryServerConfig>,
}

/// The config setting for metastore service with the cluster.
/// Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterStatusAtProviderVirtualClusterConfigAuxiliaryServicesConfigMetastoreConfig {
    /// Resource name of an existing Dataproc Metastore service.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dataprocMetastoreService")]
    pub dataproc_metastore_service: Option<String>,
}

/// The Spark History Server configuration for the workload.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterStatusAtProviderVirtualClusterConfigAuxiliaryServicesConfigSparkHistoryServerConfig {
    /// Resource name of an existing Dataproc Cluster to act as a Spark History Server for the workload.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dataprocCluster")]
    pub dataproc_cluster: Option<String>,
}

/// The configuration for running the Dataproc cluster on Kubernetes.
/// Structure defined below.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterStatusAtProviderVirtualClusterConfigKubernetesClusterConfig {
    /// The configuration for running the Dataproc cluster on GKE.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gkeClusterConfig")]
    pub gke_cluster_config: Option<ClusterStatusAtProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfig>,
    /// A namespace within the Kubernetes cluster to deploy into.
    /// If this namespace does not exist, it is created.
    /// If it  exists, Dataproc verifies that another Dataproc VirtualCluster is not installed into it.
    /// If not specified, the name of the Dataproc Cluster is used.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kubernetesNamespace")]
    pub kubernetes_namespace: Option<String>,
    /// The software configuration for this Dataproc cluster running on Kubernetes.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kubernetesSoftwareConfig")]
    pub kubernetes_software_config: Option<ClusterStatusAtProviderVirtualClusterConfigKubernetesClusterConfigKubernetesSoftwareConfig>,
}

/// The configuration for running the Dataproc cluster on GKE.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterStatusAtProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfig {
    /// A target GKE cluster to deploy to. It must be in the same project and region as the Dataproc cluster
    /// (the GKE cluster can be zonal or regional)
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "gkeClusterTarget")]
    pub gke_cluster_target: Option<String>,
    /// GKE node pools where workloads will be scheduled. At least one node pool must be assigned the DEFAULT
    /// GkeNodePoolTarget.Role. If a GkeNodePoolTarget is not specified, Dataproc constructs a DEFAULT GkeNodePoolTarget.
    /// Each role can be given to only one GkeNodePoolTarget. All node pools must have the same location settings.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodePoolTarget")]
    pub node_pool_target: Option<Vec<ClusterStatusAtProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTarget>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterStatusAtProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTarget {
    /// The target GKE node pool.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodePool")]
    pub node_pool: Option<String>,
    /// (Input only) The configuration for the GKE node pool.
    /// If specified, Dataproc attempts to create a node pool with the specified shape.
    /// If one with the same name already exists, it is verified against all specified fields.
    /// If a field differs, the virtual cluster creation will fail.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "nodePoolConfig")]
    pub node_pool_config: Option<ClusterStatusAtProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfig>,
    /// The roles associated with the GKE node pool.
    /// One of "DEFAULT", "CONTROLLER", "SPARK_DRIVER" or "SPARK_EXECUTOR".
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub roles: Option<Vec<String>>,
}

/// (Input only) The configuration for the GKE node pool.
/// If specified, Dataproc attempts to create a node pool with the specified shape.
/// If one with the same name already exists, it is verified against all specified fields.
/// If a field differs, the virtual cluster creation will fail.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterStatusAtProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfig {
    /// The autoscaler configuration for this node pool.
    /// The autoscaler is enabled only when a valid configuration is present.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub autoscaling: Option<ClusterStatusAtProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigAutoscaling>,
    /// The node pool configuration.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub config: Option<ClusterStatusAtProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfig>,
    /// The list of Compute Engine zones where node pool nodes associated
    /// with a Dataproc on GKE virtual cluster will be located.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub locations: Option<Vec<String>>,
}

/// The autoscaler configuration for this node pool.
/// The autoscaler is enabled only when a valid configuration is present.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterStatusAtProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigAutoscaling {
    /// The maximum number of nodes in the node pool. Must be >= minNodeCount, and must be > 0.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxNodeCount")]
    pub max_node_count: Option<f64>,
    /// The minimum number of nodes in the node pool. Must be >= 0 and <= maxNodeCount.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "minNodeCount")]
    pub min_node_count: Option<f64>,
}

/// The node pool configuration.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterStatusAtProviderVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfig {
    /// The number of local SSD disks to attach to the node,
    /// which is limited by the maximum number of disks allowable per zone.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "localSsdCount")]
    pub local_ssd_count: Option<f64>,
    /// The name of a Compute Engine machine type.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "machineType")]
    pub machine_type: Option<String>,
    /// Minimum CPU platform to be used by this instance.
    /// The instance may be scheduled on the specified or a newer CPU platform.
    /// Specify the friendly names of CPU platforms, such as "Intel Haswell" or "Intel Sandy Bridge".
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "minCpuPlatform")]
    pub min_cpu_platform: Option<String>,
    /// Whether the nodes are created as preemptible VM instances.
    /// Preemptible nodes cannot be used in a node pool with the CONTROLLER role or in the DEFAULT node pool if the
    /// CONTROLLER role is not assigned (the DEFAULT node pool will assume the CONTROLLER role).
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub preemptible: Option<bool>,
    /// Spot flag for enabling Spot VM, which is a rebrand of the existing preemptible flag.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub spot: Option<bool>,
}

/// The software configuration for this Dataproc cluster running on Kubernetes.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub struct ClusterStatusAtProviderVirtualClusterConfigKubernetesClusterConfigKubernetesSoftwareConfig {
    /// The components that should be installed in this Dataproc cluster. The key must be a string from the
    /// KubernetesComponent enumeration. The value is the version of the software to be installed. At least one entry must be specified.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "componentVersion")]
    pub component_version: Option<HashMap<String, String>>,
    /// The properties to set on daemon config files. Property keys are specified in prefix:property format,
    /// for example spark:spark.kubernetes.container.image.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub properties: Option<HashMap<String, String>>,
}

