// WARNING: generated by kopium - manual changes will be overwritten
// kopium version: 0.21.2

#[allow(unused_imports)]
mod prelude {
    pub use kube::CustomResource;
    pub use typed_builder::TypedBuilder;
    pub use schemars::JsonSchema;
    pub use serde::{Serialize, Deserialize};
    pub use std::collections::HashMap;
    pub use k8s_openapi::apimachinery::pkg::apis::meta::v1::Condition;
}
use self::prelude::*;

/// FlowSpec defines the desired state of Flow
#[derive(CustomResource, Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
#[kube(group = "appflow.aws.upbound.io", version = "v1beta2", kind = "Flow", plural = "flows")]
#[kube(status = "FlowStatus")]
pub struct FlowSpec {
    /// DeletionPolicy specifies what will happen to the underlying external
    /// when this managed resource is deleted - either "Delete" or "Orphan" the
    /// external resource.
    /// This field is planned to be deprecated in favor of the ManagementPolicies
    /// field in a future release. Currently, both could be set independently and
    /// non-default values would be honored if the feature flag is enabled.
    /// See the design doc for more information: https://github.com/crossplane/crossplane/blob/499895a25d1a1a0ba1604944ef98ac7a1a71f197/design/design-doc-observe-only-resources.md?plain=1#L223
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "deletionPolicy")]
    #[builder(default)]
    pub deletion_policy: Option<FlowDeletionPolicy>,
    #[serde(rename = "forProvider")]
    pub for_provider: FlowForProvider,
    /// THIS IS A BETA FIELD. It will be honored
    /// unless the Management Policies feature flag is disabled.
    /// InitProvider holds the same fields as ForProvider, with the exception
    /// of Identifier and other resource reference fields. The fields that are
    /// in InitProvider are merged into ForProvider when the resource is created.
    /// The same fields are also added to the terraform ignore_changes hook, to
    /// avoid updating them after creation. This is useful for fields that are
    /// required on creation, but we do not desire to update them after creation,
    /// for example because of an external controller is managing them, like an
    /// autoscaler.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initProvider")]
    #[builder(default)]
    pub init_provider: Option<FlowInitProvider>,
    /// THIS IS A BETA FIELD. It is on by default but can be opted out
    /// through a Crossplane feature flag.
    /// ManagementPolicies specify the array of actions Crossplane is allowed to
    /// take on the managed and external resources.
    /// This field is planned to replace the DeletionPolicy field in a future
    /// release. Currently, both could be set independently and non-default
    /// values would be honored if the feature flag is enabled. If both are
    /// custom, the DeletionPolicy field will be ignored.
    /// See the design doc for more information: https://github.com/crossplane/crossplane/blob/499895a25d1a1a0ba1604944ef98ac7a1a71f197/design/design-doc-observe-only-resources.md?plain=1#L223
    /// and this one: https://github.com/crossplane/crossplane/blob/444267e84783136daa93568b364a5f01228cacbe/design/one-pager-ignore-changes.md
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "managementPolicies")]
    #[builder(default)]
    pub management_policies: Option<Vec<String>>,
    /// ProviderConfigReference specifies how the provider that will be used to
    /// create, observe, update, and delete this managed resource should be
    /// configured.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "providerConfigRef")]
    #[builder(default)]
    pub provider_config_ref: Option<FlowProviderConfigRef>,
    /// PublishConnectionDetailsTo specifies the connection secret config which
    /// contains a name, metadata and a reference to secret store config to
    /// which any connection details for this managed resource should be written.
    /// Connection details frequently include the endpoint, username,
    /// and password required to connect to the managed resource.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "publishConnectionDetailsTo")]
    #[builder(default)]
    pub publish_connection_details_to: Option<FlowPublishConnectionDetailsTo>,
    /// WriteConnectionSecretToReference specifies the namespace and name of a
    /// Secret to which any connection details for this managed resource should
    /// be written. Connection details frequently include the endpoint, username,
    /// and password required to connect to the managed resource.
    /// This field is planned to be replaced in a future release in favor of
    /// PublishConnectionDetailsTo. Currently, both could be set independently
    /// and connection details would be published to both without affecting
    /// each other.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "writeConnectionSecretToRef")]
    #[builder(default)]
    pub write_connection_secret_to_ref: Option<FlowWriteConnectionSecretToRef>,
}

/// FlowSpec defines the desired state of Flow
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum FlowDeletionPolicy {
    Orphan,
    Delete,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProvider {
    /// Description of the flow you want to create.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub description: Option<String>,
    /// A Destination Flow Config that controls how Amazon AppFlow places data in the destination connector.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "destinationFlowConfig")]
    #[builder(default)]
    pub destination_flow_config: Option<Vec<FlowForProviderDestinationFlowConfig>>,
    /// ARN (Amazon Resource Name) of the Key Management Service (KMS) key you provide for encryption. This is required if you do not want to use the Amazon AppFlow-managed KMS key. If you don't provide anything here, Amazon AppFlow uses the Amazon AppFlow-managed KMS key.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kmsArn")]
    #[builder(default)]
    pub kms_arn: Option<String>,
    /// A Catalog that determines the configuration that Amazon AppFlow uses when it catalogs the data thatâ€™s transferred by the associated flow. When Amazon AppFlow catalogs the data from a flow, it stores metadata in a data catalog.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "metadataCatalogConfig")]
    #[builder(default)]
    pub metadata_catalog_config: Option<FlowForProviderMetadataCatalogConfig>,
    /// Region is the region you'd like your resource to be created in.
    pub region: String,
    /// The Source Flow Config that controls how Amazon AppFlow retrieves data from the source connector.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sourceFlowConfig")]
    #[builder(default)]
    pub source_flow_config: Option<FlowForProviderSourceFlowConfig>,
    /// Key-value map of resource tags.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub tags: Option<HashMap<String, String>>,
    /// A Task that Amazon AppFlow performs while transferring the data in the flow run.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub task: Option<Vec<FlowForProviderTask>>,
    /// A Trigger that determine how and when the flow runs.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "triggerConfig")]
    #[builder(default)]
    pub trigger_config: Option<FlowForProviderTriggerConfig>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderDestinationFlowConfig {
    /// API version that the destination connector uses.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "apiVersion")]
    #[builder(default)]
    pub api_version: Option<String>,
    /// Name of the connector profile. This name must be unique for each connector profile in the AWS account.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "connectorProfileName")]
    #[builder(default)]
    pub connector_profile_name: Option<String>,
    /// Type of connector, such as Salesforce, Amplitude, and so on. Valid values are Salesforce, Singular, Slack, Redshift, S3, Marketo, Googleanalytics, Zendesk, Servicenow, Datadog, Trendmicro, Snowflake, Dynatrace, Infornexus, Amplitude, Veeva, EventBridge, LookoutMetrics, Upsolver, Honeycode, CustomerProfiles, SAPOData, and CustomConnector.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "connectorType")]
    #[builder(default)]
    pub connector_type: Option<String>,
    /// This stores the information that is required to query a particular connector. See Destination Connector Properties for more information.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "destinationConnectorProperties")]
    #[builder(default)]
    pub destination_connector_properties: Option<FlowForProviderDestinationFlowConfigDestinationConnectorProperties>,
}

/// This stores the information that is required to query a particular connector. See Destination Connector Properties for more information.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderDestinationFlowConfigDestinationConnectorProperties {
    /// Properties that are required to query the custom Connector. See Custom Connector Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "customConnector")]
    #[builder(default)]
    pub custom_connector: Option<FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesCustomConnector>,
    /// Properties that are required to query Amazon Connect Customer Profiles. See Customer Profiles Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "customerProfiles")]
    #[builder(default)]
    pub customer_profiles: Option<FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesCustomerProfiles>,
    /// Properties that are required to query Amazon EventBridge. See Generic Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "eventBridge")]
    #[builder(default)]
    pub event_bridge: Option<FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesEventBridge>,
    /// Properties that are required to query Amazon Honeycode. See Generic Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub honeycode: Option<FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesHoneycode>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "lookoutMetrics")]
    #[builder(default)]
    pub lookout_metrics: Option<FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesLookoutMetrics>,
    /// Properties that are required to query Marketo. See Generic Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub marketo: Option<FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesMarketo>,
    /// Properties that are required to query Amazon Redshift. See Redshift Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub redshift: Option<FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesRedshift>,
    /// Properties that are required to query Amazon S3. See S3 Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub s3: Option<FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesS3>,
    /// Properties that are required to query Salesforce. See Salesforce Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub salesforce: Option<FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesSalesforce>,
    /// Properties that are required to query SAPOData. See SAPOData Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sapoData")]
    #[builder(default)]
    pub sapo_data: Option<FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesSapoData>,
    /// Properties that are required to query Snowflake. See Snowflake Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub snowflake: Option<FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesSnowflake>,
    /// Properties that are required to query Upsolver. See Upsolver Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub upsolver: Option<FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesUpsolver>,
    /// Properties that are required to query Zendesk. See Zendesk Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub zendesk: Option<FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesZendesk>,
}

/// Properties that are required to query the custom Connector. See Custom Connector Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesCustomConnector {
    /// Custom properties that are specific to the connector when it's used as a destination in the flow. Maximum of 50 items.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "customProperties")]
    #[builder(default)]
    pub custom_properties: Option<HashMap<String, String>>,
    /// Entity specified in the custom connector as a destination in the flow.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "entityName")]
    #[builder(default)]
    pub entity_name: Option<String>,
    /// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "errorHandlingConfig")]
    #[builder(default)]
    pub error_handling_config: Option<FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesCustomConnectorErrorHandlingConfig>,
    /// Name of the field that Amazon AppFlow uses as an ID when performing a write operation such as update, delete, or upsert.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "idFieldNames")]
    #[builder(default)]
    pub id_field_names: Option<Vec<String>>,
    /// Type of write operation to be performed in the custom connector when it's used as destination. Valid values are INSERT, UPSERT, UPDATE, and DELETE.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "writeOperationType")]
    #[builder(default)]
    pub write_operation_type: Option<String>,
}

/// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesCustomConnectorErrorHandlingConfig {
    /// Amazon S3 bucket name in which Amazon AppFlow places the transferred data.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketName")]
    #[builder(default)]
    pub bucket_name: Option<String>,
    /// Object key for the bucket in which Amazon AppFlow places the destination files.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketPrefix")]
    #[builder(default)]
    pub bucket_prefix: Option<String>,
    /// If the flow should fail after the first instance of a failure when attempting to place data in the destination.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failOnFirstDestinationError")]
    #[builder(default)]
    pub fail_on_first_destination_error: Option<bool>,
}

/// Properties that are required to query Amazon Connect Customer Profiles. See Customer Profiles Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesCustomerProfiles {
    /// Unique name of the Amazon Connect Customer Profiles domain.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "domainName")]
    #[builder(default)]
    pub domain_name: Option<String>,
    /// Object specified in the Amazon Connect Customer Profiles flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "objectTypeName")]
    #[builder(default)]
    pub object_type_name: Option<String>,
}

/// Properties that are required to query Amazon EventBridge. See Generic Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesEventBridge {
    /// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "errorHandlingConfig")]
    #[builder(default)]
    pub error_handling_config: Option<FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesEventBridgeErrorHandlingConfig>,
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

/// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesEventBridgeErrorHandlingConfig {
    /// Amazon S3 bucket name in which Amazon AppFlow places the transferred data.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketName")]
    #[builder(default)]
    pub bucket_name: Option<String>,
    /// Object key for the bucket in which Amazon AppFlow places the destination files.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketPrefix")]
    #[builder(default)]
    pub bucket_prefix: Option<String>,
    /// If the flow should fail after the first instance of a failure when attempting to place data in the destination.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failOnFirstDestinationError")]
    #[builder(default)]
    pub fail_on_first_destination_error: Option<bool>,
}

/// Properties that are required to query Amazon Honeycode. See Generic Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesHoneycode {
    /// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "errorHandlingConfig")]
    #[builder(default)]
    pub error_handling_config: Option<FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesHoneycodeErrorHandlingConfig>,
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

/// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesHoneycodeErrorHandlingConfig {
    /// Amazon S3 bucket name in which Amazon AppFlow places the transferred data.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketName")]
    #[builder(default)]
    pub bucket_name: Option<String>,
    /// Object key for the bucket in which Amazon AppFlow places the destination files.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketPrefix")]
    #[builder(default)]
    pub bucket_prefix: Option<String>,
    /// If the flow should fail after the first instance of a failure when attempting to place data in the destination.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failOnFirstDestinationError")]
    #[builder(default)]
    pub fail_on_first_destination_error: Option<bool>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesLookoutMetrics {
}

/// Properties that are required to query Marketo. See Generic Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesMarketo {
    /// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "errorHandlingConfig")]
    #[builder(default)]
    pub error_handling_config: Option<FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesMarketoErrorHandlingConfig>,
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

/// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesMarketoErrorHandlingConfig {
    /// Amazon S3 bucket name in which Amazon AppFlow places the transferred data.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketName")]
    #[builder(default)]
    pub bucket_name: Option<String>,
    /// Object key for the bucket in which Amazon AppFlow places the destination files.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketPrefix")]
    #[builder(default)]
    pub bucket_prefix: Option<String>,
    /// If the flow should fail after the first instance of a failure when attempting to place data in the destination.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failOnFirstDestinationError")]
    #[builder(default)]
    pub fail_on_first_destination_error: Option<bool>,
}

/// Properties that are required to query Amazon Redshift. See Redshift Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesRedshift {
    /// Object key for the bucket in which Amazon AppFlow places the destination files.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketPrefix")]
    #[builder(default)]
    pub bucket_prefix: Option<String>,
    /// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "errorHandlingConfig")]
    #[builder(default)]
    pub error_handling_config: Option<FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesRedshiftErrorHandlingConfig>,
    /// Intermediate bucket that Amazon AppFlow uses when moving data into Amazon Redshift.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "intermediateBucketName")]
    #[builder(default)]
    pub intermediate_bucket_name: Option<String>,
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

/// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesRedshiftErrorHandlingConfig {
    /// Amazon S3 bucket name in which Amazon AppFlow places the transferred data.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketName")]
    #[builder(default)]
    pub bucket_name: Option<String>,
    /// Object key for the bucket in which Amazon AppFlow places the destination files.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketPrefix")]
    #[builder(default)]
    pub bucket_prefix: Option<String>,
    /// If the flow should fail after the first instance of a failure when attempting to place data in the destination.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failOnFirstDestinationError")]
    #[builder(default)]
    pub fail_on_first_destination_error: Option<bool>,
}

/// Properties that are required to query Amazon S3. See S3 Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesS3 {
    /// Amazon S3 bucket name in which Amazon AppFlow places the transferred data.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketName")]
    #[builder(default)]
    pub bucket_name: Option<String>,
    /// Reference to a BucketPolicy in s3 to populate bucketName.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketNameRef")]
    #[builder(default)]
    pub bucket_name_ref: Option<FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesS3BucketNameRef>,
    /// Selector for a BucketPolicy in s3 to populate bucketName.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketNameSelector")]
    #[builder(default)]
    pub bucket_name_selector: Option<FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesS3BucketNameSelector>,
    /// Object key for the bucket in which Amazon AppFlow places the destination files.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketPrefix")]
    #[builder(default)]
    pub bucket_prefix: Option<String>,
    /// Configuration that determines how Amazon AppFlow should format the flow output data when Amazon S3 is used as the destination. See S3 Output Format Config for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "s3OutputFormatConfig")]
    #[builder(default)]
    pub s3_output_format_config: Option<FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesS3S3OutputFormatConfig>,
}

/// Reference to a BucketPolicy in s3 to populate bucketName.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesS3BucketNameRef {
    /// Name of the referenced object.
    pub name: String,
    /// Policies for referencing.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub policy: Option<FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesS3BucketNameRefPolicy>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesS3BucketNameRefPolicy {
    /// Resolution specifies whether resolution of this reference is required.
    /// The default is 'Required', which means the reconcile will fail if the
    /// reference cannot be resolved. 'Optional' means this reference will be
    /// a no-op if it cannot be resolved.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub resolution: Option<FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesS3BucketNameRefPolicyResolution>,
    /// Resolve specifies when this reference should be resolved. The default
    /// is 'IfNotPresent', which will attempt to resolve the reference only when
    /// the corresponding field is not present. Use 'Always' to resolve the
    /// reference on every reconcile.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub resolve: Option<FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesS3BucketNameRefPolicyResolve>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesS3BucketNameRefPolicyResolution {
    Required,
    Optional,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesS3BucketNameRefPolicyResolve {
    Always,
    IfNotPresent,
}

/// Selector for a BucketPolicy in s3 to populate bucketName.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesS3BucketNameSelector {
    /// MatchControllerRef ensures an object with the same controller reference
    /// as the selecting object is selected.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchControllerRef")]
    #[builder(default)]
    pub match_controller_ref: Option<bool>,
    /// MatchLabels ensures an object with matching labels is selected.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    #[builder(default)]
    pub match_labels: Option<HashMap<String, String>>,
    /// Policies for selection.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub policy: Option<FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesS3BucketNameSelectorPolicy>,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesS3BucketNameSelectorPolicy {
    /// Resolution specifies whether resolution of this reference is required.
    /// The default is 'Required', which means the reconcile will fail if the
    /// reference cannot be resolved. 'Optional' means this reference will be
    /// a no-op if it cannot be resolved.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub resolution: Option<FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesS3BucketNameSelectorPolicyResolution>,
    /// Resolve specifies when this reference should be resolved. The default
    /// is 'IfNotPresent', which will attempt to resolve the reference only when
    /// the corresponding field is not present. Use 'Always' to resolve the
    /// reference on every reconcile.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub resolve: Option<FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesS3BucketNameSelectorPolicyResolve>,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesS3BucketNameSelectorPolicyResolution {
    Required,
    Optional,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesS3BucketNameSelectorPolicyResolve {
    Always,
    IfNotPresent,
}

/// Configuration that determines how Amazon AppFlow should format the flow output data when Amazon S3 is used as the destination. See S3 Output Format Config for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesS3S3OutputFormatConfig {
    /// Aggregation settings that you can use to customize the output format of your flow data. See Aggregation Config for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "aggregationConfig")]
    #[builder(default)]
    pub aggregation_config: Option<FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesS3S3OutputFormatConfigAggregationConfig>,
    /// File type that Amazon AppFlow places in the Amazon S3 bucket. Valid values are CSV, JSON, and PARQUET.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fileType")]
    #[builder(default)]
    pub file_type: Option<String>,
    /// Determines the prefix that Amazon AppFlow applies to the folder name in the Amazon S3 bucket. You can name folders according to the flow frequency and date. See Prefix Config for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "prefixConfig")]
    #[builder(default)]
    pub prefix_config: Option<FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesS3S3OutputFormatConfigPrefixConfig>,
    /// Whether the data types from the source system need to be preserved (Only valid for Parquet file type)
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "preserveSourceDataTyping")]
    #[builder(default)]
    pub preserve_source_data_typing: Option<bool>,
}

/// Aggregation settings that you can use to customize the output format of your flow data. See Aggregation Config for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesS3S3OutputFormatConfigAggregationConfig {
    /// Whether Amazon AppFlow aggregates the flow records into a single file, or leave them unaggregated. Valid values are None and SingleFile.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "aggregationType")]
    #[builder(default)]
    pub aggregation_type: Option<String>,
    /// The desired file size, in MB, for each output file that Amazon AppFlow writes to the flow destination. Integer value.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "targetFileSize")]
    #[builder(default)]
    pub target_file_size: Option<f64>,
}

/// Determines the prefix that Amazon AppFlow applies to the folder name in the Amazon S3 bucket. You can name folders according to the flow frequency and date. See Prefix Config for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesS3S3OutputFormatConfigPrefixConfig {
    /// Determines the level of granularity that's included in the prefix. Valid values are YEAR, MONTH, DAY, HOUR, and MINUTE.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "prefixFormat")]
    #[builder(default)]
    pub prefix_format: Option<String>,
    /// Determines whether the destination file path includes either or both of the selected elements. Valid values are EXECUTION_ID and SCHEMA_VERSION
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "prefixHierarchy")]
    #[builder(default)]
    pub prefix_hierarchy: Option<Vec<String>>,
    /// Determines the format of the prefix, and whether it applies to the file name, file path, or both. Valid values are FILENAME, PATH, and PATH_AND_FILENAME.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "prefixType")]
    #[builder(default)]
    pub prefix_type: Option<String>,
}

/// Properties that are required to query Salesforce. See Salesforce Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesSalesforce {
    /// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "errorHandlingConfig")]
    #[builder(default)]
    pub error_handling_config: Option<FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesSalesforceErrorHandlingConfig>,
    /// Name of the field that Amazon AppFlow uses as an ID when performing a write operation such as update, delete, or upsert.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "idFieldNames")]
    #[builder(default)]
    pub id_field_names: Option<Vec<String>>,
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
    /// Type of write operation to be performed in the custom connector when it's used as destination. Valid values are INSERT, UPSERT, UPDATE, and DELETE.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "writeOperationType")]
    #[builder(default)]
    pub write_operation_type: Option<String>,
}

/// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesSalesforceErrorHandlingConfig {
    /// Amazon S3 bucket name in which Amazon AppFlow places the transferred data.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketName")]
    #[builder(default)]
    pub bucket_name: Option<String>,
    /// Object key for the bucket in which Amazon AppFlow places the destination files.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketPrefix")]
    #[builder(default)]
    pub bucket_prefix: Option<String>,
    /// If the flow should fail after the first instance of a failure when attempting to place data in the destination.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failOnFirstDestinationError")]
    #[builder(default)]
    pub fail_on_first_destination_error: Option<bool>,
}

/// Properties that are required to query SAPOData. See SAPOData Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesSapoData {
    /// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "errorHandlingConfig")]
    #[builder(default)]
    pub error_handling_config: Option<FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesSapoDataErrorHandlingConfig>,
    /// Name of the field that Amazon AppFlow uses as an ID when performing a write operation such as update, delete, or upsert.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "idFieldNames")]
    #[builder(default)]
    pub id_field_names: Option<Vec<String>>,
    /// Object path specified in the SAPOData flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "objectPath")]
    #[builder(default)]
    pub object_path: Option<String>,
    /// Determines how Amazon AppFlow handles the success response that it gets from the connector after placing data. See Success Response Handling Config for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "successResponseHandlingConfig")]
    #[builder(default)]
    pub success_response_handling_config: Option<FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesSapoDataSuccessResponseHandlingConfig>,
    /// Type of write operation to be performed in the custom connector when it's used as destination. Valid values are INSERT, UPSERT, UPDATE, and DELETE.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "writeOperationType")]
    #[builder(default)]
    pub write_operation_type: Option<String>,
}

/// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesSapoDataErrorHandlingConfig {
    /// Amazon S3 bucket name in which Amazon AppFlow places the transferred data.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketName")]
    #[builder(default)]
    pub bucket_name: Option<String>,
    /// Object key for the bucket in which Amazon AppFlow places the destination files.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketPrefix")]
    #[builder(default)]
    pub bucket_prefix: Option<String>,
    /// If the flow should fail after the first instance of a failure when attempting to place data in the destination.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failOnFirstDestinationError")]
    #[builder(default)]
    pub fail_on_first_destination_error: Option<bool>,
}

/// Determines how Amazon AppFlow handles the success response that it gets from the connector after placing data. See Success Response Handling Config for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesSapoDataSuccessResponseHandlingConfig {
    /// Amazon S3 bucket name in which Amazon AppFlow places the transferred data.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketName")]
    #[builder(default)]
    pub bucket_name: Option<String>,
    /// Object key for the bucket in which Amazon AppFlow places the destination files.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketPrefix")]
    #[builder(default)]
    pub bucket_prefix: Option<String>,
}

/// Properties that are required to query Snowflake. See Snowflake Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesSnowflake {
    /// Object key for the bucket in which Amazon AppFlow places the destination files.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketPrefix")]
    #[builder(default)]
    pub bucket_prefix: Option<String>,
    /// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "errorHandlingConfig")]
    #[builder(default)]
    pub error_handling_config: Option<FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesSnowflakeErrorHandlingConfig>,
    /// Intermediate bucket that Amazon AppFlow uses when moving data into Amazon Redshift.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "intermediateBucketName")]
    #[builder(default)]
    pub intermediate_bucket_name: Option<String>,
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

/// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesSnowflakeErrorHandlingConfig {
    /// Amazon S3 bucket name in which Amazon AppFlow places the transferred data.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketName")]
    #[builder(default)]
    pub bucket_name: Option<String>,
    /// Object key for the bucket in which Amazon AppFlow places the destination files.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketPrefix")]
    #[builder(default)]
    pub bucket_prefix: Option<String>,
    /// If the flow should fail after the first instance of a failure when attempting to place data in the destination.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failOnFirstDestinationError")]
    #[builder(default)]
    pub fail_on_first_destination_error: Option<bool>,
}

/// Properties that are required to query Upsolver. See Upsolver Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesUpsolver {
    /// Amazon S3 bucket name in which Amazon AppFlow places the transferred data.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketName")]
    #[builder(default)]
    pub bucket_name: Option<String>,
    /// Object key for the bucket in which Amazon AppFlow places the destination files.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketPrefix")]
    #[builder(default)]
    pub bucket_prefix: Option<String>,
    /// Configuration that determines how Amazon AppFlow should format the flow output data when Amazon S3 is used as the destination. See S3 Output Format Config for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "s3OutputFormatConfig")]
    #[builder(default)]
    pub s3_output_format_config: Option<FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesUpsolverS3OutputFormatConfig>,
}

/// Configuration that determines how Amazon AppFlow should format the flow output data when Amazon S3 is used as the destination. See S3 Output Format Config for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesUpsolverS3OutputFormatConfig {
    /// Aggregation settings that you can use to customize the output format of your flow data. See Aggregation Config for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "aggregationConfig")]
    #[builder(default)]
    pub aggregation_config: Option<FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesUpsolverS3OutputFormatConfigAggregationConfig>,
    /// File type that Amazon AppFlow places in the Amazon S3 bucket. Valid values are CSV, JSON, and PARQUET.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fileType")]
    #[builder(default)]
    pub file_type: Option<String>,
    /// Determines the prefix that Amazon AppFlow applies to the folder name in the Amazon S3 bucket. You can name folders according to the flow frequency and date. See Prefix Config for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "prefixConfig")]
    #[builder(default)]
    pub prefix_config: Option<FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesUpsolverS3OutputFormatConfigPrefixConfig>,
}

/// Aggregation settings that you can use to customize the output format of your flow data. See Aggregation Config for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesUpsolverS3OutputFormatConfigAggregationConfig {
    /// Whether Amazon AppFlow aggregates the flow records into a single file, or leave them unaggregated. Valid values are None and SingleFile.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "aggregationType")]
    #[builder(default)]
    pub aggregation_type: Option<String>,
}

/// Determines the prefix that Amazon AppFlow applies to the folder name in the Amazon S3 bucket. You can name folders according to the flow frequency and date. See Prefix Config for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesUpsolverS3OutputFormatConfigPrefixConfig {
    /// Determines the level of granularity that's included in the prefix. Valid values are YEAR, MONTH, DAY, HOUR, and MINUTE.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "prefixFormat")]
    #[builder(default)]
    pub prefix_format: Option<String>,
    /// Determines whether the destination file path includes either or both of the selected elements. Valid values are EXECUTION_ID and SCHEMA_VERSION
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "prefixHierarchy")]
    #[builder(default)]
    pub prefix_hierarchy: Option<Vec<String>>,
    /// Determines the format of the prefix, and whether it applies to the file name, file path, or both. Valid values are FILENAME, PATH, and PATH_AND_FILENAME.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "prefixType")]
    #[builder(default)]
    pub prefix_type: Option<String>,
}

/// Properties that are required to query Zendesk. See Zendesk Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesZendesk {
    /// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "errorHandlingConfig")]
    #[builder(default)]
    pub error_handling_config: Option<FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesZendeskErrorHandlingConfig>,
    /// Name of the field that Amazon AppFlow uses as an ID when performing a write operation such as update, delete, or upsert.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "idFieldNames")]
    #[builder(default)]
    pub id_field_names: Option<Vec<String>>,
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
    /// Type of write operation to be performed in the custom connector when it's used as destination. Valid values are INSERT, UPSERT, UPDATE, and DELETE.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "writeOperationType")]
    #[builder(default)]
    pub write_operation_type: Option<String>,
}

/// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderDestinationFlowConfigDestinationConnectorPropertiesZendeskErrorHandlingConfig {
    /// Amazon S3 bucket name in which Amazon AppFlow places the transferred data.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketName")]
    #[builder(default)]
    pub bucket_name: Option<String>,
    /// Object key for the bucket in which Amazon AppFlow places the destination files.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketPrefix")]
    #[builder(default)]
    pub bucket_prefix: Option<String>,
    /// If the flow should fail after the first instance of a failure when attempting to place data in the destination.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failOnFirstDestinationError")]
    #[builder(default)]
    pub fail_on_first_destination_error: Option<bool>,
}

/// A Catalog that determines the configuration that Amazon AppFlow uses when it catalogs the data thatâ€™s transferred by the associated flow. When Amazon AppFlow catalogs the data from a flow, it stores metadata in a data catalog.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderMetadataCatalogConfig {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "glueDataCatalog")]
    #[builder(default)]
    pub glue_data_catalog: Option<FlowForProviderMetadataCatalogConfigGlueDataCatalog>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderMetadataCatalogConfigGlueDataCatalog {
    /// The name of an existing Glue database to store the metadata tables that Amazon AppFlow creates.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "databaseName")]
    #[builder(default)]
    pub database_name: Option<String>,
    /// The ARN of an IAM role that grants AppFlow the permissions it needs to create Data Catalog tables, databases, and partitions.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "roleArn")]
    #[builder(default)]
    pub role_arn: Option<String>,
    /// A naming prefix for each Data Catalog table that Amazon AppFlow creates
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tablePrefix")]
    #[builder(default)]
    pub table_prefix: Option<String>,
}

/// The Source Flow Config that controls how Amazon AppFlow retrieves data from the source connector.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderSourceFlowConfig {
    /// API version that the destination connector uses.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "apiVersion")]
    #[builder(default)]
    pub api_version: Option<String>,
    /// Name of the connector profile. This name must be unique for each connector profile in the AWS account.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "connectorProfileName")]
    #[builder(default)]
    pub connector_profile_name: Option<String>,
    /// Type of connector, such as Salesforce, Amplitude, and so on. Valid values are Salesforce, Singular, Slack, Redshift, S3, Marketo, Googleanalytics, Zendesk, Servicenow, Datadog, Trendmicro, Snowflake, Dynatrace, Infornexus, Amplitude, Veeva, EventBridge, LookoutMetrics, Upsolver, Honeycode, CustomerProfiles, SAPOData, and CustomConnector.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "connectorType")]
    #[builder(default)]
    pub connector_type: Option<String>,
    /// Defines the configuration for a scheduled incremental data pull. If a valid configuration is provided, the fields specified in the configuration are used when querying for the incremental data pull. See Incremental Pull Config for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "incrementalPullConfig")]
    #[builder(default)]
    pub incremental_pull_config: Option<FlowForProviderSourceFlowConfigIncrementalPullConfig>,
    /// Information that is required to query a particular source connector. See Source Connector Properties for details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sourceConnectorProperties")]
    #[builder(default)]
    pub source_connector_properties: Option<FlowForProviderSourceFlowConfigSourceConnectorProperties>,
}

/// Defines the configuration for a scheduled incremental data pull. If a valid configuration is provided, the fields specified in the configuration are used when querying for the incremental data pull. See Incremental Pull Config for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderSourceFlowConfigIncrementalPullConfig {
    /// Field that specifies the date time or timestamp field as the criteria to use when importing incremental records from the source.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "datetimeTypeFieldName")]
    #[builder(default)]
    pub datetime_type_field_name: Option<String>,
}

/// Information that is required to query a particular source connector. See Source Connector Properties for details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderSourceFlowConfigSourceConnectorProperties {
    /// Information that is required for querying Amplitude. See Generic Source Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub amplitude: Option<FlowForProviderSourceFlowConfigSourceConnectorPropertiesAmplitude>,
    /// Properties that are required to query the custom Connector. See Custom Connector Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "customConnector")]
    #[builder(default)]
    pub custom_connector: Option<FlowForProviderSourceFlowConfigSourceConnectorPropertiesCustomConnector>,
    /// Information that is required for querying Datadog. See Generic Source Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub datadog: Option<FlowForProviderSourceFlowConfigSourceConnectorPropertiesDatadog>,
    /// Operation to be performed on the provided Dynatrace source fields. Valid values are PROJECTION, BETWEEN, EQUAL_TO, ADDITION, MULTIPLICATION, DIVISION, SUBTRACTION, MASK_ALL, MASK_FIRST_N, MASK_LAST_N, VALIDATE_NON_NULL, VALIDATE_NON_ZERO, VALIDATE_NON_NEGATIVE, VALIDATE_NUMERIC, and NO_OP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub dynatrace: Option<FlowForProviderSourceFlowConfigSourceConnectorPropertiesDynatrace>,
    /// Operation to be performed on the provided Google Analytics source fields. Valid values are PROJECTION and BETWEEN.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "googleAnalytics")]
    #[builder(default)]
    pub google_analytics: Option<FlowForProviderSourceFlowConfigSourceConnectorPropertiesGoogleAnalytics>,
    /// Information that is required for querying Infor Nexus. See Generic Source Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "inforNexus")]
    #[builder(default)]
    pub infor_nexus: Option<FlowForProviderSourceFlowConfigSourceConnectorPropertiesInforNexus>,
    /// Properties that are required to query Marketo. See Generic Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub marketo: Option<FlowForProviderSourceFlowConfigSourceConnectorPropertiesMarketo>,
    /// Properties that are required to query Amazon S3. See S3 Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub s3: Option<FlowForProviderSourceFlowConfigSourceConnectorPropertiesS3>,
    /// Properties that are required to query Salesforce. See Salesforce Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub salesforce: Option<FlowForProviderSourceFlowConfigSourceConnectorPropertiesSalesforce>,
    /// Properties that are required to query SAPOData. See SAPOData Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sapoData")]
    #[builder(default)]
    pub sapo_data: Option<FlowForProviderSourceFlowConfigSourceConnectorPropertiesSapoData>,
    /// Information that is required for querying ServiceNow. See Generic Source Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceNow")]
    #[builder(default)]
    pub service_now: Option<FlowForProviderSourceFlowConfigSourceConnectorPropertiesServiceNow>,
    /// Information that is required for querying Singular. See Generic Source Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub singular: Option<FlowForProviderSourceFlowConfigSourceConnectorPropertiesSingular>,
    /// Information that is required for querying Slack. See Generic Source Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub slack: Option<FlowForProviderSourceFlowConfigSourceConnectorPropertiesSlack>,
    /// Operation to be performed on the provided Trend Micro source fields. Valid values are PROJECTION, EQUAL_TO, ADDITION, MULTIPLICATION, DIVISION, SUBTRACTION, MASK_ALL, MASK_FIRST_N, MASK_LAST_N, VALIDATE_NON_NULL, VALIDATE_NON_ZERO, VALIDATE_NON_NEGATIVE, VALIDATE_NUMERIC, and NO_OP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub trendmicro: Option<FlowForProviderSourceFlowConfigSourceConnectorPropertiesTrendmicro>,
    /// Information that is required for querying Veeva. See Veeva Source Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub veeva: Option<FlowForProviderSourceFlowConfigSourceConnectorPropertiesVeeva>,
    /// Properties that are required to query Zendesk. See Zendesk Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub zendesk: Option<FlowForProviderSourceFlowConfigSourceConnectorPropertiesZendesk>,
}

/// Information that is required for querying Amplitude. See Generic Source Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderSourceFlowConfigSourceConnectorPropertiesAmplitude {
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

/// Properties that are required to query the custom Connector. See Custom Connector Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderSourceFlowConfigSourceConnectorPropertiesCustomConnector {
    /// Custom properties that are specific to the connector when it's used as a destination in the flow. Maximum of 50 items.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "customProperties")]
    #[builder(default)]
    pub custom_properties: Option<HashMap<String, String>>,
    /// Entity specified in the custom connector as a destination in the flow.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "entityName")]
    #[builder(default)]
    pub entity_name: Option<String>,
}

/// Information that is required for querying Datadog. See Generic Source Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderSourceFlowConfigSourceConnectorPropertiesDatadog {
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

/// Operation to be performed on the provided Dynatrace source fields. Valid values are PROJECTION, BETWEEN, EQUAL_TO, ADDITION, MULTIPLICATION, DIVISION, SUBTRACTION, MASK_ALL, MASK_FIRST_N, MASK_LAST_N, VALIDATE_NON_NULL, VALIDATE_NON_ZERO, VALIDATE_NON_NEGATIVE, VALIDATE_NUMERIC, and NO_OP.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderSourceFlowConfigSourceConnectorPropertiesDynatrace {
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

/// Operation to be performed on the provided Google Analytics source fields. Valid values are PROJECTION and BETWEEN.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderSourceFlowConfigSourceConnectorPropertiesGoogleAnalytics {
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

/// Information that is required for querying Infor Nexus. See Generic Source Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderSourceFlowConfigSourceConnectorPropertiesInforNexus {
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

/// Properties that are required to query Marketo. See Generic Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderSourceFlowConfigSourceConnectorPropertiesMarketo {
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

/// Properties that are required to query Amazon S3. See S3 Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderSourceFlowConfigSourceConnectorPropertiesS3 {
    /// Amazon S3 bucket name in which Amazon AppFlow places the transferred data.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketName")]
    #[builder(default)]
    pub bucket_name: Option<String>,
    /// Reference to a BucketPolicy in s3 to populate bucketName.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketNameRef")]
    #[builder(default)]
    pub bucket_name_ref: Option<FlowForProviderSourceFlowConfigSourceConnectorPropertiesS3BucketNameRef>,
    /// Selector for a BucketPolicy in s3 to populate bucketName.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketNameSelector")]
    #[builder(default)]
    pub bucket_name_selector: Option<FlowForProviderSourceFlowConfigSourceConnectorPropertiesS3BucketNameSelector>,
    /// Object key for the bucket in which Amazon AppFlow places the destination files.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketPrefix")]
    #[builder(default)]
    pub bucket_prefix: Option<String>,
    /// When you use Amazon S3 as the source, the configuration format that you provide the flow input data. See S3 Input Format Config for details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "s3InputFormatConfig")]
    #[builder(default)]
    pub s3_input_format_config: Option<FlowForProviderSourceFlowConfigSourceConnectorPropertiesS3S3InputFormatConfig>,
}

/// Reference to a BucketPolicy in s3 to populate bucketName.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderSourceFlowConfigSourceConnectorPropertiesS3BucketNameRef {
    /// Name of the referenced object.
    pub name: String,
    /// Policies for referencing.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub policy: Option<FlowForProviderSourceFlowConfigSourceConnectorPropertiesS3BucketNameRefPolicy>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderSourceFlowConfigSourceConnectorPropertiesS3BucketNameRefPolicy {
    /// Resolution specifies whether resolution of this reference is required.
    /// The default is 'Required', which means the reconcile will fail if the
    /// reference cannot be resolved. 'Optional' means this reference will be
    /// a no-op if it cannot be resolved.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub resolution: Option<FlowForProviderSourceFlowConfigSourceConnectorPropertiesS3BucketNameRefPolicyResolution>,
    /// Resolve specifies when this reference should be resolved. The default
    /// is 'IfNotPresent', which will attempt to resolve the reference only when
    /// the corresponding field is not present. Use 'Always' to resolve the
    /// reference on every reconcile.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub resolve: Option<FlowForProviderSourceFlowConfigSourceConnectorPropertiesS3BucketNameRefPolicyResolve>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum FlowForProviderSourceFlowConfigSourceConnectorPropertiesS3BucketNameRefPolicyResolution {
    Required,
    Optional,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum FlowForProviderSourceFlowConfigSourceConnectorPropertiesS3BucketNameRefPolicyResolve {
    Always,
    IfNotPresent,
}

/// Selector for a BucketPolicy in s3 to populate bucketName.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderSourceFlowConfigSourceConnectorPropertiesS3BucketNameSelector {
    /// MatchControllerRef ensures an object with the same controller reference
    /// as the selecting object is selected.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchControllerRef")]
    #[builder(default)]
    pub match_controller_ref: Option<bool>,
    /// MatchLabels ensures an object with matching labels is selected.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    #[builder(default)]
    pub match_labels: Option<HashMap<String, String>>,
    /// Policies for selection.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub policy: Option<FlowForProviderSourceFlowConfigSourceConnectorPropertiesS3BucketNameSelectorPolicy>,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderSourceFlowConfigSourceConnectorPropertiesS3BucketNameSelectorPolicy {
    /// Resolution specifies whether resolution of this reference is required.
    /// The default is 'Required', which means the reconcile will fail if the
    /// reference cannot be resolved. 'Optional' means this reference will be
    /// a no-op if it cannot be resolved.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub resolution: Option<FlowForProviderSourceFlowConfigSourceConnectorPropertiesS3BucketNameSelectorPolicyResolution>,
    /// Resolve specifies when this reference should be resolved. The default
    /// is 'IfNotPresent', which will attempt to resolve the reference only when
    /// the corresponding field is not present. Use 'Always' to resolve the
    /// reference on every reconcile.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub resolve: Option<FlowForProviderSourceFlowConfigSourceConnectorPropertiesS3BucketNameSelectorPolicyResolve>,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum FlowForProviderSourceFlowConfigSourceConnectorPropertiesS3BucketNameSelectorPolicyResolution {
    Required,
    Optional,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum FlowForProviderSourceFlowConfigSourceConnectorPropertiesS3BucketNameSelectorPolicyResolve {
    Always,
    IfNotPresent,
}

/// When you use Amazon S3 as the source, the configuration format that you provide the flow input data. See S3 Input Format Config for details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderSourceFlowConfigSourceConnectorPropertiesS3S3InputFormatConfig {
    /// File type that Amazon AppFlow gets from your Amazon S3 bucket. Valid values are CSV and JSON.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "s3InputFileType")]
    #[builder(default)]
    pub s3_input_file_type: Option<String>,
}

/// Properties that are required to query Salesforce. See Salesforce Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderSourceFlowConfigSourceConnectorPropertiesSalesforce {
    /// Flag that enables dynamic fetching of new (recently added) fields in the Salesforce objects while running a flow.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableDynamicFieldUpdate")]
    #[builder(default)]
    pub enable_dynamic_field_update: Option<bool>,
    /// Whether Amazon AppFlow includes deleted files in the flow run.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "includeDeletedRecords")]
    #[builder(default)]
    pub include_deleted_records: Option<bool>,
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

/// Properties that are required to query SAPOData. See SAPOData Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderSourceFlowConfigSourceConnectorPropertiesSapoData {
    /// Object path specified in the SAPOData flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "objectPath")]
    #[builder(default)]
    pub object_path: Option<String>,
    /// Sets the page size for each concurrent process that transfers OData records from your SAP instance.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "paginationConfig")]
    #[builder(default)]
    pub pagination_config: Option<FlowForProviderSourceFlowConfigSourceConnectorPropertiesSapoDataPaginationConfig>,
    /// Sets the number of concurrent processes that transfers OData records from your SAP instance.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "parallelismConfig")]
    #[builder(default)]
    pub parallelism_config: Option<FlowForProviderSourceFlowConfigSourceConnectorPropertiesSapoDataParallelismConfig>,
}

/// Sets the page size for each concurrent process that transfers OData records from your SAP instance.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderSourceFlowConfigSourceConnectorPropertiesSapoDataPaginationConfig {
    /// he maximum number of records that Amazon AppFlow receives in each page of the response from your SAP application.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxPageSize")]
    #[builder(default)]
    pub max_page_size: Option<f64>,
}

/// Sets the number of concurrent processes that transfers OData records from your SAP instance.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderSourceFlowConfigSourceConnectorPropertiesSapoDataParallelismConfig {
    /// he maximum number of records that Amazon AppFlow receives in each page of the response from your SAP application.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxPageSize")]
    #[builder(default)]
    pub max_page_size: Option<f64>,
}

/// Information that is required for querying ServiceNow. See Generic Source Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderSourceFlowConfigSourceConnectorPropertiesServiceNow {
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

/// Information that is required for querying Singular. See Generic Source Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderSourceFlowConfigSourceConnectorPropertiesSingular {
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

/// Information that is required for querying Slack. See Generic Source Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderSourceFlowConfigSourceConnectorPropertiesSlack {
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

/// Operation to be performed on the provided Trend Micro source fields. Valid values are PROJECTION, EQUAL_TO, ADDITION, MULTIPLICATION, DIVISION, SUBTRACTION, MASK_ALL, MASK_FIRST_N, MASK_LAST_N, VALIDATE_NON_NULL, VALIDATE_NON_ZERO, VALIDATE_NON_NEGATIVE, VALIDATE_NUMERIC, and NO_OP.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderSourceFlowConfigSourceConnectorPropertiesTrendmicro {
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

/// Information that is required for querying Veeva. See Veeva Source Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderSourceFlowConfigSourceConnectorPropertiesVeeva {
    /// Document type specified in the Veeva document extract flow.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "documentType")]
    #[builder(default)]
    pub document_type: Option<String>,
    /// Boolean value to include All Versions of files in Veeva document extract flow.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "includeAllVersions")]
    #[builder(default)]
    pub include_all_versions: Option<bool>,
    /// Boolean value to include file renditions in Veeva document extract flow.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "includeRenditions")]
    #[builder(default)]
    pub include_renditions: Option<bool>,
    /// Boolean value to include source files in Veeva document extract flow.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "includeSourceFiles")]
    #[builder(default)]
    pub include_source_files: Option<bool>,
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

/// Properties that are required to query Zendesk. See Zendesk Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderSourceFlowConfigSourceConnectorPropertiesZendesk {
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderTask {
    /// Operation to be performed on the provided source fields. See Connector Operator for details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "connectorOperator")]
    #[builder(default)]
    pub connector_operator: Option<Vec<FlowForProviderTaskConnectorOperator>>,
    /// Field in a destination connector, or a field value against which Amazon AppFlow validates a source field.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "destinationField")]
    #[builder(default)]
    pub destination_field: Option<String>,
    /// Source fields to which a particular task is applied.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sourceFields")]
    #[builder(default)]
    pub source_fields: Option<Vec<String>>,
    /// Map used to store task-related information. The execution service looks for particular information based on the TaskType. Valid keys are VALUE, VALUES, DATA_TYPE, UPPER_BOUND, LOWER_BOUND, SOURCE_DATA_TYPE, DESTINATION_DATA_TYPE, VALIDATION_ACTION, MASK_VALUE, MASK_LENGTH, TRUNCATE_LENGTH, MATH_OPERATION_FIELDS_ORDER, CONCAT_FORMAT, SUBFIELD_CATEGORY_MAP, and EXCLUDE_SOURCE_FIELDS_LIST.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "taskProperties")]
    #[builder(default)]
    pub task_properties: Option<HashMap<String, String>>,
    /// Particular task implementation that Amazon AppFlow performs. Valid values are Arithmetic, Filter, Map, Map_all, Mask, Merge, Passthrough, Truncate, and Validate.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "taskType")]
    #[builder(default)]
    pub task_type: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderTaskConnectorOperator {
    /// Information that is required for querying Amplitude. See Generic Source Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub amplitude: Option<String>,
    /// Properties that are required to query the custom Connector. See Custom Connector Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "customConnector")]
    #[builder(default)]
    pub custom_connector: Option<String>,
    /// Information that is required for querying Datadog. See Generic Source Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub datadog: Option<String>,
    /// Operation to be performed on the provided Dynatrace source fields. Valid values are PROJECTION, BETWEEN, EQUAL_TO, ADDITION, MULTIPLICATION, DIVISION, SUBTRACTION, MASK_ALL, MASK_FIRST_N, MASK_LAST_N, VALIDATE_NON_NULL, VALIDATE_NON_ZERO, VALIDATE_NON_NEGATIVE, VALIDATE_NUMERIC, and NO_OP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub dynatrace: Option<String>,
    /// Operation to be performed on the provided Google Analytics source fields. Valid values are PROJECTION and BETWEEN.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "googleAnalytics")]
    #[builder(default)]
    pub google_analytics: Option<String>,
    /// Information that is required for querying Infor Nexus. See Generic Source Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "inforNexus")]
    #[builder(default)]
    pub infor_nexus: Option<String>,
    /// Properties that are required to query Marketo. See Generic Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub marketo: Option<String>,
    /// Properties that are required to query Amazon S3. See S3 Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub s3: Option<String>,
    /// Properties that are required to query Salesforce. See Salesforce Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub salesforce: Option<String>,
    /// Properties that are required to query SAPOData. See SAPOData Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sapoData")]
    #[builder(default)]
    pub sapo_data: Option<String>,
    /// Information that is required for querying ServiceNow. See Generic Source Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceNow")]
    #[builder(default)]
    pub service_now: Option<String>,
    /// Information that is required for querying Singular. See Generic Source Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub singular: Option<String>,
    /// Information that is required for querying Slack. See Generic Source Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub slack: Option<String>,
    /// Operation to be performed on the provided Trend Micro source fields. Valid values are PROJECTION, EQUAL_TO, ADDITION, MULTIPLICATION, DIVISION, SUBTRACTION, MASK_ALL, MASK_FIRST_N, MASK_LAST_N, VALIDATE_NON_NULL, VALIDATE_NON_ZERO, VALIDATE_NON_NEGATIVE, VALIDATE_NUMERIC, and NO_OP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub trendmicro: Option<String>,
    /// Information that is required for querying Veeva. See Veeva Source Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub veeva: Option<String>,
    /// Properties that are required to query Zendesk. See Zendesk Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub zendesk: Option<String>,
}

/// A Trigger that determine how and when the flow runs.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderTriggerConfig {
    /// Configuration details of a schedule-triggered flow as defined by the user. Currently, these settings only apply to the Scheduled trigger type. See Scheduled Trigger Properties for details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "triggerProperties")]
    #[builder(default)]
    pub trigger_properties: Option<FlowForProviderTriggerConfigTriggerProperties>,
    /// Type of flow trigger. Valid values are Scheduled, Event, and OnDemand.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "triggerType")]
    #[builder(default)]
    pub trigger_type: Option<String>,
}

/// Configuration details of a schedule-triggered flow as defined by the user. Currently, these settings only apply to the Scheduled trigger type. See Scheduled Trigger Properties for details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderTriggerConfigTriggerProperties {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub scheduled: Option<FlowForProviderTriggerConfigTriggerPropertiesScheduled>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowForProviderTriggerConfigTriggerPropertiesScheduled {
    /// Whether a scheduled flow has an incremental data transfer or a complete data transfer for each flow run. Valid values are Incremental and Complete.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dataPullMode")]
    #[builder(default)]
    pub data_pull_mode: Option<String>,
    /// Date range for the records to import from the connector in the first flow run. Must be a valid RFC3339 timestamp.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "firstExecutionFrom")]
    #[builder(default)]
    pub first_execution_from: Option<String>,
    /// Scheduled end time for a schedule-triggered flow. Must be a valid RFC3339 timestamp.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "scheduleEndTime")]
    #[builder(default)]
    pub schedule_end_time: Option<String>,
    /// Scheduling expression that determines the rate at which the schedule will run, for example rate(5minutes).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "scheduleExpression")]
    #[builder(default)]
    pub schedule_expression: Option<String>,
    /// Optional offset that is added to the time interval for a schedule-triggered flow. Maximum value of 36000.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "scheduleOffset")]
    #[builder(default)]
    pub schedule_offset: Option<f64>,
    /// Scheduled start time for a schedule-triggered flow. Must be a valid RFC3339 timestamp.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "scheduleStartTime")]
    #[builder(default)]
    pub schedule_start_time: Option<String>,
    /// Time zone used when referring to the date and time of a scheduled-triggered flow, such as America/New_York.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub timezone: Option<String>,
}

/// THIS IS A BETA FIELD. It will be honored
/// unless the Management Policies feature flag is disabled.
/// InitProvider holds the same fields as ForProvider, with the exception
/// of Identifier and other resource reference fields. The fields that are
/// in InitProvider are merged into ForProvider when the resource is created.
/// The same fields are also added to the terraform ignore_changes hook, to
/// avoid updating them after creation. This is useful for fields that are
/// required on creation, but we do not desire to update them after creation,
/// for example because of an external controller is managing them, like an
/// autoscaler.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProvider {
    /// Description of the flow you want to create.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub description: Option<String>,
    /// A Destination Flow Config that controls how Amazon AppFlow places data in the destination connector.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "destinationFlowConfig")]
    #[builder(default)]
    pub destination_flow_config: Option<Vec<FlowInitProviderDestinationFlowConfig>>,
    /// ARN (Amazon Resource Name) of the Key Management Service (KMS) key you provide for encryption. This is required if you do not want to use the Amazon AppFlow-managed KMS key. If you don't provide anything here, Amazon AppFlow uses the Amazon AppFlow-managed KMS key.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kmsArn")]
    #[builder(default)]
    pub kms_arn: Option<String>,
    /// A Catalog that determines the configuration that Amazon AppFlow uses when it catalogs the data thatâ€™s transferred by the associated flow. When Amazon AppFlow catalogs the data from a flow, it stores metadata in a data catalog.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "metadataCatalogConfig")]
    #[builder(default)]
    pub metadata_catalog_config: Option<FlowInitProviderMetadataCatalogConfig>,
    /// The Source Flow Config that controls how Amazon AppFlow retrieves data from the source connector.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sourceFlowConfig")]
    #[builder(default)]
    pub source_flow_config: Option<FlowInitProviderSourceFlowConfig>,
    /// Key-value map of resource tags.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub tags: Option<HashMap<String, String>>,
    /// A Task that Amazon AppFlow performs while transferring the data in the flow run.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub task: Option<Vec<FlowInitProviderTask>>,
    /// A Trigger that determine how and when the flow runs.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "triggerConfig")]
    #[builder(default)]
    pub trigger_config: Option<FlowInitProviderTriggerConfig>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderDestinationFlowConfig {
    /// API version that the destination connector uses.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "apiVersion")]
    #[builder(default)]
    pub api_version: Option<String>,
    /// Name of the connector profile. This name must be unique for each connector profile in the AWS account.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "connectorProfileName")]
    #[builder(default)]
    pub connector_profile_name: Option<String>,
    /// Type of connector, such as Salesforce, Amplitude, and so on. Valid values are Salesforce, Singular, Slack, Redshift, S3, Marketo, Googleanalytics, Zendesk, Servicenow, Datadog, Trendmicro, Snowflake, Dynatrace, Infornexus, Amplitude, Veeva, EventBridge, LookoutMetrics, Upsolver, Honeycode, CustomerProfiles, SAPOData, and CustomConnector.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "connectorType")]
    #[builder(default)]
    pub connector_type: Option<String>,
    /// This stores the information that is required to query a particular connector. See Destination Connector Properties for more information.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "destinationConnectorProperties")]
    #[builder(default)]
    pub destination_connector_properties: Option<FlowInitProviderDestinationFlowConfigDestinationConnectorProperties>,
}

/// This stores the information that is required to query a particular connector. See Destination Connector Properties for more information.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderDestinationFlowConfigDestinationConnectorProperties {
    /// Properties that are required to query the custom Connector. See Custom Connector Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "customConnector")]
    #[builder(default)]
    pub custom_connector: Option<FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesCustomConnector>,
    /// Properties that are required to query Amazon Connect Customer Profiles. See Customer Profiles Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "customerProfiles")]
    #[builder(default)]
    pub customer_profiles: Option<FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesCustomerProfiles>,
    /// Properties that are required to query Amazon EventBridge. See Generic Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "eventBridge")]
    #[builder(default)]
    pub event_bridge: Option<FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesEventBridge>,
    /// Properties that are required to query Amazon Honeycode. See Generic Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub honeycode: Option<FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesHoneycode>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "lookoutMetrics")]
    #[builder(default)]
    pub lookout_metrics: Option<FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesLookoutMetrics>,
    /// Properties that are required to query Marketo. See Generic Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub marketo: Option<FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesMarketo>,
    /// Properties that are required to query Amazon Redshift. See Redshift Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub redshift: Option<FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesRedshift>,
    /// Properties that are required to query Amazon S3. See S3 Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub s3: Option<FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesS3>,
    /// Properties that are required to query Salesforce. See Salesforce Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub salesforce: Option<FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesSalesforce>,
    /// Properties that are required to query SAPOData. See SAPOData Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sapoData")]
    #[builder(default)]
    pub sapo_data: Option<FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesSapoData>,
    /// Properties that are required to query Snowflake. See Snowflake Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub snowflake: Option<FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesSnowflake>,
    /// Properties that are required to query Upsolver. See Upsolver Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub upsolver: Option<FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesUpsolver>,
    /// Properties that are required to query Zendesk. See Zendesk Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub zendesk: Option<FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesZendesk>,
}

/// Properties that are required to query the custom Connector. See Custom Connector Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesCustomConnector {
    /// Custom properties that are specific to the connector when it's used as a destination in the flow. Maximum of 50 items.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "customProperties")]
    #[builder(default)]
    pub custom_properties: Option<HashMap<String, String>>,
    /// Entity specified in the custom connector as a destination in the flow.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "entityName")]
    #[builder(default)]
    pub entity_name: Option<String>,
    /// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "errorHandlingConfig")]
    #[builder(default)]
    pub error_handling_config: Option<FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesCustomConnectorErrorHandlingConfig>,
    /// Name of the field that Amazon AppFlow uses as an ID when performing a write operation such as update, delete, or upsert.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "idFieldNames")]
    #[builder(default)]
    pub id_field_names: Option<Vec<String>>,
    /// Type of write operation to be performed in the custom connector when it's used as destination. Valid values are INSERT, UPSERT, UPDATE, and DELETE.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "writeOperationType")]
    #[builder(default)]
    pub write_operation_type: Option<String>,
}

/// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesCustomConnectorErrorHandlingConfig {
    /// Amazon S3 bucket name in which Amazon AppFlow places the transferred data.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketName")]
    #[builder(default)]
    pub bucket_name: Option<String>,
    /// Object key for the bucket in which Amazon AppFlow places the destination files.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketPrefix")]
    #[builder(default)]
    pub bucket_prefix: Option<String>,
    /// If the flow should fail after the first instance of a failure when attempting to place data in the destination.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failOnFirstDestinationError")]
    #[builder(default)]
    pub fail_on_first_destination_error: Option<bool>,
}

/// Properties that are required to query Amazon Connect Customer Profiles. See Customer Profiles Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesCustomerProfiles {
    /// Unique name of the Amazon Connect Customer Profiles domain.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "domainName")]
    #[builder(default)]
    pub domain_name: Option<String>,
    /// Object specified in the Amazon Connect Customer Profiles flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "objectTypeName")]
    #[builder(default)]
    pub object_type_name: Option<String>,
}

/// Properties that are required to query Amazon EventBridge. See Generic Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesEventBridge {
    /// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "errorHandlingConfig")]
    #[builder(default)]
    pub error_handling_config: Option<FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesEventBridgeErrorHandlingConfig>,
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

/// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesEventBridgeErrorHandlingConfig {
    /// Amazon S3 bucket name in which Amazon AppFlow places the transferred data.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketName")]
    #[builder(default)]
    pub bucket_name: Option<String>,
    /// Object key for the bucket in which Amazon AppFlow places the destination files.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketPrefix")]
    #[builder(default)]
    pub bucket_prefix: Option<String>,
    /// If the flow should fail after the first instance of a failure when attempting to place data in the destination.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failOnFirstDestinationError")]
    #[builder(default)]
    pub fail_on_first_destination_error: Option<bool>,
}

/// Properties that are required to query Amazon Honeycode. See Generic Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesHoneycode {
    /// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "errorHandlingConfig")]
    #[builder(default)]
    pub error_handling_config: Option<FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesHoneycodeErrorHandlingConfig>,
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

/// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesHoneycodeErrorHandlingConfig {
    /// Amazon S3 bucket name in which Amazon AppFlow places the transferred data.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketName")]
    #[builder(default)]
    pub bucket_name: Option<String>,
    /// Object key for the bucket in which Amazon AppFlow places the destination files.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketPrefix")]
    #[builder(default)]
    pub bucket_prefix: Option<String>,
    /// If the flow should fail after the first instance of a failure when attempting to place data in the destination.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failOnFirstDestinationError")]
    #[builder(default)]
    pub fail_on_first_destination_error: Option<bool>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesLookoutMetrics {
}

/// Properties that are required to query Marketo. See Generic Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesMarketo {
    /// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "errorHandlingConfig")]
    #[builder(default)]
    pub error_handling_config: Option<FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesMarketoErrorHandlingConfig>,
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

/// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesMarketoErrorHandlingConfig {
    /// Amazon S3 bucket name in which Amazon AppFlow places the transferred data.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketName")]
    #[builder(default)]
    pub bucket_name: Option<String>,
    /// Object key for the bucket in which Amazon AppFlow places the destination files.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketPrefix")]
    #[builder(default)]
    pub bucket_prefix: Option<String>,
    /// If the flow should fail after the first instance of a failure when attempting to place data in the destination.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failOnFirstDestinationError")]
    #[builder(default)]
    pub fail_on_first_destination_error: Option<bool>,
}

/// Properties that are required to query Amazon Redshift. See Redshift Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesRedshift {
    /// Object key for the bucket in which Amazon AppFlow places the destination files.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketPrefix")]
    #[builder(default)]
    pub bucket_prefix: Option<String>,
    /// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "errorHandlingConfig")]
    #[builder(default)]
    pub error_handling_config: Option<FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesRedshiftErrorHandlingConfig>,
    /// Intermediate bucket that Amazon AppFlow uses when moving data into Amazon Redshift.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "intermediateBucketName")]
    #[builder(default)]
    pub intermediate_bucket_name: Option<String>,
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

/// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesRedshiftErrorHandlingConfig {
    /// Amazon S3 bucket name in which Amazon AppFlow places the transferred data.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketName")]
    #[builder(default)]
    pub bucket_name: Option<String>,
    /// Object key for the bucket in which Amazon AppFlow places the destination files.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketPrefix")]
    #[builder(default)]
    pub bucket_prefix: Option<String>,
    /// If the flow should fail after the first instance of a failure when attempting to place data in the destination.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failOnFirstDestinationError")]
    #[builder(default)]
    pub fail_on_first_destination_error: Option<bool>,
}

/// Properties that are required to query Amazon S3. See S3 Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesS3 {
    /// Amazon S3 bucket name in which Amazon AppFlow places the transferred data.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketName")]
    #[builder(default)]
    pub bucket_name: Option<String>,
    /// Reference to a BucketPolicy in s3 to populate bucketName.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketNameRef")]
    #[builder(default)]
    pub bucket_name_ref: Option<FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesS3BucketNameRef>,
    /// Selector for a BucketPolicy in s3 to populate bucketName.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketNameSelector")]
    #[builder(default)]
    pub bucket_name_selector: Option<FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesS3BucketNameSelector>,
    /// Object key for the bucket in which Amazon AppFlow places the destination files.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketPrefix")]
    #[builder(default)]
    pub bucket_prefix: Option<String>,
    /// Configuration that determines how Amazon AppFlow should format the flow output data when Amazon S3 is used as the destination. See S3 Output Format Config for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "s3OutputFormatConfig")]
    #[builder(default)]
    pub s3_output_format_config: Option<FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesS3S3OutputFormatConfig>,
}

/// Reference to a BucketPolicy in s3 to populate bucketName.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesS3BucketNameRef {
    /// Name of the referenced object.
    pub name: String,
    /// Policies for referencing.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub policy: Option<FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesS3BucketNameRefPolicy>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesS3BucketNameRefPolicy {
    /// Resolution specifies whether resolution of this reference is required.
    /// The default is 'Required', which means the reconcile will fail if the
    /// reference cannot be resolved. 'Optional' means this reference will be
    /// a no-op if it cannot be resolved.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub resolution: Option<FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesS3BucketNameRefPolicyResolution>,
    /// Resolve specifies when this reference should be resolved. The default
    /// is 'IfNotPresent', which will attempt to resolve the reference only when
    /// the corresponding field is not present. Use 'Always' to resolve the
    /// reference on every reconcile.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub resolve: Option<FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesS3BucketNameRefPolicyResolve>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesS3BucketNameRefPolicyResolution {
    Required,
    Optional,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesS3BucketNameRefPolicyResolve {
    Always,
    IfNotPresent,
}

/// Selector for a BucketPolicy in s3 to populate bucketName.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesS3BucketNameSelector {
    /// MatchControllerRef ensures an object with the same controller reference
    /// as the selecting object is selected.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchControllerRef")]
    #[builder(default)]
    pub match_controller_ref: Option<bool>,
    /// MatchLabels ensures an object with matching labels is selected.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    #[builder(default)]
    pub match_labels: Option<HashMap<String, String>>,
    /// Policies for selection.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub policy: Option<FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesS3BucketNameSelectorPolicy>,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesS3BucketNameSelectorPolicy {
    /// Resolution specifies whether resolution of this reference is required.
    /// The default is 'Required', which means the reconcile will fail if the
    /// reference cannot be resolved. 'Optional' means this reference will be
    /// a no-op if it cannot be resolved.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub resolution: Option<FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesS3BucketNameSelectorPolicyResolution>,
    /// Resolve specifies when this reference should be resolved. The default
    /// is 'IfNotPresent', which will attempt to resolve the reference only when
    /// the corresponding field is not present. Use 'Always' to resolve the
    /// reference on every reconcile.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub resolve: Option<FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesS3BucketNameSelectorPolicyResolve>,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesS3BucketNameSelectorPolicyResolution {
    Required,
    Optional,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesS3BucketNameSelectorPolicyResolve {
    Always,
    IfNotPresent,
}

/// Configuration that determines how Amazon AppFlow should format the flow output data when Amazon S3 is used as the destination. See S3 Output Format Config for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesS3S3OutputFormatConfig {
    /// Aggregation settings that you can use to customize the output format of your flow data. See Aggregation Config for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "aggregationConfig")]
    #[builder(default)]
    pub aggregation_config: Option<FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesS3S3OutputFormatConfigAggregationConfig>,
    /// File type that Amazon AppFlow places in the Amazon S3 bucket. Valid values are CSV, JSON, and PARQUET.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fileType")]
    #[builder(default)]
    pub file_type: Option<String>,
    /// Determines the prefix that Amazon AppFlow applies to the folder name in the Amazon S3 bucket. You can name folders according to the flow frequency and date. See Prefix Config for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "prefixConfig")]
    #[builder(default)]
    pub prefix_config: Option<FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesS3S3OutputFormatConfigPrefixConfig>,
    /// Whether the data types from the source system need to be preserved (Only valid for Parquet file type)
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "preserveSourceDataTyping")]
    #[builder(default)]
    pub preserve_source_data_typing: Option<bool>,
}

/// Aggregation settings that you can use to customize the output format of your flow data. See Aggregation Config for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesS3S3OutputFormatConfigAggregationConfig {
    /// Whether Amazon AppFlow aggregates the flow records into a single file, or leave them unaggregated. Valid values are None and SingleFile.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "aggregationType")]
    #[builder(default)]
    pub aggregation_type: Option<String>,
    /// The desired file size, in MB, for each output file that Amazon AppFlow writes to the flow destination. Integer value.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "targetFileSize")]
    #[builder(default)]
    pub target_file_size: Option<f64>,
}

/// Determines the prefix that Amazon AppFlow applies to the folder name in the Amazon S3 bucket. You can name folders according to the flow frequency and date. See Prefix Config for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesS3S3OutputFormatConfigPrefixConfig {
    /// Determines the level of granularity that's included in the prefix. Valid values are YEAR, MONTH, DAY, HOUR, and MINUTE.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "prefixFormat")]
    #[builder(default)]
    pub prefix_format: Option<String>,
    /// Determines whether the destination file path includes either or both of the selected elements. Valid values are EXECUTION_ID and SCHEMA_VERSION
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "prefixHierarchy")]
    #[builder(default)]
    pub prefix_hierarchy: Option<Vec<String>>,
    /// Determines the format of the prefix, and whether it applies to the file name, file path, or both. Valid values are FILENAME, PATH, and PATH_AND_FILENAME.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "prefixType")]
    #[builder(default)]
    pub prefix_type: Option<String>,
}

/// Properties that are required to query Salesforce. See Salesforce Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesSalesforce {
    /// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "errorHandlingConfig")]
    #[builder(default)]
    pub error_handling_config: Option<FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesSalesforceErrorHandlingConfig>,
    /// Name of the field that Amazon AppFlow uses as an ID when performing a write operation such as update, delete, or upsert.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "idFieldNames")]
    #[builder(default)]
    pub id_field_names: Option<Vec<String>>,
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
    /// Type of write operation to be performed in the custom connector when it's used as destination. Valid values are INSERT, UPSERT, UPDATE, and DELETE.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "writeOperationType")]
    #[builder(default)]
    pub write_operation_type: Option<String>,
}

/// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesSalesforceErrorHandlingConfig {
    /// Amazon S3 bucket name in which Amazon AppFlow places the transferred data.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketName")]
    #[builder(default)]
    pub bucket_name: Option<String>,
    /// Object key for the bucket in which Amazon AppFlow places the destination files.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketPrefix")]
    #[builder(default)]
    pub bucket_prefix: Option<String>,
    /// If the flow should fail after the first instance of a failure when attempting to place data in the destination.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failOnFirstDestinationError")]
    #[builder(default)]
    pub fail_on_first_destination_error: Option<bool>,
}

/// Properties that are required to query SAPOData. See SAPOData Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesSapoData {
    /// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "errorHandlingConfig")]
    #[builder(default)]
    pub error_handling_config: Option<FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesSapoDataErrorHandlingConfig>,
    /// Name of the field that Amazon AppFlow uses as an ID when performing a write operation such as update, delete, or upsert.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "idFieldNames")]
    #[builder(default)]
    pub id_field_names: Option<Vec<String>>,
    /// Object path specified in the SAPOData flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "objectPath")]
    #[builder(default)]
    pub object_path: Option<String>,
    /// Determines how Amazon AppFlow handles the success response that it gets from the connector after placing data. See Success Response Handling Config for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "successResponseHandlingConfig")]
    #[builder(default)]
    pub success_response_handling_config: Option<FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesSapoDataSuccessResponseHandlingConfig>,
    /// Type of write operation to be performed in the custom connector when it's used as destination. Valid values are INSERT, UPSERT, UPDATE, and DELETE.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "writeOperationType")]
    #[builder(default)]
    pub write_operation_type: Option<String>,
}

/// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesSapoDataErrorHandlingConfig {
    /// Amazon S3 bucket name in which Amazon AppFlow places the transferred data.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketName")]
    #[builder(default)]
    pub bucket_name: Option<String>,
    /// Object key for the bucket in which Amazon AppFlow places the destination files.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketPrefix")]
    #[builder(default)]
    pub bucket_prefix: Option<String>,
    /// If the flow should fail after the first instance of a failure when attempting to place data in the destination.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failOnFirstDestinationError")]
    #[builder(default)]
    pub fail_on_first_destination_error: Option<bool>,
}

/// Determines how Amazon AppFlow handles the success response that it gets from the connector after placing data. See Success Response Handling Config for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesSapoDataSuccessResponseHandlingConfig {
    /// Amazon S3 bucket name in which Amazon AppFlow places the transferred data.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketName")]
    #[builder(default)]
    pub bucket_name: Option<String>,
    /// Object key for the bucket in which Amazon AppFlow places the destination files.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketPrefix")]
    #[builder(default)]
    pub bucket_prefix: Option<String>,
}

/// Properties that are required to query Snowflake. See Snowflake Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesSnowflake {
    /// Object key for the bucket in which Amazon AppFlow places the destination files.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketPrefix")]
    #[builder(default)]
    pub bucket_prefix: Option<String>,
    /// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "errorHandlingConfig")]
    #[builder(default)]
    pub error_handling_config: Option<FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesSnowflakeErrorHandlingConfig>,
    /// Intermediate bucket that Amazon AppFlow uses when moving data into Amazon Redshift.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "intermediateBucketName")]
    #[builder(default)]
    pub intermediate_bucket_name: Option<String>,
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

/// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesSnowflakeErrorHandlingConfig {
    /// Amazon S3 bucket name in which Amazon AppFlow places the transferred data.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketName")]
    #[builder(default)]
    pub bucket_name: Option<String>,
    /// Object key for the bucket in which Amazon AppFlow places the destination files.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketPrefix")]
    #[builder(default)]
    pub bucket_prefix: Option<String>,
    /// If the flow should fail after the first instance of a failure when attempting to place data in the destination.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failOnFirstDestinationError")]
    #[builder(default)]
    pub fail_on_first_destination_error: Option<bool>,
}

/// Properties that are required to query Upsolver. See Upsolver Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesUpsolver {
    /// Amazon S3 bucket name in which Amazon AppFlow places the transferred data.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketName")]
    #[builder(default)]
    pub bucket_name: Option<String>,
    /// Object key for the bucket in which Amazon AppFlow places the destination files.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketPrefix")]
    #[builder(default)]
    pub bucket_prefix: Option<String>,
    /// Configuration that determines how Amazon AppFlow should format the flow output data when Amazon S3 is used as the destination. See S3 Output Format Config for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "s3OutputFormatConfig")]
    #[builder(default)]
    pub s3_output_format_config: Option<FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesUpsolverS3OutputFormatConfig>,
}

/// Configuration that determines how Amazon AppFlow should format the flow output data when Amazon S3 is used as the destination. See S3 Output Format Config for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesUpsolverS3OutputFormatConfig {
    /// Aggregation settings that you can use to customize the output format of your flow data. See Aggregation Config for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "aggregationConfig")]
    #[builder(default)]
    pub aggregation_config: Option<FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesUpsolverS3OutputFormatConfigAggregationConfig>,
    /// File type that Amazon AppFlow places in the Amazon S3 bucket. Valid values are CSV, JSON, and PARQUET.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fileType")]
    #[builder(default)]
    pub file_type: Option<String>,
    /// Determines the prefix that Amazon AppFlow applies to the folder name in the Amazon S3 bucket. You can name folders according to the flow frequency and date. See Prefix Config for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "prefixConfig")]
    #[builder(default)]
    pub prefix_config: Option<FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesUpsolverS3OutputFormatConfigPrefixConfig>,
}

/// Aggregation settings that you can use to customize the output format of your flow data. See Aggregation Config for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesUpsolverS3OutputFormatConfigAggregationConfig {
    /// Whether Amazon AppFlow aggregates the flow records into a single file, or leave them unaggregated. Valid values are None and SingleFile.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "aggregationType")]
    #[builder(default)]
    pub aggregation_type: Option<String>,
}

/// Determines the prefix that Amazon AppFlow applies to the folder name in the Amazon S3 bucket. You can name folders according to the flow frequency and date. See Prefix Config for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesUpsolverS3OutputFormatConfigPrefixConfig {
    /// Determines the level of granularity that's included in the prefix. Valid values are YEAR, MONTH, DAY, HOUR, and MINUTE.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "prefixFormat")]
    #[builder(default)]
    pub prefix_format: Option<String>,
    /// Determines whether the destination file path includes either or both of the selected elements. Valid values are EXECUTION_ID and SCHEMA_VERSION
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "prefixHierarchy")]
    #[builder(default)]
    pub prefix_hierarchy: Option<Vec<String>>,
    /// Determines the format of the prefix, and whether it applies to the file name, file path, or both. Valid values are FILENAME, PATH, and PATH_AND_FILENAME.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "prefixType")]
    #[builder(default)]
    pub prefix_type: Option<String>,
}

/// Properties that are required to query Zendesk. See Zendesk Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesZendesk {
    /// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "errorHandlingConfig")]
    #[builder(default)]
    pub error_handling_config: Option<FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesZendeskErrorHandlingConfig>,
    /// Name of the field that Amazon AppFlow uses as an ID when performing a write operation such as update, delete, or upsert.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "idFieldNames")]
    #[builder(default)]
    pub id_field_names: Option<Vec<String>>,
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
    /// Type of write operation to be performed in the custom connector when it's used as destination. Valid values are INSERT, UPSERT, UPDATE, and DELETE.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "writeOperationType")]
    #[builder(default)]
    pub write_operation_type: Option<String>,
}

/// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderDestinationFlowConfigDestinationConnectorPropertiesZendeskErrorHandlingConfig {
    /// Amazon S3 bucket name in which Amazon AppFlow places the transferred data.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketName")]
    #[builder(default)]
    pub bucket_name: Option<String>,
    /// Object key for the bucket in which Amazon AppFlow places the destination files.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketPrefix")]
    #[builder(default)]
    pub bucket_prefix: Option<String>,
    /// If the flow should fail after the first instance of a failure when attempting to place data in the destination.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failOnFirstDestinationError")]
    #[builder(default)]
    pub fail_on_first_destination_error: Option<bool>,
}

/// A Catalog that determines the configuration that Amazon AppFlow uses when it catalogs the data thatâ€™s transferred by the associated flow. When Amazon AppFlow catalogs the data from a flow, it stores metadata in a data catalog.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderMetadataCatalogConfig {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "glueDataCatalog")]
    #[builder(default)]
    pub glue_data_catalog: Option<FlowInitProviderMetadataCatalogConfigGlueDataCatalog>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderMetadataCatalogConfigGlueDataCatalog {
    /// The name of an existing Glue database to store the metadata tables that Amazon AppFlow creates.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "databaseName")]
    #[builder(default)]
    pub database_name: Option<String>,
    /// The ARN of an IAM role that grants AppFlow the permissions it needs to create Data Catalog tables, databases, and partitions.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "roleArn")]
    #[builder(default)]
    pub role_arn: Option<String>,
    /// A naming prefix for each Data Catalog table that Amazon AppFlow creates
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tablePrefix")]
    #[builder(default)]
    pub table_prefix: Option<String>,
}

/// The Source Flow Config that controls how Amazon AppFlow retrieves data from the source connector.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderSourceFlowConfig {
    /// API version that the destination connector uses.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "apiVersion")]
    #[builder(default)]
    pub api_version: Option<String>,
    /// Name of the connector profile. This name must be unique for each connector profile in the AWS account.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "connectorProfileName")]
    #[builder(default)]
    pub connector_profile_name: Option<String>,
    /// Type of connector, such as Salesforce, Amplitude, and so on. Valid values are Salesforce, Singular, Slack, Redshift, S3, Marketo, Googleanalytics, Zendesk, Servicenow, Datadog, Trendmicro, Snowflake, Dynatrace, Infornexus, Amplitude, Veeva, EventBridge, LookoutMetrics, Upsolver, Honeycode, CustomerProfiles, SAPOData, and CustomConnector.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "connectorType")]
    #[builder(default)]
    pub connector_type: Option<String>,
    /// Defines the configuration for a scheduled incremental data pull. If a valid configuration is provided, the fields specified in the configuration are used when querying for the incremental data pull. See Incremental Pull Config for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "incrementalPullConfig")]
    #[builder(default)]
    pub incremental_pull_config: Option<FlowInitProviderSourceFlowConfigIncrementalPullConfig>,
    /// Information that is required to query a particular source connector. See Source Connector Properties for details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sourceConnectorProperties")]
    #[builder(default)]
    pub source_connector_properties: Option<FlowInitProviderSourceFlowConfigSourceConnectorProperties>,
}

/// Defines the configuration for a scheduled incremental data pull. If a valid configuration is provided, the fields specified in the configuration are used when querying for the incremental data pull. See Incremental Pull Config for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderSourceFlowConfigIncrementalPullConfig {
    /// Field that specifies the date time or timestamp field as the criteria to use when importing incremental records from the source.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "datetimeTypeFieldName")]
    #[builder(default)]
    pub datetime_type_field_name: Option<String>,
}

/// Information that is required to query a particular source connector. See Source Connector Properties for details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderSourceFlowConfigSourceConnectorProperties {
    /// Information that is required for querying Amplitude. See Generic Source Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub amplitude: Option<FlowInitProviderSourceFlowConfigSourceConnectorPropertiesAmplitude>,
    /// Properties that are required to query the custom Connector. See Custom Connector Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "customConnector")]
    #[builder(default)]
    pub custom_connector: Option<FlowInitProviderSourceFlowConfigSourceConnectorPropertiesCustomConnector>,
    /// Information that is required for querying Datadog. See Generic Source Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub datadog: Option<FlowInitProviderSourceFlowConfigSourceConnectorPropertiesDatadog>,
    /// Operation to be performed on the provided Dynatrace source fields. Valid values are PROJECTION, BETWEEN, EQUAL_TO, ADDITION, MULTIPLICATION, DIVISION, SUBTRACTION, MASK_ALL, MASK_FIRST_N, MASK_LAST_N, VALIDATE_NON_NULL, VALIDATE_NON_ZERO, VALIDATE_NON_NEGATIVE, VALIDATE_NUMERIC, and NO_OP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub dynatrace: Option<FlowInitProviderSourceFlowConfigSourceConnectorPropertiesDynatrace>,
    /// Operation to be performed on the provided Google Analytics source fields. Valid values are PROJECTION and BETWEEN.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "googleAnalytics")]
    #[builder(default)]
    pub google_analytics: Option<FlowInitProviderSourceFlowConfigSourceConnectorPropertiesGoogleAnalytics>,
    /// Information that is required for querying Infor Nexus. See Generic Source Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "inforNexus")]
    #[builder(default)]
    pub infor_nexus: Option<FlowInitProviderSourceFlowConfigSourceConnectorPropertiesInforNexus>,
    /// Properties that are required to query Marketo. See Generic Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub marketo: Option<FlowInitProviderSourceFlowConfigSourceConnectorPropertiesMarketo>,
    /// Properties that are required to query Amazon S3. See S3 Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub s3: Option<FlowInitProviderSourceFlowConfigSourceConnectorPropertiesS3>,
    /// Properties that are required to query Salesforce. See Salesforce Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub salesforce: Option<FlowInitProviderSourceFlowConfigSourceConnectorPropertiesSalesforce>,
    /// Properties that are required to query SAPOData. See SAPOData Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sapoData")]
    #[builder(default)]
    pub sapo_data: Option<FlowInitProviderSourceFlowConfigSourceConnectorPropertiesSapoData>,
    /// Information that is required for querying ServiceNow. See Generic Source Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceNow")]
    #[builder(default)]
    pub service_now: Option<FlowInitProviderSourceFlowConfigSourceConnectorPropertiesServiceNow>,
    /// Information that is required for querying Singular. See Generic Source Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub singular: Option<FlowInitProviderSourceFlowConfigSourceConnectorPropertiesSingular>,
    /// Information that is required for querying Slack. See Generic Source Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub slack: Option<FlowInitProviderSourceFlowConfigSourceConnectorPropertiesSlack>,
    /// Operation to be performed on the provided Trend Micro source fields. Valid values are PROJECTION, EQUAL_TO, ADDITION, MULTIPLICATION, DIVISION, SUBTRACTION, MASK_ALL, MASK_FIRST_N, MASK_LAST_N, VALIDATE_NON_NULL, VALIDATE_NON_ZERO, VALIDATE_NON_NEGATIVE, VALIDATE_NUMERIC, and NO_OP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub trendmicro: Option<FlowInitProviderSourceFlowConfigSourceConnectorPropertiesTrendmicro>,
    /// Information that is required for querying Veeva. See Veeva Source Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub veeva: Option<FlowInitProviderSourceFlowConfigSourceConnectorPropertiesVeeva>,
    /// Properties that are required to query Zendesk. See Zendesk Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub zendesk: Option<FlowInitProviderSourceFlowConfigSourceConnectorPropertiesZendesk>,
}

/// Information that is required for querying Amplitude. See Generic Source Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderSourceFlowConfigSourceConnectorPropertiesAmplitude {
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

/// Properties that are required to query the custom Connector. See Custom Connector Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderSourceFlowConfigSourceConnectorPropertiesCustomConnector {
    /// Custom properties that are specific to the connector when it's used as a destination in the flow. Maximum of 50 items.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "customProperties")]
    #[builder(default)]
    pub custom_properties: Option<HashMap<String, String>>,
    /// Entity specified in the custom connector as a destination in the flow.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "entityName")]
    #[builder(default)]
    pub entity_name: Option<String>,
}

/// Information that is required for querying Datadog. See Generic Source Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderSourceFlowConfigSourceConnectorPropertiesDatadog {
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

/// Operation to be performed on the provided Dynatrace source fields. Valid values are PROJECTION, BETWEEN, EQUAL_TO, ADDITION, MULTIPLICATION, DIVISION, SUBTRACTION, MASK_ALL, MASK_FIRST_N, MASK_LAST_N, VALIDATE_NON_NULL, VALIDATE_NON_ZERO, VALIDATE_NON_NEGATIVE, VALIDATE_NUMERIC, and NO_OP.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderSourceFlowConfigSourceConnectorPropertiesDynatrace {
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

/// Operation to be performed on the provided Google Analytics source fields. Valid values are PROJECTION and BETWEEN.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderSourceFlowConfigSourceConnectorPropertiesGoogleAnalytics {
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

/// Information that is required for querying Infor Nexus. See Generic Source Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderSourceFlowConfigSourceConnectorPropertiesInforNexus {
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

/// Properties that are required to query Marketo. See Generic Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderSourceFlowConfigSourceConnectorPropertiesMarketo {
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

/// Properties that are required to query Amazon S3. See S3 Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderSourceFlowConfigSourceConnectorPropertiesS3 {
    /// Amazon S3 bucket name in which Amazon AppFlow places the transferred data.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketName")]
    #[builder(default)]
    pub bucket_name: Option<String>,
    /// Reference to a BucketPolicy in s3 to populate bucketName.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketNameRef")]
    #[builder(default)]
    pub bucket_name_ref: Option<FlowInitProviderSourceFlowConfigSourceConnectorPropertiesS3BucketNameRef>,
    /// Selector for a BucketPolicy in s3 to populate bucketName.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketNameSelector")]
    #[builder(default)]
    pub bucket_name_selector: Option<FlowInitProviderSourceFlowConfigSourceConnectorPropertiesS3BucketNameSelector>,
    /// Object key for the bucket in which Amazon AppFlow places the destination files.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketPrefix")]
    #[builder(default)]
    pub bucket_prefix: Option<String>,
    /// When you use Amazon S3 as the source, the configuration format that you provide the flow input data. See S3 Input Format Config for details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "s3InputFormatConfig")]
    #[builder(default)]
    pub s3_input_format_config: Option<FlowInitProviderSourceFlowConfigSourceConnectorPropertiesS3S3InputFormatConfig>,
}

/// Reference to a BucketPolicy in s3 to populate bucketName.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderSourceFlowConfigSourceConnectorPropertiesS3BucketNameRef {
    /// Name of the referenced object.
    pub name: String,
    /// Policies for referencing.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub policy: Option<FlowInitProviderSourceFlowConfigSourceConnectorPropertiesS3BucketNameRefPolicy>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderSourceFlowConfigSourceConnectorPropertiesS3BucketNameRefPolicy {
    /// Resolution specifies whether resolution of this reference is required.
    /// The default is 'Required', which means the reconcile will fail if the
    /// reference cannot be resolved. 'Optional' means this reference will be
    /// a no-op if it cannot be resolved.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub resolution: Option<FlowInitProviderSourceFlowConfigSourceConnectorPropertiesS3BucketNameRefPolicyResolution>,
    /// Resolve specifies when this reference should be resolved. The default
    /// is 'IfNotPresent', which will attempt to resolve the reference only when
    /// the corresponding field is not present. Use 'Always' to resolve the
    /// reference on every reconcile.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub resolve: Option<FlowInitProviderSourceFlowConfigSourceConnectorPropertiesS3BucketNameRefPolicyResolve>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum FlowInitProviderSourceFlowConfigSourceConnectorPropertiesS3BucketNameRefPolicyResolution {
    Required,
    Optional,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum FlowInitProviderSourceFlowConfigSourceConnectorPropertiesS3BucketNameRefPolicyResolve {
    Always,
    IfNotPresent,
}

/// Selector for a BucketPolicy in s3 to populate bucketName.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderSourceFlowConfigSourceConnectorPropertiesS3BucketNameSelector {
    /// MatchControllerRef ensures an object with the same controller reference
    /// as the selecting object is selected.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchControllerRef")]
    #[builder(default)]
    pub match_controller_ref: Option<bool>,
    /// MatchLabels ensures an object with matching labels is selected.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    #[builder(default)]
    pub match_labels: Option<HashMap<String, String>>,
    /// Policies for selection.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub policy: Option<FlowInitProviderSourceFlowConfigSourceConnectorPropertiesS3BucketNameSelectorPolicy>,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderSourceFlowConfigSourceConnectorPropertiesS3BucketNameSelectorPolicy {
    /// Resolution specifies whether resolution of this reference is required.
    /// The default is 'Required', which means the reconcile will fail if the
    /// reference cannot be resolved. 'Optional' means this reference will be
    /// a no-op if it cannot be resolved.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub resolution: Option<FlowInitProviderSourceFlowConfigSourceConnectorPropertiesS3BucketNameSelectorPolicyResolution>,
    /// Resolve specifies when this reference should be resolved. The default
    /// is 'IfNotPresent', which will attempt to resolve the reference only when
    /// the corresponding field is not present. Use 'Always' to resolve the
    /// reference on every reconcile.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub resolve: Option<FlowInitProviderSourceFlowConfigSourceConnectorPropertiesS3BucketNameSelectorPolicyResolve>,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum FlowInitProviderSourceFlowConfigSourceConnectorPropertiesS3BucketNameSelectorPolicyResolution {
    Required,
    Optional,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum FlowInitProviderSourceFlowConfigSourceConnectorPropertiesS3BucketNameSelectorPolicyResolve {
    Always,
    IfNotPresent,
}

/// When you use Amazon S3 as the source, the configuration format that you provide the flow input data. See S3 Input Format Config for details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderSourceFlowConfigSourceConnectorPropertiesS3S3InputFormatConfig {
    /// File type that Amazon AppFlow gets from your Amazon S3 bucket. Valid values are CSV and JSON.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "s3InputFileType")]
    #[builder(default)]
    pub s3_input_file_type: Option<String>,
}

/// Properties that are required to query Salesforce. See Salesforce Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderSourceFlowConfigSourceConnectorPropertiesSalesforce {
    /// Flag that enables dynamic fetching of new (recently added) fields in the Salesforce objects while running a flow.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableDynamicFieldUpdate")]
    #[builder(default)]
    pub enable_dynamic_field_update: Option<bool>,
    /// Whether Amazon AppFlow includes deleted files in the flow run.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "includeDeletedRecords")]
    #[builder(default)]
    pub include_deleted_records: Option<bool>,
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

/// Properties that are required to query SAPOData. See SAPOData Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderSourceFlowConfigSourceConnectorPropertiesSapoData {
    /// Object path specified in the SAPOData flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "objectPath")]
    #[builder(default)]
    pub object_path: Option<String>,
    /// Sets the page size for each concurrent process that transfers OData records from your SAP instance.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "paginationConfig")]
    #[builder(default)]
    pub pagination_config: Option<FlowInitProviderSourceFlowConfigSourceConnectorPropertiesSapoDataPaginationConfig>,
    /// Sets the number of concurrent processes that transfers OData records from your SAP instance.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "parallelismConfig")]
    #[builder(default)]
    pub parallelism_config: Option<FlowInitProviderSourceFlowConfigSourceConnectorPropertiesSapoDataParallelismConfig>,
}

/// Sets the page size for each concurrent process that transfers OData records from your SAP instance.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderSourceFlowConfigSourceConnectorPropertiesSapoDataPaginationConfig {
    /// he maximum number of records that Amazon AppFlow receives in each page of the response from your SAP application.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxPageSize")]
    #[builder(default)]
    pub max_page_size: Option<f64>,
}

/// Sets the number of concurrent processes that transfers OData records from your SAP instance.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderSourceFlowConfigSourceConnectorPropertiesSapoDataParallelismConfig {
    /// he maximum number of records that Amazon AppFlow receives in each page of the response from your SAP application.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxPageSize")]
    #[builder(default)]
    pub max_page_size: Option<f64>,
}

/// Information that is required for querying ServiceNow. See Generic Source Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderSourceFlowConfigSourceConnectorPropertiesServiceNow {
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

/// Information that is required for querying Singular. See Generic Source Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderSourceFlowConfigSourceConnectorPropertiesSingular {
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

/// Information that is required for querying Slack. See Generic Source Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderSourceFlowConfigSourceConnectorPropertiesSlack {
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

/// Operation to be performed on the provided Trend Micro source fields. Valid values are PROJECTION, EQUAL_TO, ADDITION, MULTIPLICATION, DIVISION, SUBTRACTION, MASK_ALL, MASK_FIRST_N, MASK_LAST_N, VALIDATE_NON_NULL, VALIDATE_NON_ZERO, VALIDATE_NON_NEGATIVE, VALIDATE_NUMERIC, and NO_OP.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderSourceFlowConfigSourceConnectorPropertiesTrendmicro {
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

/// Information that is required for querying Veeva. See Veeva Source Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderSourceFlowConfigSourceConnectorPropertiesVeeva {
    /// Document type specified in the Veeva document extract flow.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "documentType")]
    #[builder(default)]
    pub document_type: Option<String>,
    /// Boolean value to include All Versions of files in Veeva document extract flow.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "includeAllVersions")]
    #[builder(default)]
    pub include_all_versions: Option<bool>,
    /// Boolean value to include file renditions in Veeva document extract flow.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "includeRenditions")]
    #[builder(default)]
    pub include_renditions: Option<bool>,
    /// Boolean value to include source files in Veeva document extract flow.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "includeSourceFiles")]
    #[builder(default)]
    pub include_source_files: Option<bool>,
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

/// Properties that are required to query Zendesk. See Zendesk Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderSourceFlowConfigSourceConnectorPropertiesZendesk {
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderTask {
    /// Operation to be performed on the provided source fields. See Connector Operator for details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "connectorOperator")]
    #[builder(default)]
    pub connector_operator: Option<Vec<FlowInitProviderTaskConnectorOperator>>,
    /// Field in a destination connector, or a field value against which Amazon AppFlow validates a source field.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "destinationField")]
    #[builder(default)]
    pub destination_field: Option<String>,
    /// Source fields to which a particular task is applied.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sourceFields")]
    #[builder(default)]
    pub source_fields: Option<Vec<String>>,
    /// Map used to store task-related information. The execution service looks for particular information based on the TaskType. Valid keys are VALUE, VALUES, DATA_TYPE, UPPER_BOUND, LOWER_BOUND, SOURCE_DATA_TYPE, DESTINATION_DATA_TYPE, VALIDATION_ACTION, MASK_VALUE, MASK_LENGTH, TRUNCATE_LENGTH, MATH_OPERATION_FIELDS_ORDER, CONCAT_FORMAT, SUBFIELD_CATEGORY_MAP, and EXCLUDE_SOURCE_FIELDS_LIST.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "taskProperties")]
    #[builder(default)]
    pub task_properties: Option<HashMap<String, String>>,
    /// Particular task implementation that Amazon AppFlow performs. Valid values are Arithmetic, Filter, Map, Map_all, Mask, Merge, Passthrough, Truncate, and Validate.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "taskType")]
    #[builder(default)]
    pub task_type: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderTaskConnectorOperator {
    /// Information that is required for querying Amplitude. See Generic Source Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub amplitude: Option<String>,
    /// Properties that are required to query the custom Connector. See Custom Connector Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "customConnector")]
    #[builder(default)]
    pub custom_connector: Option<String>,
    /// Information that is required for querying Datadog. See Generic Source Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub datadog: Option<String>,
    /// Operation to be performed on the provided Dynatrace source fields. Valid values are PROJECTION, BETWEEN, EQUAL_TO, ADDITION, MULTIPLICATION, DIVISION, SUBTRACTION, MASK_ALL, MASK_FIRST_N, MASK_LAST_N, VALIDATE_NON_NULL, VALIDATE_NON_ZERO, VALIDATE_NON_NEGATIVE, VALIDATE_NUMERIC, and NO_OP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub dynatrace: Option<String>,
    /// Operation to be performed on the provided Google Analytics source fields. Valid values are PROJECTION and BETWEEN.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "googleAnalytics")]
    #[builder(default)]
    pub google_analytics: Option<String>,
    /// Information that is required for querying Infor Nexus. See Generic Source Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "inforNexus")]
    #[builder(default)]
    pub infor_nexus: Option<String>,
    /// Properties that are required to query Marketo. See Generic Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub marketo: Option<String>,
    /// Properties that are required to query Amazon S3. See S3 Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub s3: Option<String>,
    /// Properties that are required to query Salesforce. See Salesforce Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub salesforce: Option<String>,
    /// Properties that are required to query SAPOData. See SAPOData Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sapoData")]
    #[builder(default)]
    pub sapo_data: Option<String>,
    /// Information that is required for querying ServiceNow. See Generic Source Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceNow")]
    #[builder(default)]
    pub service_now: Option<String>,
    /// Information that is required for querying Singular. See Generic Source Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub singular: Option<String>,
    /// Information that is required for querying Slack. See Generic Source Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub slack: Option<String>,
    /// Operation to be performed on the provided Trend Micro source fields. Valid values are PROJECTION, EQUAL_TO, ADDITION, MULTIPLICATION, DIVISION, SUBTRACTION, MASK_ALL, MASK_FIRST_N, MASK_LAST_N, VALIDATE_NON_NULL, VALIDATE_NON_ZERO, VALIDATE_NON_NEGATIVE, VALIDATE_NUMERIC, and NO_OP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub trendmicro: Option<String>,
    /// Information that is required for querying Veeva. See Veeva Source Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub veeva: Option<String>,
    /// Properties that are required to query Zendesk. See Zendesk Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub zendesk: Option<String>,
}

/// A Trigger that determine how and when the flow runs.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderTriggerConfig {
    /// Configuration details of a schedule-triggered flow as defined by the user. Currently, these settings only apply to the Scheduled trigger type. See Scheduled Trigger Properties for details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "triggerProperties")]
    #[builder(default)]
    pub trigger_properties: Option<FlowInitProviderTriggerConfigTriggerProperties>,
    /// Type of flow trigger. Valid values are Scheduled, Event, and OnDemand.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "triggerType")]
    #[builder(default)]
    pub trigger_type: Option<String>,
}

/// Configuration details of a schedule-triggered flow as defined by the user. Currently, these settings only apply to the Scheduled trigger type. See Scheduled Trigger Properties for details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderTriggerConfigTriggerProperties {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub scheduled: Option<FlowInitProviderTriggerConfigTriggerPropertiesScheduled>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowInitProviderTriggerConfigTriggerPropertiesScheduled {
    /// Whether a scheduled flow has an incremental data transfer or a complete data transfer for each flow run. Valid values are Incremental and Complete.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dataPullMode")]
    #[builder(default)]
    pub data_pull_mode: Option<String>,
    /// Date range for the records to import from the connector in the first flow run. Must be a valid RFC3339 timestamp.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "firstExecutionFrom")]
    #[builder(default)]
    pub first_execution_from: Option<String>,
    /// Scheduled end time for a schedule-triggered flow. Must be a valid RFC3339 timestamp.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "scheduleEndTime")]
    #[builder(default)]
    pub schedule_end_time: Option<String>,
    /// Scheduling expression that determines the rate at which the schedule will run, for example rate(5minutes).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "scheduleExpression")]
    #[builder(default)]
    pub schedule_expression: Option<String>,
    /// Optional offset that is added to the time interval for a schedule-triggered flow. Maximum value of 36000.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "scheduleOffset")]
    #[builder(default)]
    pub schedule_offset: Option<f64>,
    /// Scheduled start time for a schedule-triggered flow. Must be a valid RFC3339 timestamp.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "scheduleStartTime")]
    #[builder(default)]
    pub schedule_start_time: Option<String>,
    /// Time zone used when referring to the date and time of a scheduled-triggered flow, such as America/New_York.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub timezone: Option<String>,
}

/// ProviderConfigReference specifies how the provider that will be used to
/// create, observe, update, and delete this managed resource should be
/// configured.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowProviderConfigRef {
    /// Name of the referenced object.
    pub name: String,
    /// Policies for referencing.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub policy: Option<FlowProviderConfigRefPolicy>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowProviderConfigRefPolicy {
    /// Resolution specifies whether resolution of this reference is required.
    /// The default is 'Required', which means the reconcile will fail if the
    /// reference cannot be resolved. 'Optional' means this reference will be
    /// a no-op if it cannot be resolved.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub resolution: Option<FlowProviderConfigRefPolicyResolution>,
    /// Resolve specifies when this reference should be resolved. The default
    /// is 'IfNotPresent', which will attempt to resolve the reference only when
    /// the corresponding field is not present. Use 'Always' to resolve the
    /// reference on every reconcile.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub resolve: Option<FlowProviderConfigRefPolicyResolve>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum FlowProviderConfigRefPolicyResolution {
    Required,
    Optional,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum FlowProviderConfigRefPolicyResolve {
    Always,
    IfNotPresent,
}

/// PublishConnectionDetailsTo specifies the connection secret config which
/// contains a name, metadata and a reference to secret store config to
/// which any connection details for this managed resource should be written.
/// Connection details frequently include the endpoint, username,
/// and password required to connect to the managed resource.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowPublishConnectionDetailsTo {
    /// SecretStoreConfigRef specifies which secret store config should be used
    /// for this ConnectionSecret.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configRef")]
    #[builder(default)]
    pub config_ref: Option<FlowPublishConnectionDetailsToConfigRef>,
    /// Metadata is the metadata for connection secret.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub metadata: Option<FlowPublishConnectionDetailsToMetadata>,
    /// Name is the name of the connection secret.
    pub name: String,
}

/// SecretStoreConfigRef specifies which secret store config should be used
/// for this ConnectionSecret.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowPublishConnectionDetailsToConfigRef {
    /// Name of the referenced object.
    pub name: String,
    /// Policies for referencing.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub policy: Option<FlowPublishConnectionDetailsToConfigRefPolicy>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowPublishConnectionDetailsToConfigRefPolicy {
    /// Resolution specifies whether resolution of this reference is required.
    /// The default is 'Required', which means the reconcile will fail if the
    /// reference cannot be resolved. 'Optional' means this reference will be
    /// a no-op if it cannot be resolved.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub resolution: Option<FlowPublishConnectionDetailsToConfigRefPolicyResolution>,
    /// Resolve specifies when this reference should be resolved. The default
    /// is 'IfNotPresent', which will attempt to resolve the reference only when
    /// the corresponding field is not present. Use 'Always' to resolve the
    /// reference on every reconcile.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub resolve: Option<FlowPublishConnectionDetailsToConfigRefPolicyResolve>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum FlowPublishConnectionDetailsToConfigRefPolicyResolution {
    Required,
    Optional,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum FlowPublishConnectionDetailsToConfigRefPolicyResolve {
    Always,
    IfNotPresent,
}

/// Metadata is the metadata for connection secret.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowPublishConnectionDetailsToMetadata {
    /// Annotations are the annotations to be added to connection secret.
    /// - For Kubernetes secrets, this will be used as "metadata.annotations".
    /// - It is up to Secret Store implementation for others store types.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub annotations: Option<HashMap<String, String>>,
    /// Labels are the labels/tags to be added to connection secret.
    /// - For Kubernetes secrets, this will be used as "metadata.labels".
    /// - It is up to Secret Store implementation for others store types.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub labels: Option<HashMap<String, String>>,
    /// Type is the SecretType for the connection secret.
    /// - Only valid for Kubernetes Secret Stores.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    #[builder(default)]
    pub r#type: Option<String>,
}

/// WriteConnectionSecretToReference specifies the namespace and name of a
/// Secret to which any connection details for this managed resource should
/// be written. Connection details frequently include the endpoint, username,
/// and password required to connect to the managed resource.
/// This field is planned to be replaced in a future release in favor of
/// PublishConnectionDetailsTo. Currently, both could be set independently
/// and connection details would be published to both without affecting
/// each other.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowWriteConnectionSecretToRef {
    /// Name of the secret.
    pub name: String,
    /// Namespace of the secret.
    pub namespace: String,
}

/// FlowStatus defines the observed state of Flow.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatus {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "atProvider")]
    #[builder(default)]
    pub at_provider: Option<FlowStatusAtProvider>,
    /// Conditions of the resource.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub conditions: Option<Vec<Condition>>,
    /// ObservedGeneration is the latest metadata.generation
    /// which resulted in either a ready state, or stalled due to error
    /// it can not recover from without human intervention.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "observedGeneration")]
    #[builder(default)]
    pub observed_generation: Option<i64>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProvider {
    /// Flow's ARN.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub arn: Option<String>,
    /// Description of the flow you want to create.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub description: Option<String>,
    /// A Destination Flow Config that controls how Amazon AppFlow places data in the destination connector.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "destinationFlowConfig")]
    #[builder(default)]
    pub destination_flow_config: Option<Vec<FlowStatusAtProviderDestinationFlowConfig>>,
    /// The current status of the flow.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "flowStatus")]
    #[builder(default)]
    pub flow_status: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub id: Option<String>,
    /// ARN (Amazon Resource Name) of the Key Management Service (KMS) key you provide for encryption. This is required if you do not want to use the Amazon AppFlow-managed KMS key. If you don't provide anything here, Amazon AppFlow uses the Amazon AppFlow-managed KMS key.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "kmsArn")]
    #[builder(default)]
    pub kms_arn: Option<String>,
    /// A Catalog that determines the configuration that Amazon AppFlow uses when it catalogs the data thatâ€™s transferred by the associated flow. When Amazon AppFlow catalogs the data from a flow, it stores metadata in a data catalog.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "metadataCatalogConfig")]
    #[builder(default)]
    pub metadata_catalog_config: Option<FlowStatusAtProviderMetadataCatalogConfig>,
    /// The Source Flow Config that controls how Amazon AppFlow retrieves data from the source connector.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sourceFlowConfig")]
    #[builder(default)]
    pub source_flow_config: Option<FlowStatusAtProviderSourceFlowConfig>,
    /// Key-value map of resource tags.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub tags: Option<HashMap<String, String>>,
    /// Map of tags assigned to the resource, including those inherited from the provider default_tags configuration block.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tagsAll")]
    #[builder(default)]
    pub tags_all: Option<HashMap<String, String>>,
    /// A Task that Amazon AppFlow performs while transferring the data in the flow run.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub task: Option<Vec<FlowStatusAtProviderTask>>,
    /// A Trigger that determine how and when the flow runs.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "triggerConfig")]
    #[builder(default)]
    pub trigger_config: Option<FlowStatusAtProviderTriggerConfig>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderDestinationFlowConfig {
    /// API version that the destination connector uses.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "apiVersion")]
    #[builder(default)]
    pub api_version: Option<String>,
    /// Name of the connector profile. This name must be unique for each connector profile in the AWS account.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "connectorProfileName")]
    #[builder(default)]
    pub connector_profile_name: Option<String>,
    /// Type of connector, such as Salesforce, Amplitude, and so on. Valid values are Salesforce, Singular, Slack, Redshift, S3, Marketo, Googleanalytics, Zendesk, Servicenow, Datadog, Trendmicro, Snowflake, Dynatrace, Infornexus, Amplitude, Veeva, EventBridge, LookoutMetrics, Upsolver, Honeycode, CustomerProfiles, SAPOData, and CustomConnector.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "connectorType")]
    #[builder(default)]
    pub connector_type: Option<String>,
    /// This stores the information that is required to query a particular connector. See Destination Connector Properties for more information.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "destinationConnectorProperties")]
    #[builder(default)]
    pub destination_connector_properties: Option<FlowStatusAtProviderDestinationFlowConfigDestinationConnectorProperties>,
}

/// This stores the information that is required to query a particular connector. See Destination Connector Properties for more information.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderDestinationFlowConfigDestinationConnectorProperties {
    /// Properties that are required to query the custom Connector. See Custom Connector Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "customConnector")]
    #[builder(default)]
    pub custom_connector: Option<FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesCustomConnector>,
    /// Properties that are required to query Amazon Connect Customer Profiles. See Customer Profiles Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "customerProfiles")]
    #[builder(default)]
    pub customer_profiles: Option<FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesCustomerProfiles>,
    /// Properties that are required to query Amazon EventBridge. See Generic Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "eventBridge")]
    #[builder(default)]
    pub event_bridge: Option<FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesEventBridge>,
    /// Properties that are required to query Amazon Honeycode. See Generic Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub honeycode: Option<FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesHoneycode>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "lookoutMetrics")]
    #[builder(default)]
    pub lookout_metrics: Option<FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesLookoutMetrics>,
    /// Properties that are required to query Marketo. See Generic Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub marketo: Option<FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesMarketo>,
    /// Properties that are required to query Amazon Redshift. See Redshift Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub redshift: Option<FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesRedshift>,
    /// Properties that are required to query Amazon S3. See S3 Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub s3: Option<FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesS3>,
    /// Properties that are required to query Salesforce. See Salesforce Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub salesforce: Option<FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesSalesforce>,
    /// Properties that are required to query SAPOData. See SAPOData Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sapoData")]
    #[builder(default)]
    pub sapo_data: Option<FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesSapoData>,
    /// Properties that are required to query Snowflake. See Snowflake Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub snowflake: Option<FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesSnowflake>,
    /// Properties that are required to query Upsolver. See Upsolver Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub upsolver: Option<FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesUpsolver>,
    /// Properties that are required to query Zendesk. See Zendesk Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub zendesk: Option<FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesZendesk>,
}

/// Properties that are required to query the custom Connector. See Custom Connector Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesCustomConnector {
    /// Custom properties that are specific to the connector when it's used as a destination in the flow. Maximum of 50 items.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "customProperties")]
    #[builder(default)]
    pub custom_properties: Option<HashMap<String, String>>,
    /// Entity specified in the custom connector as a destination in the flow.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "entityName")]
    #[builder(default)]
    pub entity_name: Option<String>,
    /// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "errorHandlingConfig")]
    #[builder(default)]
    pub error_handling_config: Option<FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesCustomConnectorErrorHandlingConfig>,
    /// Name of the field that Amazon AppFlow uses as an ID when performing a write operation such as update, delete, or upsert.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "idFieldNames")]
    #[builder(default)]
    pub id_field_names: Option<Vec<String>>,
    /// Type of write operation to be performed in the custom connector when it's used as destination. Valid values are INSERT, UPSERT, UPDATE, and DELETE.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "writeOperationType")]
    #[builder(default)]
    pub write_operation_type: Option<String>,
}

/// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesCustomConnectorErrorHandlingConfig {
    /// Amazon S3 bucket name in which Amazon AppFlow places the transferred data.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketName")]
    #[builder(default)]
    pub bucket_name: Option<String>,
    /// Object key for the bucket in which Amazon AppFlow places the destination files.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketPrefix")]
    #[builder(default)]
    pub bucket_prefix: Option<String>,
    /// If the flow should fail after the first instance of a failure when attempting to place data in the destination.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failOnFirstDestinationError")]
    #[builder(default)]
    pub fail_on_first_destination_error: Option<bool>,
}

/// Properties that are required to query Amazon Connect Customer Profiles. See Customer Profiles Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesCustomerProfiles {
    /// Unique name of the Amazon Connect Customer Profiles domain.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "domainName")]
    #[builder(default)]
    pub domain_name: Option<String>,
    /// Object specified in the Amazon Connect Customer Profiles flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "objectTypeName")]
    #[builder(default)]
    pub object_type_name: Option<String>,
}

/// Properties that are required to query Amazon EventBridge. See Generic Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesEventBridge {
    /// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "errorHandlingConfig")]
    #[builder(default)]
    pub error_handling_config: Option<FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesEventBridgeErrorHandlingConfig>,
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

/// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesEventBridgeErrorHandlingConfig {
    /// Amazon S3 bucket name in which Amazon AppFlow places the transferred data.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketName")]
    #[builder(default)]
    pub bucket_name: Option<String>,
    /// Object key for the bucket in which Amazon AppFlow places the destination files.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketPrefix")]
    #[builder(default)]
    pub bucket_prefix: Option<String>,
    /// If the flow should fail after the first instance of a failure when attempting to place data in the destination.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failOnFirstDestinationError")]
    #[builder(default)]
    pub fail_on_first_destination_error: Option<bool>,
}

/// Properties that are required to query Amazon Honeycode. See Generic Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesHoneycode {
    /// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "errorHandlingConfig")]
    #[builder(default)]
    pub error_handling_config: Option<FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesHoneycodeErrorHandlingConfig>,
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

/// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesHoneycodeErrorHandlingConfig {
    /// Amazon S3 bucket name in which Amazon AppFlow places the transferred data.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketName")]
    #[builder(default)]
    pub bucket_name: Option<String>,
    /// Object key for the bucket in which Amazon AppFlow places the destination files.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketPrefix")]
    #[builder(default)]
    pub bucket_prefix: Option<String>,
    /// If the flow should fail after the first instance of a failure when attempting to place data in the destination.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failOnFirstDestinationError")]
    #[builder(default)]
    pub fail_on_first_destination_error: Option<bool>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesLookoutMetrics {
}

/// Properties that are required to query Marketo. See Generic Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesMarketo {
    /// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "errorHandlingConfig")]
    #[builder(default)]
    pub error_handling_config: Option<FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesMarketoErrorHandlingConfig>,
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

/// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesMarketoErrorHandlingConfig {
    /// Amazon S3 bucket name in which Amazon AppFlow places the transferred data.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketName")]
    #[builder(default)]
    pub bucket_name: Option<String>,
    /// Object key for the bucket in which Amazon AppFlow places the destination files.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketPrefix")]
    #[builder(default)]
    pub bucket_prefix: Option<String>,
    /// If the flow should fail after the first instance of a failure when attempting to place data in the destination.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failOnFirstDestinationError")]
    #[builder(default)]
    pub fail_on_first_destination_error: Option<bool>,
}

/// Properties that are required to query Amazon Redshift. See Redshift Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesRedshift {
    /// Object key for the bucket in which Amazon AppFlow places the destination files.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketPrefix")]
    #[builder(default)]
    pub bucket_prefix: Option<String>,
    /// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "errorHandlingConfig")]
    #[builder(default)]
    pub error_handling_config: Option<FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesRedshiftErrorHandlingConfig>,
    /// Intermediate bucket that Amazon AppFlow uses when moving data into Amazon Redshift.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "intermediateBucketName")]
    #[builder(default)]
    pub intermediate_bucket_name: Option<String>,
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

/// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesRedshiftErrorHandlingConfig {
    /// Amazon S3 bucket name in which Amazon AppFlow places the transferred data.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketName")]
    #[builder(default)]
    pub bucket_name: Option<String>,
    /// Object key for the bucket in which Amazon AppFlow places the destination files.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketPrefix")]
    #[builder(default)]
    pub bucket_prefix: Option<String>,
    /// If the flow should fail after the first instance of a failure when attempting to place data in the destination.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failOnFirstDestinationError")]
    #[builder(default)]
    pub fail_on_first_destination_error: Option<bool>,
}

/// Properties that are required to query Amazon S3. See S3 Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesS3 {
    /// Amazon S3 bucket name in which Amazon AppFlow places the transferred data.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketName")]
    #[builder(default)]
    pub bucket_name: Option<String>,
    /// Object key for the bucket in which Amazon AppFlow places the destination files.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketPrefix")]
    #[builder(default)]
    pub bucket_prefix: Option<String>,
    /// Configuration that determines how Amazon AppFlow should format the flow output data when Amazon S3 is used as the destination. See S3 Output Format Config for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "s3OutputFormatConfig")]
    #[builder(default)]
    pub s3_output_format_config: Option<FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesS3S3OutputFormatConfig>,
}

/// Configuration that determines how Amazon AppFlow should format the flow output data when Amazon S3 is used as the destination. See S3 Output Format Config for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesS3S3OutputFormatConfig {
    /// Aggregation settings that you can use to customize the output format of your flow data. See Aggregation Config for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "aggregationConfig")]
    #[builder(default)]
    pub aggregation_config: Option<FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesS3S3OutputFormatConfigAggregationConfig>,
    /// File type that Amazon AppFlow places in the Amazon S3 bucket. Valid values are CSV, JSON, and PARQUET.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fileType")]
    #[builder(default)]
    pub file_type: Option<String>,
    /// Determines the prefix that Amazon AppFlow applies to the folder name in the Amazon S3 bucket. You can name folders according to the flow frequency and date. See Prefix Config for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "prefixConfig")]
    #[builder(default)]
    pub prefix_config: Option<FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesS3S3OutputFormatConfigPrefixConfig>,
    /// Whether the data types from the source system need to be preserved (Only valid for Parquet file type)
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "preserveSourceDataTyping")]
    #[builder(default)]
    pub preserve_source_data_typing: Option<bool>,
}

/// Aggregation settings that you can use to customize the output format of your flow data. See Aggregation Config for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesS3S3OutputFormatConfigAggregationConfig {
    /// Whether Amazon AppFlow aggregates the flow records into a single file, or leave them unaggregated. Valid values are None and SingleFile.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "aggregationType")]
    #[builder(default)]
    pub aggregation_type: Option<String>,
    /// The desired file size, in MB, for each output file that Amazon AppFlow writes to the flow destination. Integer value.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "targetFileSize")]
    #[builder(default)]
    pub target_file_size: Option<f64>,
}

/// Determines the prefix that Amazon AppFlow applies to the folder name in the Amazon S3 bucket. You can name folders according to the flow frequency and date. See Prefix Config for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesS3S3OutputFormatConfigPrefixConfig {
    /// Determines the level of granularity that's included in the prefix. Valid values are YEAR, MONTH, DAY, HOUR, and MINUTE.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "prefixFormat")]
    #[builder(default)]
    pub prefix_format: Option<String>,
    /// Determines whether the destination file path includes either or both of the selected elements. Valid values are EXECUTION_ID and SCHEMA_VERSION
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "prefixHierarchy")]
    #[builder(default)]
    pub prefix_hierarchy: Option<Vec<String>>,
    /// Determines the format of the prefix, and whether it applies to the file name, file path, or both. Valid values are FILENAME, PATH, and PATH_AND_FILENAME.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "prefixType")]
    #[builder(default)]
    pub prefix_type: Option<String>,
}

/// Properties that are required to query Salesforce. See Salesforce Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesSalesforce {
    /// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "errorHandlingConfig")]
    #[builder(default)]
    pub error_handling_config: Option<FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesSalesforceErrorHandlingConfig>,
    /// Name of the field that Amazon AppFlow uses as an ID when performing a write operation such as update, delete, or upsert.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "idFieldNames")]
    #[builder(default)]
    pub id_field_names: Option<Vec<String>>,
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
    /// Type of write operation to be performed in the custom connector when it's used as destination. Valid values are INSERT, UPSERT, UPDATE, and DELETE.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "writeOperationType")]
    #[builder(default)]
    pub write_operation_type: Option<String>,
}

/// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesSalesforceErrorHandlingConfig {
    /// Amazon S3 bucket name in which Amazon AppFlow places the transferred data.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketName")]
    #[builder(default)]
    pub bucket_name: Option<String>,
    /// Object key for the bucket in which Amazon AppFlow places the destination files.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketPrefix")]
    #[builder(default)]
    pub bucket_prefix: Option<String>,
    /// If the flow should fail after the first instance of a failure when attempting to place data in the destination.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failOnFirstDestinationError")]
    #[builder(default)]
    pub fail_on_first_destination_error: Option<bool>,
}

/// Properties that are required to query SAPOData. See SAPOData Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesSapoData {
    /// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "errorHandlingConfig")]
    #[builder(default)]
    pub error_handling_config: Option<FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesSapoDataErrorHandlingConfig>,
    /// Name of the field that Amazon AppFlow uses as an ID when performing a write operation such as update, delete, or upsert.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "idFieldNames")]
    #[builder(default)]
    pub id_field_names: Option<Vec<String>>,
    /// Object path specified in the SAPOData flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "objectPath")]
    #[builder(default)]
    pub object_path: Option<String>,
    /// Determines how Amazon AppFlow handles the success response that it gets from the connector after placing data. See Success Response Handling Config for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "successResponseHandlingConfig")]
    #[builder(default)]
    pub success_response_handling_config: Option<FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesSapoDataSuccessResponseHandlingConfig>,
    /// Type of write operation to be performed in the custom connector when it's used as destination. Valid values are INSERT, UPSERT, UPDATE, and DELETE.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "writeOperationType")]
    #[builder(default)]
    pub write_operation_type: Option<String>,
}

/// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesSapoDataErrorHandlingConfig {
    /// Amazon S3 bucket name in which Amazon AppFlow places the transferred data.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketName")]
    #[builder(default)]
    pub bucket_name: Option<String>,
    /// Object key for the bucket in which Amazon AppFlow places the destination files.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketPrefix")]
    #[builder(default)]
    pub bucket_prefix: Option<String>,
    /// If the flow should fail after the first instance of a failure when attempting to place data in the destination.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failOnFirstDestinationError")]
    #[builder(default)]
    pub fail_on_first_destination_error: Option<bool>,
}

/// Determines how Amazon AppFlow handles the success response that it gets from the connector after placing data. See Success Response Handling Config for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesSapoDataSuccessResponseHandlingConfig {
    /// Amazon S3 bucket name in which Amazon AppFlow places the transferred data.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketName")]
    #[builder(default)]
    pub bucket_name: Option<String>,
    /// Object key for the bucket in which Amazon AppFlow places the destination files.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketPrefix")]
    #[builder(default)]
    pub bucket_prefix: Option<String>,
}

/// Properties that are required to query Snowflake. See Snowflake Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesSnowflake {
    /// Object key for the bucket in which Amazon AppFlow places the destination files.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketPrefix")]
    #[builder(default)]
    pub bucket_prefix: Option<String>,
    /// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "errorHandlingConfig")]
    #[builder(default)]
    pub error_handling_config: Option<FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesSnowflakeErrorHandlingConfig>,
    /// Intermediate bucket that Amazon AppFlow uses when moving data into Amazon Redshift.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "intermediateBucketName")]
    #[builder(default)]
    pub intermediate_bucket_name: Option<String>,
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

/// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesSnowflakeErrorHandlingConfig {
    /// Amazon S3 bucket name in which Amazon AppFlow places the transferred data.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketName")]
    #[builder(default)]
    pub bucket_name: Option<String>,
    /// Object key for the bucket in which Amazon AppFlow places the destination files.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketPrefix")]
    #[builder(default)]
    pub bucket_prefix: Option<String>,
    /// If the flow should fail after the first instance of a failure when attempting to place data in the destination.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failOnFirstDestinationError")]
    #[builder(default)]
    pub fail_on_first_destination_error: Option<bool>,
}

/// Properties that are required to query Upsolver. See Upsolver Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesUpsolver {
    /// Amazon S3 bucket name in which Amazon AppFlow places the transferred data.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketName")]
    #[builder(default)]
    pub bucket_name: Option<String>,
    /// Object key for the bucket in which Amazon AppFlow places the destination files.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketPrefix")]
    #[builder(default)]
    pub bucket_prefix: Option<String>,
    /// Configuration that determines how Amazon AppFlow should format the flow output data when Amazon S3 is used as the destination. See S3 Output Format Config for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "s3OutputFormatConfig")]
    #[builder(default)]
    pub s3_output_format_config: Option<FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesUpsolverS3OutputFormatConfig>,
}

/// Configuration that determines how Amazon AppFlow should format the flow output data when Amazon S3 is used as the destination. See S3 Output Format Config for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesUpsolverS3OutputFormatConfig {
    /// Aggregation settings that you can use to customize the output format of your flow data. See Aggregation Config for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "aggregationConfig")]
    #[builder(default)]
    pub aggregation_config: Option<FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesUpsolverS3OutputFormatConfigAggregationConfig>,
    /// File type that Amazon AppFlow places in the Amazon S3 bucket. Valid values are CSV, JSON, and PARQUET.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fileType")]
    #[builder(default)]
    pub file_type: Option<String>,
    /// Determines the prefix that Amazon AppFlow applies to the folder name in the Amazon S3 bucket. You can name folders according to the flow frequency and date. See Prefix Config for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "prefixConfig")]
    #[builder(default)]
    pub prefix_config: Option<FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesUpsolverS3OutputFormatConfigPrefixConfig>,
}

/// Aggregation settings that you can use to customize the output format of your flow data. See Aggregation Config for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesUpsolverS3OutputFormatConfigAggregationConfig {
    /// Whether Amazon AppFlow aggregates the flow records into a single file, or leave them unaggregated. Valid values are None and SingleFile.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "aggregationType")]
    #[builder(default)]
    pub aggregation_type: Option<String>,
}

/// Determines the prefix that Amazon AppFlow applies to the folder name in the Amazon S3 bucket. You can name folders according to the flow frequency and date. See Prefix Config for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesUpsolverS3OutputFormatConfigPrefixConfig {
    /// Determines the level of granularity that's included in the prefix. Valid values are YEAR, MONTH, DAY, HOUR, and MINUTE.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "prefixFormat")]
    #[builder(default)]
    pub prefix_format: Option<String>,
    /// Determines whether the destination file path includes either or both of the selected elements. Valid values are EXECUTION_ID and SCHEMA_VERSION
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "prefixHierarchy")]
    #[builder(default)]
    pub prefix_hierarchy: Option<Vec<String>>,
    /// Determines the format of the prefix, and whether it applies to the file name, file path, or both. Valid values are FILENAME, PATH, and PATH_AND_FILENAME.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "prefixType")]
    #[builder(default)]
    pub prefix_type: Option<String>,
}

/// Properties that are required to query Zendesk. See Zendesk Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesZendesk {
    /// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "errorHandlingConfig")]
    #[builder(default)]
    pub error_handling_config: Option<FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesZendeskErrorHandlingConfig>,
    /// Name of the field that Amazon AppFlow uses as an ID when performing a write operation such as update, delete, or upsert.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "idFieldNames")]
    #[builder(default)]
    pub id_field_names: Option<Vec<String>>,
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
    /// Type of write operation to be performed in the custom connector when it's used as destination. Valid values are INSERT, UPSERT, UPDATE, and DELETE.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "writeOperationType")]
    #[builder(default)]
    pub write_operation_type: Option<String>,
}

/// Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See Error Handling Config for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderDestinationFlowConfigDestinationConnectorPropertiesZendeskErrorHandlingConfig {
    /// Amazon S3 bucket name in which Amazon AppFlow places the transferred data.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketName")]
    #[builder(default)]
    pub bucket_name: Option<String>,
    /// Object key for the bucket in which Amazon AppFlow places the destination files.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketPrefix")]
    #[builder(default)]
    pub bucket_prefix: Option<String>,
    /// If the flow should fail after the first instance of a failure when attempting to place data in the destination.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "failOnFirstDestinationError")]
    #[builder(default)]
    pub fail_on_first_destination_error: Option<bool>,
}

/// A Catalog that determines the configuration that Amazon AppFlow uses when it catalogs the data thatâ€™s transferred by the associated flow. When Amazon AppFlow catalogs the data from a flow, it stores metadata in a data catalog.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderMetadataCatalogConfig {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "glueDataCatalog")]
    #[builder(default)]
    pub glue_data_catalog: Option<FlowStatusAtProviderMetadataCatalogConfigGlueDataCatalog>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderMetadataCatalogConfigGlueDataCatalog {
    /// The name of an existing Glue database to store the metadata tables that Amazon AppFlow creates.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "databaseName")]
    #[builder(default)]
    pub database_name: Option<String>,
    /// The ARN of an IAM role that grants AppFlow the permissions it needs to create Data Catalog tables, databases, and partitions.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "roleArn")]
    #[builder(default)]
    pub role_arn: Option<String>,
    /// A naming prefix for each Data Catalog table that Amazon AppFlow creates
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "tablePrefix")]
    #[builder(default)]
    pub table_prefix: Option<String>,
}

/// The Source Flow Config that controls how Amazon AppFlow retrieves data from the source connector.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderSourceFlowConfig {
    /// API version that the destination connector uses.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "apiVersion")]
    #[builder(default)]
    pub api_version: Option<String>,
    /// Name of the connector profile. This name must be unique for each connector profile in the AWS account.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "connectorProfileName")]
    #[builder(default)]
    pub connector_profile_name: Option<String>,
    /// Type of connector, such as Salesforce, Amplitude, and so on. Valid values are Salesforce, Singular, Slack, Redshift, S3, Marketo, Googleanalytics, Zendesk, Servicenow, Datadog, Trendmicro, Snowflake, Dynatrace, Infornexus, Amplitude, Veeva, EventBridge, LookoutMetrics, Upsolver, Honeycode, CustomerProfiles, SAPOData, and CustomConnector.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "connectorType")]
    #[builder(default)]
    pub connector_type: Option<String>,
    /// Defines the configuration for a scheduled incremental data pull. If a valid configuration is provided, the fields specified in the configuration are used when querying for the incremental data pull. See Incremental Pull Config for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "incrementalPullConfig")]
    #[builder(default)]
    pub incremental_pull_config: Option<FlowStatusAtProviderSourceFlowConfigIncrementalPullConfig>,
    /// Information that is required to query a particular source connector. See Source Connector Properties for details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sourceConnectorProperties")]
    #[builder(default)]
    pub source_connector_properties: Option<FlowStatusAtProviderSourceFlowConfigSourceConnectorProperties>,
}

/// Defines the configuration for a scheduled incremental data pull. If a valid configuration is provided, the fields specified in the configuration are used when querying for the incremental data pull. See Incremental Pull Config for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderSourceFlowConfigIncrementalPullConfig {
    /// Field that specifies the date time or timestamp field as the criteria to use when importing incremental records from the source.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "datetimeTypeFieldName")]
    #[builder(default)]
    pub datetime_type_field_name: Option<String>,
}

/// Information that is required to query a particular source connector. See Source Connector Properties for details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderSourceFlowConfigSourceConnectorProperties {
    /// Information that is required for querying Amplitude. See Generic Source Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub amplitude: Option<FlowStatusAtProviderSourceFlowConfigSourceConnectorPropertiesAmplitude>,
    /// Properties that are required to query the custom Connector. See Custom Connector Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "customConnector")]
    #[builder(default)]
    pub custom_connector: Option<FlowStatusAtProviderSourceFlowConfigSourceConnectorPropertiesCustomConnector>,
    /// Information that is required for querying Datadog. See Generic Source Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub datadog: Option<FlowStatusAtProviderSourceFlowConfigSourceConnectorPropertiesDatadog>,
    /// Operation to be performed on the provided Dynatrace source fields. Valid values are PROJECTION, BETWEEN, EQUAL_TO, ADDITION, MULTIPLICATION, DIVISION, SUBTRACTION, MASK_ALL, MASK_FIRST_N, MASK_LAST_N, VALIDATE_NON_NULL, VALIDATE_NON_ZERO, VALIDATE_NON_NEGATIVE, VALIDATE_NUMERIC, and NO_OP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub dynatrace: Option<FlowStatusAtProviderSourceFlowConfigSourceConnectorPropertiesDynatrace>,
    /// Operation to be performed on the provided Google Analytics source fields. Valid values are PROJECTION and BETWEEN.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "googleAnalytics")]
    #[builder(default)]
    pub google_analytics: Option<FlowStatusAtProviderSourceFlowConfigSourceConnectorPropertiesGoogleAnalytics>,
    /// Information that is required for querying Infor Nexus. See Generic Source Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "inforNexus")]
    #[builder(default)]
    pub infor_nexus: Option<FlowStatusAtProviderSourceFlowConfigSourceConnectorPropertiesInforNexus>,
    /// Properties that are required to query Marketo. See Generic Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub marketo: Option<FlowStatusAtProviderSourceFlowConfigSourceConnectorPropertiesMarketo>,
    /// Properties that are required to query Amazon S3. See S3 Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub s3: Option<FlowStatusAtProviderSourceFlowConfigSourceConnectorPropertiesS3>,
    /// Properties that are required to query Salesforce. See Salesforce Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub salesforce: Option<FlowStatusAtProviderSourceFlowConfigSourceConnectorPropertiesSalesforce>,
    /// Properties that are required to query SAPOData. See SAPOData Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sapoData")]
    #[builder(default)]
    pub sapo_data: Option<FlowStatusAtProviderSourceFlowConfigSourceConnectorPropertiesSapoData>,
    /// Information that is required for querying ServiceNow. See Generic Source Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceNow")]
    #[builder(default)]
    pub service_now: Option<FlowStatusAtProviderSourceFlowConfigSourceConnectorPropertiesServiceNow>,
    /// Information that is required for querying Singular. See Generic Source Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub singular: Option<FlowStatusAtProviderSourceFlowConfigSourceConnectorPropertiesSingular>,
    /// Information that is required for querying Slack. See Generic Source Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub slack: Option<FlowStatusAtProviderSourceFlowConfigSourceConnectorPropertiesSlack>,
    /// Operation to be performed on the provided Trend Micro source fields. Valid values are PROJECTION, EQUAL_TO, ADDITION, MULTIPLICATION, DIVISION, SUBTRACTION, MASK_ALL, MASK_FIRST_N, MASK_LAST_N, VALIDATE_NON_NULL, VALIDATE_NON_ZERO, VALIDATE_NON_NEGATIVE, VALIDATE_NUMERIC, and NO_OP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub trendmicro: Option<FlowStatusAtProviderSourceFlowConfigSourceConnectorPropertiesTrendmicro>,
    /// Information that is required for querying Veeva. See Veeva Source Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub veeva: Option<FlowStatusAtProviderSourceFlowConfigSourceConnectorPropertiesVeeva>,
    /// Properties that are required to query Zendesk. See Zendesk Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub zendesk: Option<FlowStatusAtProviderSourceFlowConfigSourceConnectorPropertiesZendesk>,
}

/// Information that is required for querying Amplitude. See Generic Source Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderSourceFlowConfigSourceConnectorPropertiesAmplitude {
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

/// Properties that are required to query the custom Connector. See Custom Connector Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderSourceFlowConfigSourceConnectorPropertiesCustomConnector {
    /// Custom properties that are specific to the connector when it's used as a destination in the flow. Maximum of 50 items.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "customProperties")]
    #[builder(default)]
    pub custom_properties: Option<HashMap<String, String>>,
    /// Entity specified in the custom connector as a destination in the flow.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "entityName")]
    #[builder(default)]
    pub entity_name: Option<String>,
}

/// Information that is required for querying Datadog. See Generic Source Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderSourceFlowConfigSourceConnectorPropertiesDatadog {
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

/// Operation to be performed on the provided Dynatrace source fields. Valid values are PROJECTION, BETWEEN, EQUAL_TO, ADDITION, MULTIPLICATION, DIVISION, SUBTRACTION, MASK_ALL, MASK_FIRST_N, MASK_LAST_N, VALIDATE_NON_NULL, VALIDATE_NON_ZERO, VALIDATE_NON_NEGATIVE, VALIDATE_NUMERIC, and NO_OP.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderSourceFlowConfigSourceConnectorPropertiesDynatrace {
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

/// Operation to be performed on the provided Google Analytics source fields. Valid values are PROJECTION and BETWEEN.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderSourceFlowConfigSourceConnectorPropertiesGoogleAnalytics {
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

/// Information that is required for querying Infor Nexus. See Generic Source Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderSourceFlowConfigSourceConnectorPropertiesInforNexus {
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

/// Properties that are required to query Marketo. See Generic Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderSourceFlowConfigSourceConnectorPropertiesMarketo {
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

/// Properties that are required to query Amazon S3. See S3 Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderSourceFlowConfigSourceConnectorPropertiesS3 {
    /// Amazon S3 bucket name in which Amazon AppFlow places the transferred data.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketName")]
    #[builder(default)]
    pub bucket_name: Option<String>,
    /// Object key for the bucket in which Amazon AppFlow places the destination files.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "bucketPrefix")]
    #[builder(default)]
    pub bucket_prefix: Option<String>,
    /// When you use Amazon S3 as the source, the configuration format that you provide the flow input data. See S3 Input Format Config for details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "s3InputFormatConfig")]
    #[builder(default)]
    pub s3_input_format_config: Option<FlowStatusAtProviderSourceFlowConfigSourceConnectorPropertiesS3S3InputFormatConfig>,
}

/// When you use Amazon S3 as the source, the configuration format that you provide the flow input data. See S3 Input Format Config for details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderSourceFlowConfigSourceConnectorPropertiesS3S3InputFormatConfig {
    /// File type that Amazon AppFlow gets from your Amazon S3 bucket. Valid values are CSV and JSON.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "s3InputFileType")]
    #[builder(default)]
    pub s3_input_file_type: Option<String>,
}

/// Properties that are required to query Salesforce. See Salesforce Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderSourceFlowConfigSourceConnectorPropertiesSalesforce {
    /// Flag that enables dynamic fetching of new (recently added) fields in the Salesforce objects while running a flow.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "enableDynamicFieldUpdate")]
    #[builder(default)]
    pub enable_dynamic_field_update: Option<bool>,
    /// Whether Amazon AppFlow includes deleted files in the flow run.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "includeDeletedRecords")]
    #[builder(default)]
    pub include_deleted_records: Option<bool>,
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

/// Properties that are required to query SAPOData. See SAPOData Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderSourceFlowConfigSourceConnectorPropertiesSapoData {
    /// Object path specified in the SAPOData flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "objectPath")]
    #[builder(default)]
    pub object_path: Option<String>,
    /// Sets the page size for each concurrent process that transfers OData records from your SAP instance.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "paginationConfig")]
    #[builder(default)]
    pub pagination_config: Option<FlowStatusAtProviderSourceFlowConfigSourceConnectorPropertiesSapoDataPaginationConfig>,
    /// Sets the number of concurrent processes that transfers OData records from your SAP instance.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "parallelismConfig")]
    #[builder(default)]
    pub parallelism_config: Option<FlowStatusAtProviderSourceFlowConfigSourceConnectorPropertiesSapoDataParallelismConfig>,
}

/// Sets the page size for each concurrent process that transfers OData records from your SAP instance.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderSourceFlowConfigSourceConnectorPropertiesSapoDataPaginationConfig {
    /// he maximum number of records that Amazon AppFlow receives in each page of the response from your SAP application.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxPageSize")]
    #[builder(default)]
    pub max_page_size: Option<f64>,
}

/// Sets the number of concurrent processes that transfers OData records from your SAP instance.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderSourceFlowConfigSourceConnectorPropertiesSapoDataParallelismConfig {
    /// he maximum number of records that Amazon AppFlow receives in each page of the response from your SAP application.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxPageSize")]
    #[builder(default)]
    pub max_page_size: Option<f64>,
}

/// Information that is required for querying ServiceNow. See Generic Source Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderSourceFlowConfigSourceConnectorPropertiesServiceNow {
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

/// Information that is required for querying Singular. See Generic Source Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderSourceFlowConfigSourceConnectorPropertiesSingular {
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

/// Information that is required for querying Slack. See Generic Source Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderSourceFlowConfigSourceConnectorPropertiesSlack {
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

/// Operation to be performed on the provided Trend Micro source fields. Valid values are PROJECTION, EQUAL_TO, ADDITION, MULTIPLICATION, DIVISION, SUBTRACTION, MASK_ALL, MASK_FIRST_N, MASK_LAST_N, VALIDATE_NON_NULL, VALIDATE_NON_ZERO, VALIDATE_NON_NEGATIVE, VALIDATE_NUMERIC, and NO_OP.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderSourceFlowConfigSourceConnectorPropertiesTrendmicro {
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

/// Information that is required for querying Veeva. See Veeva Source Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderSourceFlowConfigSourceConnectorPropertiesVeeva {
    /// Document type specified in the Veeva document extract flow.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "documentType")]
    #[builder(default)]
    pub document_type: Option<String>,
    /// Boolean value to include All Versions of files in Veeva document extract flow.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "includeAllVersions")]
    #[builder(default)]
    pub include_all_versions: Option<bool>,
    /// Boolean value to include file renditions in Veeva document extract flow.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "includeRenditions")]
    #[builder(default)]
    pub include_renditions: Option<bool>,
    /// Boolean value to include source files in Veeva document extract flow.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "includeSourceFiles")]
    #[builder(default)]
    pub include_source_files: Option<bool>,
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

/// Properties that are required to query Zendesk. See Zendesk Destination Properties for more details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderSourceFlowConfigSourceConnectorPropertiesZendesk {
    /// Object specified in the flow destination.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub object: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderTask {
    /// Operation to be performed on the provided source fields. See Connector Operator for details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "connectorOperator")]
    #[builder(default)]
    pub connector_operator: Option<Vec<FlowStatusAtProviderTaskConnectorOperator>>,
    /// Field in a destination connector, or a field value against which Amazon AppFlow validates a source field.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "destinationField")]
    #[builder(default)]
    pub destination_field: Option<String>,
    /// Source fields to which a particular task is applied.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sourceFields")]
    #[builder(default)]
    pub source_fields: Option<Vec<String>>,
    /// Map used to store task-related information. The execution service looks for particular information based on the TaskType. Valid keys are VALUE, VALUES, DATA_TYPE, UPPER_BOUND, LOWER_BOUND, SOURCE_DATA_TYPE, DESTINATION_DATA_TYPE, VALIDATION_ACTION, MASK_VALUE, MASK_LENGTH, TRUNCATE_LENGTH, MATH_OPERATION_FIELDS_ORDER, CONCAT_FORMAT, SUBFIELD_CATEGORY_MAP, and EXCLUDE_SOURCE_FIELDS_LIST.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "taskProperties")]
    #[builder(default)]
    pub task_properties: Option<HashMap<String, String>>,
    /// Particular task implementation that Amazon AppFlow performs. Valid values are Arithmetic, Filter, Map, Map_all, Mask, Merge, Passthrough, Truncate, and Validate.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "taskType")]
    #[builder(default)]
    pub task_type: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderTaskConnectorOperator {
    /// Information that is required for querying Amplitude. See Generic Source Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub amplitude: Option<String>,
    /// Properties that are required to query the custom Connector. See Custom Connector Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "customConnector")]
    #[builder(default)]
    pub custom_connector: Option<String>,
    /// Information that is required for querying Datadog. See Generic Source Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub datadog: Option<String>,
    /// Operation to be performed on the provided Dynatrace source fields. Valid values are PROJECTION, BETWEEN, EQUAL_TO, ADDITION, MULTIPLICATION, DIVISION, SUBTRACTION, MASK_ALL, MASK_FIRST_N, MASK_LAST_N, VALIDATE_NON_NULL, VALIDATE_NON_ZERO, VALIDATE_NON_NEGATIVE, VALIDATE_NUMERIC, and NO_OP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub dynatrace: Option<String>,
    /// Operation to be performed on the provided Google Analytics source fields. Valid values are PROJECTION and BETWEEN.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "googleAnalytics")]
    #[builder(default)]
    pub google_analytics: Option<String>,
    /// Information that is required for querying Infor Nexus. See Generic Source Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "inforNexus")]
    #[builder(default)]
    pub infor_nexus: Option<String>,
    /// Properties that are required to query Marketo. See Generic Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub marketo: Option<String>,
    /// Properties that are required to query Amazon S3. See S3 Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub s3: Option<String>,
    /// Properties that are required to query Salesforce. See Salesforce Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub salesforce: Option<String>,
    /// Properties that are required to query SAPOData. See SAPOData Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sapoData")]
    #[builder(default)]
    pub sapo_data: Option<String>,
    /// Information that is required for querying ServiceNow. See Generic Source Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "serviceNow")]
    #[builder(default)]
    pub service_now: Option<String>,
    /// Information that is required for querying Singular. See Generic Source Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub singular: Option<String>,
    /// Information that is required for querying Slack. See Generic Source Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub slack: Option<String>,
    /// Operation to be performed on the provided Trend Micro source fields. Valid values are PROJECTION, EQUAL_TO, ADDITION, MULTIPLICATION, DIVISION, SUBTRACTION, MASK_ALL, MASK_FIRST_N, MASK_LAST_N, VALIDATE_NON_NULL, VALIDATE_NON_ZERO, VALIDATE_NON_NEGATIVE, VALIDATE_NUMERIC, and NO_OP.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub trendmicro: Option<String>,
    /// Information that is required for querying Veeva. See Veeva Source Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub veeva: Option<String>,
    /// Properties that are required to query Zendesk. See Zendesk Destination Properties for more details.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub zendesk: Option<String>,
}

/// A Trigger that determine how and when the flow runs.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderTriggerConfig {
    /// Configuration details of a schedule-triggered flow as defined by the user. Currently, these settings only apply to the Scheduled trigger type. See Scheduled Trigger Properties for details.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "triggerProperties")]
    #[builder(default)]
    pub trigger_properties: Option<FlowStatusAtProviderTriggerConfigTriggerProperties>,
    /// Type of flow trigger. Valid values are Scheduled, Event, and OnDemand.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "triggerType")]
    #[builder(default)]
    pub trigger_type: Option<String>,
}

/// Configuration details of a schedule-triggered flow as defined by the user. Currently, these settings only apply to the Scheduled trigger type. See Scheduled Trigger Properties for details.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderTriggerConfigTriggerProperties {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub scheduled: Option<FlowStatusAtProviderTriggerConfigTriggerPropertiesScheduled>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct FlowStatusAtProviderTriggerConfigTriggerPropertiesScheduled {
    /// Whether a scheduled flow has an incremental data transfer or a complete data transfer for each flow run. Valid values are Incremental and Complete.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "dataPullMode")]
    #[builder(default)]
    pub data_pull_mode: Option<String>,
    /// Date range for the records to import from the connector in the first flow run. Must be a valid RFC3339 timestamp.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "firstExecutionFrom")]
    #[builder(default)]
    pub first_execution_from: Option<String>,
    /// Scheduled end time for a schedule-triggered flow. Must be a valid RFC3339 timestamp.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "scheduleEndTime")]
    #[builder(default)]
    pub schedule_end_time: Option<String>,
    /// Scheduling expression that determines the rate at which the schedule will run, for example rate(5minutes).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "scheduleExpression")]
    #[builder(default)]
    pub schedule_expression: Option<String>,
    /// Optional offset that is added to the time interval for a schedule-triggered flow. Maximum value of 36000.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "scheduleOffset")]
    #[builder(default)]
    pub schedule_offset: Option<f64>,
    /// Scheduled start time for a schedule-triggered flow. Must be a valid RFC3339 timestamp.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "scheduleStartTime")]
    #[builder(default)]
    pub schedule_start_time: Option<String>,
    /// Time zone used when referring to the date and time of a scheduled-triggered flow, such as America/New_York.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub timezone: Option<String>,
}

