// WARNING: generated by kopium - manual changes will be overwritten
// kopium version: 0.21.2

#[allow(unused_imports)]
mod prelude {
    pub use kube::CustomResource;
    pub use typed_builder::TypedBuilder;
    pub use schemars::JsonSchema;
    pub use serde::{Serialize, Deserialize};
    pub use std::collections::HashMap;
    pub use k8s_openapi::apimachinery::pkg::apis::meta::v1::Condition;
}
use self::prelude::*;

/// JobSpec defines the desired state of Job
#[derive(CustomResource, Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
#[kube(group = "dataproc.gcp.upbound.io", version = "v1beta2", kind = "Job", plural = "jobs")]
#[kube(status = "JobStatus")]
pub struct JobSpec {
    /// DeletionPolicy specifies what will happen to the underlying external
    /// when this managed resource is deleted - either "Delete" or "Orphan" the
    /// external resource.
    /// This field is planned to be deprecated in favor of the ManagementPolicies
    /// field in a future release. Currently, both could be set independently and
    /// non-default values would be honored if the feature flag is enabled.
    /// See the design doc for more information: https://github.com/crossplane/crossplane/blob/499895a25d1a1a0ba1604944ef98ac7a1a71f197/design/design-doc-observe-only-resources.md?plain=1#L223
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "deletionPolicy")]
    #[builder(default)]
    pub deletion_policy: Option<JobDeletionPolicy>,
    #[serde(rename = "forProvider")]
    pub for_provider: JobForProvider,
    /// THIS IS A BETA FIELD. It will be honored
    /// unless the Management Policies feature flag is disabled.
    /// InitProvider holds the same fields as ForProvider, with the exception
    /// of Identifier and other resource reference fields. The fields that are
    /// in InitProvider are merged into ForProvider when the resource is created.
    /// The same fields are also added to the terraform ignore_changes hook, to
    /// avoid updating them after creation. This is useful for fields that are
    /// required on creation, but we do not desire to update them after creation,
    /// for example because of an external controller is managing them, like an
    /// autoscaler.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "initProvider")]
    #[builder(default)]
    pub init_provider: Option<JobInitProvider>,
    /// THIS IS A BETA FIELD. It is on by default but can be opted out
    /// through a Crossplane feature flag.
    /// ManagementPolicies specify the array of actions Crossplane is allowed to
    /// take on the managed and external resources.
    /// This field is planned to replace the DeletionPolicy field in a future
    /// release. Currently, both could be set independently and non-default
    /// values would be honored if the feature flag is enabled. If both are
    /// custom, the DeletionPolicy field will be ignored.
    /// See the design doc for more information: https://github.com/crossplane/crossplane/blob/499895a25d1a1a0ba1604944ef98ac7a1a71f197/design/design-doc-observe-only-resources.md?plain=1#L223
    /// and this one: https://github.com/crossplane/crossplane/blob/444267e84783136daa93568b364a5f01228cacbe/design/one-pager-ignore-changes.md
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "managementPolicies")]
    #[builder(default)]
    pub management_policies: Option<Vec<String>>,
    /// ProviderConfigReference specifies how the provider that will be used to
    /// create, observe, update, and delete this managed resource should be
    /// configured.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "providerConfigRef")]
    #[builder(default)]
    pub provider_config_ref: Option<JobProviderConfigRef>,
    /// PublishConnectionDetailsTo specifies the connection secret config which
    /// contains a name, metadata and a reference to secret store config to
    /// which any connection details for this managed resource should be written.
    /// Connection details frequently include the endpoint, username,
    /// and password required to connect to the managed resource.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "publishConnectionDetailsTo")]
    #[builder(default)]
    pub publish_connection_details_to: Option<JobPublishConnectionDetailsTo>,
    /// WriteConnectionSecretToReference specifies the namespace and name of a
    /// Secret to which any connection details for this managed resource should
    /// be written. Connection details frequently include the endpoint, username,
    /// and password required to connect to the managed resource.
    /// This field is planned to be replaced in a future release in favor of
    /// PublishConnectionDetailsTo. Currently, both could be set independently
    /// and connection details would be published to both without affecting
    /// each other.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "writeConnectionSecretToRef")]
    #[builder(default)]
    pub write_connection_secret_to_ref: Option<JobWriteConnectionSecretToRef>,
}

/// JobSpec defines the desired state of Job
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum JobDeletionPolicy {
    Orphan,
    Delete,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobForProvider {
    /// By default, you can only delete inactive jobs within
    /// Dataproc. Setting this to true, and calling destroy, will ensure that the
    /// job is first cancelled before issuing the delete.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "forceDelete")]
    #[builder(default)]
    pub force_delete: Option<bool>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hadoopConfig")]
    #[builder(default)]
    pub hadoop_config: Option<JobForProviderHadoopConfig>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hiveConfig")]
    #[builder(default)]
    pub hive_config: Option<JobForProviderHiveConfig>,
    /// The list of labels (key/value pairs) to add to the job.
    /// Note: This field is non-authoritative, and will only manage the labels present in your configuration.
    /// Please refer to the field 'effective_labels' for all of the labels present on the resource.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub labels: Option<HashMap<String, String>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "pigConfig")]
    #[builder(default)]
    pub pig_config: Option<JobForProviderPigConfig>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub placement: Option<JobForProviderPlacement>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "prestoConfig")]
    #[builder(default)]
    pub presto_config: Option<JobForProviderPrestoConfig>,
    /// The project in which the cluster can be found and jobs
    /// subsequently run against. If it is not provided, the provider project is used.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub project: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "pysparkConfig")]
    #[builder(default)]
    pub pyspark_config: Option<JobForProviderPysparkConfig>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub reference: Option<JobForProviderReference>,
    /// The Cloud Dataproc region. This essentially determines which clusters are available
    /// for this job to be submitted to. If not specified, defaults to global.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub region: Option<String>,
    /// Reference to a Cluster in dataproc to populate region.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "regionRef")]
    #[builder(default)]
    pub region_ref: Option<JobForProviderRegionRef>,
    /// Selector for a Cluster in dataproc to populate region.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "regionSelector")]
    #[builder(default)]
    pub region_selector: Option<JobForProviderRegionSelector>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub scheduling: Option<JobForProviderScheduling>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sparkConfig")]
    #[builder(default)]
    pub spark_config: Option<JobForProviderSparkConfig>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sparksqlConfig")]
    #[builder(default)]
    pub sparksql_config: Option<JobForProviderSparksqlConfig>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobForProviderHadoopConfig {
    /// HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "archiveUris")]
    #[builder(default)]
    pub archive_uris: Option<Vec<String>>,
    /// The arguments to pass to the driver. Do not include arguments, such as -libjars or -Dfoo=bar, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub args: Option<Vec<String>>,
    /// HCFS URIs of files to be copied to the working directory of Hadoop drivers and distributed tasks. Useful for naively parallel tasks.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fileUris")]
    #[builder(default)]
    pub file_uris: Option<Vec<String>>,
    /// HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jarFileUris")]
    #[builder(default)]
    pub jar_file_uris: Option<Vec<String>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "loggingConfig")]
    #[builder(default)]
    pub logging_config: Option<JobForProviderHadoopConfigLoggingConfig>,
    /// The name of the driver's main class. The jar file containing the class must be in the default CLASSPATH or specified in jar_file_uris. Conflicts with main_jar_file_uri
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mainClass")]
    #[builder(default)]
    pub main_class: Option<String>,
    /// The HCFS URI of the jar file containing the main class. Examples: 'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar' 'hdfs:/tmp/test-samples/custom-wordcount.jar' 'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'. Conflicts with main_class
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mainJarFileUri")]
    #[builder(default)]
    pub main_jar_file_uri: Option<String>,
    /// A mapping of property names to values, used to configure Hadoop. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site and classes in user code..
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub properties: Option<HashMap<String, String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobForProviderHadoopConfigLoggingConfig {
    /// The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "driverLogLevels")]
    #[builder(default)]
    pub driver_log_levels: Option<HashMap<String, String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobForProviderHiveConfig {
    /// Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries. Defaults to false.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "continueOnFailure")]
    #[builder(default)]
    pub continue_on_failure: Option<bool>,
    /// HCFS URIs of jar files to add to the CLASSPATH of the Hive server and Hadoop MapReduce (MR) tasks. Can contain Hive SerDes and UDFs.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jarFileUris")]
    #[builder(default)]
    pub jar_file_uris: Option<Vec<String>>,
    /// A mapping of property names and values, used to configure Hive. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/hive/conf/hive-site.xml, and classes in user code..
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub properties: Option<HashMap<String, String>>,
    /// HCFS URI of file containing Hive script to execute as the job.
    /// Conflicts with query_list
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryFileUri")]
    #[builder(default)]
    pub query_file_uri: Option<String>,
    /// The list of Hive queries or statements to execute as part of the job.
    /// Conflicts with query_file_uri
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryList")]
    #[builder(default)]
    pub query_list: Option<Vec<String>>,
    /// Mapping of query variable names to values (equivalent to the Hive command: SET name="value";).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "scriptVariables")]
    #[builder(default)]
    pub script_variables: Option<HashMap<String, String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobForProviderPigConfig {
    /// Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries. Defaults to false.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "continueOnFailure")]
    #[builder(default)]
    pub continue_on_failure: Option<bool>,
    /// HCFS URIs of jar files to add to the CLASSPATH of the Pig Client and Hadoop MapReduce (MR) tasks. Can contain Pig UDFs.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jarFileUris")]
    #[builder(default)]
    pub jar_file_uris: Option<Vec<String>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "loggingConfig")]
    #[builder(default)]
    pub logging_config: Option<JobForProviderPigConfigLoggingConfig>,
    /// A mapping of property names to values, used to configure Pig. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/pig/conf/pig.properties, and classes in user code.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub properties: Option<HashMap<String, String>>,
    /// HCFS URI of file containing Hive script to execute as the job.
    /// Conflicts with query_list
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryFileUri")]
    #[builder(default)]
    pub query_file_uri: Option<String>,
    /// The list of Hive queries or statements to execute as part of the job.
    /// Conflicts with query_file_uri
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryList")]
    #[builder(default)]
    pub query_list: Option<Vec<String>>,
    /// Mapping of query variable names to values (equivalent to the Pig command: name=[value]).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "scriptVariables")]
    #[builder(default)]
    pub script_variables: Option<HashMap<String, String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobForProviderPigConfigLoggingConfig {
    /// The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "driverLogLevels")]
    #[builder(default)]
    pub driver_log_levels: Option<HashMap<String, String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobForProviderPlacement {
    /// The name of the cluster where the job
    /// will be submitted.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clusterName")]
    #[builder(default)]
    pub cluster_name: Option<String>,
    /// Reference to a Cluster in dataproc to populate clusterName.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clusterNameRef")]
    #[builder(default)]
    pub cluster_name_ref: Option<JobForProviderPlacementClusterNameRef>,
    /// Selector for a Cluster in dataproc to populate clusterName.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clusterNameSelector")]
    #[builder(default)]
    pub cluster_name_selector: Option<JobForProviderPlacementClusterNameSelector>,
}

/// Reference to a Cluster in dataproc to populate clusterName.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobForProviderPlacementClusterNameRef {
    /// Name of the referenced object.
    pub name: String,
    /// Policies for referencing.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub policy: Option<JobForProviderPlacementClusterNameRefPolicy>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobForProviderPlacementClusterNameRefPolicy {
    /// Resolution specifies whether resolution of this reference is required.
    /// The default is 'Required', which means the reconcile will fail if the
    /// reference cannot be resolved. 'Optional' means this reference will be
    /// a no-op if it cannot be resolved.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub resolution: Option<JobForProviderPlacementClusterNameRefPolicyResolution>,
    /// Resolve specifies when this reference should be resolved. The default
    /// is 'IfNotPresent', which will attempt to resolve the reference only when
    /// the corresponding field is not present. Use 'Always' to resolve the
    /// reference on every reconcile.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub resolve: Option<JobForProviderPlacementClusterNameRefPolicyResolve>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum JobForProviderPlacementClusterNameRefPolicyResolution {
    Required,
    Optional,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum JobForProviderPlacementClusterNameRefPolicyResolve {
    Always,
    IfNotPresent,
}

/// Selector for a Cluster in dataproc to populate clusterName.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobForProviderPlacementClusterNameSelector {
    /// MatchControllerRef ensures an object with the same controller reference
    /// as the selecting object is selected.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchControllerRef")]
    #[builder(default)]
    pub match_controller_ref: Option<bool>,
    /// MatchLabels ensures an object with matching labels is selected.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    #[builder(default)]
    pub match_labels: Option<HashMap<String, String>>,
    /// Policies for selection.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub policy: Option<JobForProviderPlacementClusterNameSelectorPolicy>,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobForProviderPlacementClusterNameSelectorPolicy {
    /// Resolution specifies whether resolution of this reference is required.
    /// The default is 'Required', which means the reconcile will fail if the
    /// reference cannot be resolved. 'Optional' means this reference will be
    /// a no-op if it cannot be resolved.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub resolution: Option<JobForProviderPlacementClusterNameSelectorPolicyResolution>,
    /// Resolve specifies when this reference should be resolved. The default
    /// is 'IfNotPresent', which will attempt to resolve the reference only when
    /// the corresponding field is not present. Use 'Always' to resolve the
    /// reference on every reconcile.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub resolve: Option<JobForProviderPlacementClusterNameSelectorPolicyResolve>,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum JobForProviderPlacementClusterNameSelectorPolicyResolution {
    Required,
    Optional,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum JobForProviderPlacementClusterNameSelectorPolicyResolve {
    Always,
    IfNotPresent,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobForProviderPrestoConfig {
    /// Presto client tags to attach to this query.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clientTags")]
    #[builder(default)]
    pub client_tags: Option<Vec<String>>,
    /// Whether to continue executing queries if a query fails. Setting to true can be useful when executing independent parallel queries. Defaults to false.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "continueOnFailure")]
    #[builder(default)]
    pub continue_on_failure: Option<bool>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "loggingConfig")]
    #[builder(default)]
    pub logging_config: Option<JobForProviderPrestoConfigLoggingConfig>,
    /// The format in which query output will be displayed. See the Presto documentation for supported output formats.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "outputFormat")]
    #[builder(default)]
    pub output_format: Option<String>,
    /// A mapping of property names to values. Used to set Presto session properties Equivalent to using the --session flag in the Presto CLI.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub properties: Option<HashMap<String, String>>,
    /// The HCFS URI of the script that contains SQL queries.
    /// Conflicts with query_list
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryFileUri")]
    #[builder(default)]
    pub query_file_uri: Option<String>,
    /// The list of SQL queries or statements to execute as part of the job.
    /// Conflicts with query_file_uri
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryList")]
    #[builder(default)]
    pub query_list: Option<Vec<String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobForProviderPrestoConfigLoggingConfig {
    /// The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "driverLogLevels")]
    #[builder(default)]
    pub driver_log_levels: Option<HashMap<String, String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobForProviderPysparkConfig {
    /// HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "archiveUris")]
    #[builder(default)]
    pub archive_uris: Option<Vec<String>>,
    /// The arguments to pass to the driver.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub args: Option<Vec<String>>,
    /// HCFS URIs of files to be copied to the working directory of Python drivers and distributed tasks. Useful for naively parallel tasks.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fileUris")]
    #[builder(default)]
    pub file_uris: Option<Vec<String>>,
    /// HCFS URIs of jar files to add to the CLASSPATHs of the Python driver and tasks.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jarFileUris")]
    #[builder(default)]
    pub jar_file_uris: Option<Vec<String>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "loggingConfig")]
    #[builder(default)]
    pub logging_config: Option<JobForProviderPysparkConfigLoggingConfig>,
    /// The HCFS URI of the main Python file to use as the driver. Must be a .py file.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mainPythonFileUri")]
    #[builder(default)]
    pub main_python_file_uri: Option<String>,
    /// A mapping of property names to values, used to configure PySpark. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub properties: Option<HashMap<String, String>>,
    /// HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "pythonFileUris")]
    #[builder(default)]
    pub python_file_uris: Option<Vec<String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobForProviderPysparkConfigLoggingConfig {
    /// The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "driverLogLevels")]
    #[builder(default)]
    pub driver_log_levels: Option<HashMap<String, String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobForProviderReference {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jobId")]
    #[builder(default)]
    pub job_id: Option<String>,
}

/// Reference to a Cluster in dataproc to populate region.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobForProviderRegionRef {
    /// Name of the referenced object.
    pub name: String,
    /// Policies for referencing.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub policy: Option<JobForProviderRegionRefPolicy>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobForProviderRegionRefPolicy {
    /// Resolution specifies whether resolution of this reference is required.
    /// The default is 'Required', which means the reconcile will fail if the
    /// reference cannot be resolved. 'Optional' means this reference will be
    /// a no-op if it cannot be resolved.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub resolution: Option<JobForProviderRegionRefPolicyResolution>,
    /// Resolve specifies when this reference should be resolved. The default
    /// is 'IfNotPresent', which will attempt to resolve the reference only when
    /// the corresponding field is not present. Use 'Always' to resolve the
    /// reference on every reconcile.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub resolve: Option<JobForProviderRegionRefPolicyResolve>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum JobForProviderRegionRefPolicyResolution {
    Required,
    Optional,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum JobForProviderRegionRefPolicyResolve {
    Always,
    IfNotPresent,
}

/// Selector for a Cluster in dataproc to populate region.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobForProviderRegionSelector {
    /// MatchControllerRef ensures an object with the same controller reference
    /// as the selecting object is selected.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchControllerRef")]
    #[builder(default)]
    pub match_controller_ref: Option<bool>,
    /// MatchLabels ensures an object with matching labels is selected.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    #[builder(default)]
    pub match_labels: Option<HashMap<String, String>>,
    /// Policies for selection.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub policy: Option<JobForProviderRegionSelectorPolicy>,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobForProviderRegionSelectorPolicy {
    /// Resolution specifies whether resolution of this reference is required.
    /// The default is 'Required', which means the reconcile will fail if the
    /// reference cannot be resolved. 'Optional' means this reference will be
    /// a no-op if it cannot be resolved.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub resolution: Option<JobForProviderRegionSelectorPolicyResolution>,
    /// Resolve specifies when this reference should be resolved. The default
    /// is 'IfNotPresent', which will attempt to resolve the reference only when
    /// the corresponding field is not present. Use 'Always' to resolve the
    /// reference on every reconcile.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub resolve: Option<JobForProviderRegionSelectorPolicyResolve>,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum JobForProviderRegionSelectorPolicyResolution {
    Required,
    Optional,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum JobForProviderRegionSelectorPolicyResolve {
    Always,
    IfNotPresent,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobForProviderScheduling {
    /// Maximum number of times per hour a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxFailuresPerHour")]
    #[builder(default)]
    pub max_failures_per_hour: Option<f64>,
    /// Maximum number of times in total a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxFailuresTotal")]
    #[builder(default)]
    pub max_failures_total: Option<f64>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobForProviderSparkConfig {
    /// HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "archiveUris")]
    #[builder(default)]
    pub archive_uris: Option<Vec<String>>,
    /// The arguments to pass to the driver.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub args: Option<Vec<String>>,
    /// HCFS URIs of files to be copied to the working directory of Spark drivers and distributed tasks. Useful for naively parallel tasks.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fileUris")]
    #[builder(default)]
    pub file_uris: Option<Vec<String>>,
    /// HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jarFileUris")]
    #[builder(default)]
    pub jar_file_uris: Option<Vec<String>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "loggingConfig")]
    #[builder(default)]
    pub logging_config: Option<JobForProviderSparkConfigLoggingConfig>,
    /// The class containing the main method of the driver. Must be in a
    /// provided jar or jar that is already on the classpath. Conflicts with main_jar_file_uri
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mainClass")]
    #[builder(default)]
    pub main_class: Option<String>,
    /// The HCFS URI of jar file containing
    /// the driver jar. Conflicts with main_class
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mainJarFileUri")]
    #[builder(default)]
    pub main_jar_file_uri: Option<String>,
    /// A mapping of property names to values, used to configure Spark. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub properties: Option<HashMap<String, String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobForProviderSparkConfigLoggingConfig {
    /// The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "driverLogLevels")]
    #[builder(default)]
    pub driver_log_levels: Option<HashMap<String, String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobForProviderSparksqlConfig {
    /// HCFS URIs of jar files to be added to the Spark CLASSPATH.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jarFileUris")]
    #[builder(default)]
    pub jar_file_uris: Option<Vec<String>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "loggingConfig")]
    #[builder(default)]
    pub logging_config: Option<JobForProviderSparksqlConfigLoggingConfig>,
    /// A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Cloud Dataproc API may be overwritten.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub properties: Option<HashMap<String, String>>,
    /// The HCFS URI of the script that contains SQL queries.
    /// Conflicts with query_list
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryFileUri")]
    #[builder(default)]
    pub query_file_uri: Option<String>,
    /// The list of SQL queries or statements to execute as part of the job.
    /// Conflicts with query_file_uri
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryList")]
    #[builder(default)]
    pub query_list: Option<Vec<String>>,
    /// Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "scriptVariables")]
    #[builder(default)]
    pub script_variables: Option<HashMap<String, String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobForProviderSparksqlConfigLoggingConfig {
    /// The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "driverLogLevels")]
    #[builder(default)]
    pub driver_log_levels: Option<HashMap<String, String>>,
}

/// THIS IS A BETA FIELD. It will be honored
/// unless the Management Policies feature flag is disabled.
/// InitProvider holds the same fields as ForProvider, with the exception
/// of Identifier and other resource reference fields. The fields that are
/// in InitProvider are merged into ForProvider when the resource is created.
/// The same fields are also added to the terraform ignore_changes hook, to
/// avoid updating them after creation. This is useful for fields that are
/// required on creation, but we do not desire to update them after creation,
/// for example because of an external controller is managing them, like an
/// autoscaler.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobInitProvider {
    /// By default, you can only delete inactive jobs within
    /// Dataproc. Setting this to true, and calling destroy, will ensure that the
    /// job is first cancelled before issuing the delete.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "forceDelete")]
    #[builder(default)]
    pub force_delete: Option<bool>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hadoopConfig")]
    #[builder(default)]
    pub hadoop_config: Option<JobInitProviderHadoopConfig>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hiveConfig")]
    #[builder(default)]
    pub hive_config: Option<JobInitProviderHiveConfig>,
    /// The list of labels (key/value pairs) to add to the job.
    /// Note: This field is non-authoritative, and will only manage the labels present in your configuration.
    /// Please refer to the field 'effective_labels' for all of the labels present on the resource.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub labels: Option<HashMap<String, String>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "pigConfig")]
    #[builder(default)]
    pub pig_config: Option<JobInitProviderPigConfig>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub placement: Option<JobInitProviderPlacement>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "prestoConfig")]
    #[builder(default)]
    pub presto_config: Option<JobInitProviderPrestoConfig>,
    /// The project in which the cluster can be found and jobs
    /// subsequently run against. If it is not provided, the provider project is used.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub project: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "pysparkConfig")]
    #[builder(default)]
    pub pyspark_config: Option<JobInitProviderPysparkConfig>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub reference: Option<JobInitProviderReference>,
    /// The Cloud Dataproc region. This essentially determines which clusters are available
    /// for this job to be submitted to. If not specified, defaults to global.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub region: Option<String>,
    /// Reference to a Cluster in dataproc to populate region.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "regionRef")]
    #[builder(default)]
    pub region_ref: Option<JobInitProviderRegionRef>,
    /// Selector for a Cluster in dataproc to populate region.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "regionSelector")]
    #[builder(default)]
    pub region_selector: Option<JobInitProviderRegionSelector>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub scheduling: Option<JobInitProviderScheduling>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sparkConfig")]
    #[builder(default)]
    pub spark_config: Option<JobInitProviderSparkConfig>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sparksqlConfig")]
    #[builder(default)]
    pub sparksql_config: Option<JobInitProviderSparksqlConfig>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobInitProviderHadoopConfig {
    /// HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "archiveUris")]
    #[builder(default)]
    pub archive_uris: Option<Vec<String>>,
    /// The arguments to pass to the driver. Do not include arguments, such as -libjars or -Dfoo=bar, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub args: Option<Vec<String>>,
    /// HCFS URIs of files to be copied to the working directory of Hadoop drivers and distributed tasks. Useful for naively parallel tasks.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fileUris")]
    #[builder(default)]
    pub file_uris: Option<Vec<String>>,
    /// HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jarFileUris")]
    #[builder(default)]
    pub jar_file_uris: Option<Vec<String>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "loggingConfig")]
    #[builder(default)]
    pub logging_config: Option<JobInitProviderHadoopConfigLoggingConfig>,
    /// The name of the driver's main class. The jar file containing the class must be in the default CLASSPATH or specified in jar_file_uris. Conflicts with main_jar_file_uri
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mainClass")]
    #[builder(default)]
    pub main_class: Option<String>,
    /// The HCFS URI of the jar file containing the main class. Examples: 'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar' 'hdfs:/tmp/test-samples/custom-wordcount.jar' 'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'. Conflicts with main_class
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mainJarFileUri")]
    #[builder(default)]
    pub main_jar_file_uri: Option<String>,
    /// A mapping of property names to values, used to configure Hadoop. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site and classes in user code..
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub properties: Option<HashMap<String, String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobInitProviderHadoopConfigLoggingConfig {
    /// The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "driverLogLevels")]
    #[builder(default)]
    pub driver_log_levels: Option<HashMap<String, String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobInitProviderHiveConfig {
    /// Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries. Defaults to false.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "continueOnFailure")]
    #[builder(default)]
    pub continue_on_failure: Option<bool>,
    /// HCFS URIs of jar files to add to the CLASSPATH of the Hive server and Hadoop MapReduce (MR) tasks. Can contain Hive SerDes and UDFs.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jarFileUris")]
    #[builder(default)]
    pub jar_file_uris: Option<Vec<String>>,
    /// A mapping of property names and values, used to configure Hive. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/hive/conf/hive-site.xml, and classes in user code..
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub properties: Option<HashMap<String, String>>,
    /// HCFS URI of file containing Hive script to execute as the job.
    /// Conflicts with query_list
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryFileUri")]
    #[builder(default)]
    pub query_file_uri: Option<String>,
    /// The list of Hive queries or statements to execute as part of the job.
    /// Conflicts with query_file_uri
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryList")]
    #[builder(default)]
    pub query_list: Option<Vec<String>>,
    /// Mapping of query variable names to values (equivalent to the Hive command: SET name="value";).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "scriptVariables")]
    #[builder(default)]
    pub script_variables: Option<HashMap<String, String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobInitProviderPigConfig {
    /// Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries. Defaults to false.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "continueOnFailure")]
    #[builder(default)]
    pub continue_on_failure: Option<bool>,
    /// HCFS URIs of jar files to add to the CLASSPATH of the Pig Client and Hadoop MapReduce (MR) tasks. Can contain Pig UDFs.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jarFileUris")]
    #[builder(default)]
    pub jar_file_uris: Option<Vec<String>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "loggingConfig")]
    #[builder(default)]
    pub logging_config: Option<JobInitProviderPigConfigLoggingConfig>,
    /// A mapping of property names to values, used to configure Pig. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/pig/conf/pig.properties, and classes in user code.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub properties: Option<HashMap<String, String>>,
    /// HCFS URI of file containing Hive script to execute as the job.
    /// Conflicts with query_list
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryFileUri")]
    #[builder(default)]
    pub query_file_uri: Option<String>,
    /// The list of Hive queries or statements to execute as part of the job.
    /// Conflicts with query_file_uri
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryList")]
    #[builder(default)]
    pub query_list: Option<Vec<String>>,
    /// Mapping of query variable names to values (equivalent to the Pig command: name=[value]).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "scriptVariables")]
    #[builder(default)]
    pub script_variables: Option<HashMap<String, String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobInitProviderPigConfigLoggingConfig {
    /// The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "driverLogLevels")]
    #[builder(default)]
    pub driver_log_levels: Option<HashMap<String, String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobInitProviderPlacement {
    /// The name of the cluster where the job
    /// will be submitted.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clusterName")]
    #[builder(default)]
    pub cluster_name: Option<String>,
    /// Reference to a Cluster in dataproc to populate clusterName.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clusterNameRef")]
    #[builder(default)]
    pub cluster_name_ref: Option<JobInitProviderPlacementClusterNameRef>,
    /// Selector for a Cluster in dataproc to populate clusterName.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clusterNameSelector")]
    #[builder(default)]
    pub cluster_name_selector: Option<JobInitProviderPlacementClusterNameSelector>,
}

/// Reference to a Cluster in dataproc to populate clusterName.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobInitProviderPlacementClusterNameRef {
    /// Name of the referenced object.
    pub name: String,
    /// Policies for referencing.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub policy: Option<JobInitProviderPlacementClusterNameRefPolicy>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobInitProviderPlacementClusterNameRefPolicy {
    /// Resolution specifies whether resolution of this reference is required.
    /// The default is 'Required', which means the reconcile will fail if the
    /// reference cannot be resolved. 'Optional' means this reference will be
    /// a no-op if it cannot be resolved.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub resolution: Option<JobInitProviderPlacementClusterNameRefPolicyResolution>,
    /// Resolve specifies when this reference should be resolved. The default
    /// is 'IfNotPresent', which will attempt to resolve the reference only when
    /// the corresponding field is not present. Use 'Always' to resolve the
    /// reference on every reconcile.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub resolve: Option<JobInitProviderPlacementClusterNameRefPolicyResolve>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum JobInitProviderPlacementClusterNameRefPolicyResolution {
    Required,
    Optional,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum JobInitProviderPlacementClusterNameRefPolicyResolve {
    Always,
    IfNotPresent,
}

/// Selector for a Cluster in dataproc to populate clusterName.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobInitProviderPlacementClusterNameSelector {
    /// MatchControllerRef ensures an object with the same controller reference
    /// as the selecting object is selected.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchControllerRef")]
    #[builder(default)]
    pub match_controller_ref: Option<bool>,
    /// MatchLabels ensures an object with matching labels is selected.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    #[builder(default)]
    pub match_labels: Option<HashMap<String, String>>,
    /// Policies for selection.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub policy: Option<JobInitProviderPlacementClusterNameSelectorPolicy>,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobInitProviderPlacementClusterNameSelectorPolicy {
    /// Resolution specifies whether resolution of this reference is required.
    /// The default is 'Required', which means the reconcile will fail if the
    /// reference cannot be resolved. 'Optional' means this reference will be
    /// a no-op if it cannot be resolved.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub resolution: Option<JobInitProviderPlacementClusterNameSelectorPolicyResolution>,
    /// Resolve specifies when this reference should be resolved. The default
    /// is 'IfNotPresent', which will attempt to resolve the reference only when
    /// the corresponding field is not present. Use 'Always' to resolve the
    /// reference on every reconcile.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub resolve: Option<JobInitProviderPlacementClusterNameSelectorPolicyResolve>,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum JobInitProviderPlacementClusterNameSelectorPolicyResolution {
    Required,
    Optional,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum JobInitProviderPlacementClusterNameSelectorPolicyResolve {
    Always,
    IfNotPresent,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobInitProviderPrestoConfig {
    /// Presto client tags to attach to this query.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clientTags")]
    #[builder(default)]
    pub client_tags: Option<Vec<String>>,
    /// Whether to continue executing queries if a query fails. Setting to true can be useful when executing independent parallel queries. Defaults to false.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "continueOnFailure")]
    #[builder(default)]
    pub continue_on_failure: Option<bool>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "loggingConfig")]
    #[builder(default)]
    pub logging_config: Option<JobInitProviderPrestoConfigLoggingConfig>,
    /// The format in which query output will be displayed. See the Presto documentation for supported output formats.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "outputFormat")]
    #[builder(default)]
    pub output_format: Option<String>,
    /// A mapping of property names to values. Used to set Presto session properties Equivalent to using the --session flag in the Presto CLI.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub properties: Option<HashMap<String, String>>,
    /// The HCFS URI of the script that contains SQL queries.
    /// Conflicts with query_list
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryFileUri")]
    #[builder(default)]
    pub query_file_uri: Option<String>,
    /// The list of SQL queries or statements to execute as part of the job.
    /// Conflicts with query_file_uri
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryList")]
    #[builder(default)]
    pub query_list: Option<Vec<String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobInitProviderPrestoConfigLoggingConfig {
    /// The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "driverLogLevels")]
    #[builder(default)]
    pub driver_log_levels: Option<HashMap<String, String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobInitProviderPysparkConfig {
    /// HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "archiveUris")]
    #[builder(default)]
    pub archive_uris: Option<Vec<String>>,
    /// The arguments to pass to the driver.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub args: Option<Vec<String>>,
    /// HCFS URIs of files to be copied to the working directory of Python drivers and distributed tasks. Useful for naively parallel tasks.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fileUris")]
    #[builder(default)]
    pub file_uris: Option<Vec<String>>,
    /// HCFS URIs of jar files to add to the CLASSPATHs of the Python driver and tasks.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jarFileUris")]
    #[builder(default)]
    pub jar_file_uris: Option<Vec<String>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "loggingConfig")]
    #[builder(default)]
    pub logging_config: Option<JobInitProviderPysparkConfigLoggingConfig>,
    /// The HCFS URI of the main Python file to use as the driver. Must be a .py file.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mainPythonFileUri")]
    #[builder(default)]
    pub main_python_file_uri: Option<String>,
    /// A mapping of property names to values, used to configure PySpark. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub properties: Option<HashMap<String, String>>,
    /// HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "pythonFileUris")]
    #[builder(default)]
    pub python_file_uris: Option<Vec<String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobInitProviderPysparkConfigLoggingConfig {
    /// The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "driverLogLevels")]
    #[builder(default)]
    pub driver_log_levels: Option<HashMap<String, String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobInitProviderReference {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jobId")]
    #[builder(default)]
    pub job_id: Option<String>,
}

/// Reference to a Cluster in dataproc to populate region.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobInitProviderRegionRef {
    /// Name of the referenced object.
    pub name: String,
    /// Policies for referencing.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub policy: Option<JobInitProviderRegionRefPolicy>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobInitProviderRegionRefPolicy {
    /// Resolution specifies whether resolution of this reference is required.
    /// The default is 'Required', which means the reconcile will fail if the
    /// reference cannot be resolved. 'Optional' means this reference will be
    /// a no-op if it cannot be resolved.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub resolution: Option<JobInitProviderRegionRefPolicyResolution>,
    /// Resolve specifies when this reference should be resolved. The default
    /// is 'IfNotPresent', which will attempt to resolve the reference only when
    /// the corresponding field is not present. Use 'Always' to resolve the
    /// reference on every reconcile.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub resolve: Option<JobInitProviderRegionRefPolicyResolve>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum JobInitProviderRegionRefPolicyResolution {
    Required,
    Optional,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum JobInitProviderRegionRefPolicyResolve {
    Always,
    IfNotPresent,
}

/// Selector for a Cluster in dataproc to populate region.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobInitProviderRegionSelector {
    /// MatchControllerRef ensures an object with the same controller reference
    /// as the selecting object is selected.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchControllerRef")]
    #[builder(default)]
    pub match_controller_ref: Option<bool>,
    /// MatchLabels ensures an object with matching labels is selected.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "matchLabels")]
    #[builder(default)]
    pub match_labels: Option<HashMap<String, String>>,
    /// Policies for selection.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub policy: Option<JobInitProviderRegionSelectorPolicy>,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobInitProviderRegionSelectorPolicy {
    /// Resolution specifies whether resolution of this reference is required.
    /// The default is 'Required', which means the reconcile will fail if the
    /// reference cannot be resolved. 'Optional' means this reference will be
    /// a no-op if it cannot be resolved.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub resolution: Option<JobInitProviderRegionSelectorPolicyResolution>,
    /// Resolve specifies when this reference should be resolved. The default
    /// is 'IfNotPresent', which will attempt to resolve the reference only when
    /// the corresponding field is not present. Use 'Always' to resolve the
    /// reference on every reconcile.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub resolve: Option<JobInitProviderRegionSelectorPolicyResolve>,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum JobInitProviderRegionSelectorPolicyResolution {
    Required,
    Optional,
}

/// Policies for selection.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum JobInitProviderRegionSelectorPolicyResolve {
    Always,
    IfNotPresent,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobInitProviderScheduling {
    /// Maximum number of times per hour a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxFailuresPerHour")]
    #[builder(default)]
    pub max_failures_per_hour: Option<f64>,
    /// Maximum number of times in total a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxFailuresTotal")]
    #[builder(default)]
    pub max_failures_total: Option<f64>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobInitProviderSparkConfig {
    /// HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "archiveUris")]
    #[builder(default)]
    pub archive_uris: Option<Vec<String>>,
    /// The arguments to pass to the driver.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub args: Option<Vec<String>>,
    /// HCFS URIs of files to be copied to the working directory of Spark drivers and distributed tasks. Useful for naively parallel tasks.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fileUris")]
    #[builder(default)]
    pub file_uris: Option<Vec<String>>,
    /// HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jarFileUris")]
    #[builder(default)]
    pub jar_file_uris: Option<Vec<String>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "loggingConfig")]
    #[builder(default)]
    pub logging_config: Option<JobInitProviderSparkConfigLoggingConfig>,
    /// The class containing the main method of the driver. Must be in a
    /// provided jar or jar that is already on the classpath. Conflicts with main_jar_file_uri
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mainClass")]
    #[builder(default)]
    pub main_class: Option<String>,
    /// The HCFS URI of jar file containing
    /// the driver jar. Conflicts with main_class
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mainJarFileUri")]
    #[builder(default)]
    pub main_jar_file_uri: Option<String>,
    /// A mapping of property names to values, used to configure Spark. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub properties: Option<HashMap<String, String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobInitProviderSparkConfigLoggingConfig {
    /// The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "driverLogLevels")]
    #[builder(default)]
    pub driver_log_levels: Option<HashMap<String, String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobInitProviderSparksqlConfig {
    /// HCFS URIs of jar files to be added to the Spark CLASSPATH.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jarFileUris")]
    #[builder(default)]
    pub jar_file_uris: Option<Vec<String>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "loggingConfig")]
    #[builder(default)]
    pub logging_config: Option<JobInitProviderSparksqlConfigLoggingConfig>,
    /// A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Cloud Dataproc API may be overwritten.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub properties: Option<HashMap<String, String>>,
    /// The HCFS URI of the script that contains SQL queries.
    /// Conflicts with query_list
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryFileUri")]
    #[builder(default)]
    pub query_file_uri: Option<String>,
    /// The list of SQL queries or statements to execute as part of the job.
    /// Conflicts with query_file_uri
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryList")]
    #[builder(default)]
    pub query_list: Option<Vec<String>>,
    /// Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "scriptVariables")]
    #[builder(default)]
    pub script_variables: Option<HashMap<String, String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobInitProviderSparksqlConfigLoggingConfig {
    /// The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "driverLogLevels")]
    #[builder(default)]
    pub driver_log_levels: Option<HashMap<String, String>>,
}

/// ProviderConfigReference specifies how the provider that will be used to
/// create, observe, update, and delete this managed resource should be
/// configured.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobProviderConfigRef {
    /// Name of the referenced object.
    pub name: String,
    /// Policies for referencing.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub policy: Option<JobProviderConfigRefPolicy>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobProviderConfigRefPolicy {
    /// Resolution specifies whether resolution of this reference is required.
    /// The default is 'Required', which means the reconcile will fail if the
    /// reference cannot be resolved. 'Optional' means this reference will be
    /// a no-op if it cannot be resolved.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub resolution: Option<JobProviderConfigRefPolicyResolution>,
    /// Resolve specifies when this reference should be resolved. The default
    /// is 'IfNotPresent', which will attempt to resolve the reference only when
    /// the corresponding field is not present. Use 'Always' to resolve the
    /// reference on every reconcile.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub resolve: Option<JobProviderConfigRefPolicyResolve>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum JobProviderConfigRefPolicyResolution {
    Required,
    Optional,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum JobProviderConfigRefPolicyResolve {
    Always,
    IfNotPresent,
}

/// PublishConnectionDetailsTo specifies the connection secret config which
/// contains a name, metadata and a reference to secret store config to
/// which any connection details for this managed resource should be written.
/// Connection details frequently include the endpoint, username,
/// and password required to connect to the managed resource.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobPublishConnectionDetailsTo {
    /// SecretStoreConfigRef specifies which secret store config should be used
    /// for this ConnectionSecret.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "configRef")]
    #[builder(default)]
    pub config_ref: Option<JobPublishConnectionDetailsToConfigRef>,
    /// Metadata is the metadata for connection secret.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub metadata: Option<JobPublishConnectionDetailsToMetadata>,
    /// Name is the name of the connection secret.
    pub name: String,
}

/// SecretStoreConfigRef specifies which secret store config should be used
/// for this ConnectionSecret.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobPublishConnectionDetailsToConfigRef {
    /// Name of the referenced object.
    pub name: String,
    /// Policies for referencing.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub policy: Option<JobPublishConnectionDetailsToConfigRefPolicy>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobPublishConnectionDetailsToConfigRefPolicy {
    /// Resolution specifies whether resolution of this reference is required.
    /// The default is 'Required', which means the reconcile will fail if the
    /// reference cannot be resolved. 'Optional' means this reference will be
    /// a no-op if it cannot be resolved.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub resolution: Option<JobPublishConnectionDetailsToConfigRefPolicyResolution>,
    /// Resolve specifies when this reference should be resolved. The default
    /// is 'IfNotPresent', which will attempt to resolve the reference only when
    /// the corresponding field is not present. Use 'Always' to resolve the
    /// reference on every reconcile.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub resolve: Option<JobPublishConnectionDetailsToConfigRefPolicyResolve>,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum JobPublishConnectionDetailsToConfigRefPolicyResolution {
    Required,
    Optional,
}

/// Policies for referencing.
#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]
pub enum JobPublishConnectionDetailsToConfigRefPolicyResolve {
    Always,
    IfNotPresent,
}

/// Metadata is the metadata for connection secret.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobPublishConnectionDetailsToMetadata {
    /// Annotations are the annotations to be added to connection secret.
    /// - For Kubernetes secrets, this will be used as "metadata.annotations".
    /// - It is up to Secret Store implementation for others store types.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub annotations: Option<HashMap<String, String>>,
    /// Labels are the labels/tags to be added to connection secret.
    /// - For Kubernetes secrets, this will be used as "metadata.labels".
    /// - It is up to Secret Store implementation for others store types.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub labels: Option<HashMap<String, String>>,
    /// Type is the SecretType for the connection secret.
    /// - Only valid for Kubernetes Secret Stores.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "type")]
    #[builder(default)]
    pub r#type: Option<String>,
}

/// WriteConnectionSecretToReference specifies the namespace and name of a
/// Secret to which any connection details for this managed resource should
/// be written. Connection details frequently include the endpoint, username,
/// and password required to connect to the managed resource.
/// This field is planned to be replaced in a future release in favor of
/// PublishConnectionDetailsTo. Currently, both could be set independently
/// and connection details would be published to both without affecting
/// each other.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobWriteConnectionSecretToRef {
    /// Name of the secret.
    pub name: String,
    /// Namespace of the secret.
    pub namespace: String,
}

/// JobStatus defines the observed state of Job.
#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobStatus {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "atProvider")]
    #[builder(default)]
    pub at_provider: Option<JobStatusAtProvider>,
    /// Conditions of the resource.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub conditions: Option<Vec<Condition>>,
    /// ObservedGeneration is the latest metadata.generation
    /// which resulted in either a ready state, or stalled due to error
    /// it can not recover from without human intervention.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "observedGeneration")]
    #[builder(default)]
    pub observed_generation: Option<i64>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobStatusAtProvider {
    /// If present, the location of miscellaneous control files which may be used as part of job setup and handling. If not present, control files may be placed in the same location as driver_output_uri.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "driverControlsFilesUri")]
    #[builder(default)]
    pub driver_controls_files_uri: Option<String>,
    /// A URI pointing to the location of the stdout of the job's driver program.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "driverOutputResourceUri")]
    #[builder(default)]
    pub driver_output_resource_uri: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "effectiveLabels")]
    #[builder(default)]
    pub effective_labels: Option<HashMap<String, String>>,
    /// By default, you can only delete inactive jobs within
    /// Dataproc. Setting this to true, and calling destroy, will ensure that the
    /// job is first cancelled before issuing the delete.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "forceDelete")]
    #[builder(default)]
    pub force_delete: Option<bool>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hadoopConfig")]
    #[builder(default)]
    pub hadoop_config: Option<JobStatusAtProviderHadoopConfig>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "hiveConfig")]
    #[builder(default)]
    pub hive_config: Option<JobStatusAtProviderHiveConfig>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub id: Option<String>,
    /// The list of labels (key/value pairs) to add to the job.
    /// Note: This field is non-authoritative, and will only manage the labels present in your configuration.
    /// Please refer to the field 'effective_labels' for all of the labels present on the resource.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub labels: Option<HashMap<String, String>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "pigConfig")]
    #[builder(default)]
    pub pig_config: Option<JobStatusAtProviderPigConfig>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub placement: Option<JobStatusAtProviderPlacement>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "prestoConfig")]
    #[builder(default)]
    pub presto_config: Option<JobStatusAtProviderPrestoConfig>,
    /// The project in which the cluster can be found and jobs
    /// subsequently run against. If it is not provided, the provider project is used.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub project: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "pysparkConfig")]
    #[builder(default)]
    pub pyspark_config: Option<JobStatusAtProviderPysparkConfig>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub reference: Option<JobStatusAtProviderReference>,
    /// The Cloud Dataproc region. This essentially determines which clusters are available
    /// for this job to be submitted to. If not specified, defaults to global.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub region: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub scheduling: Option<JobStatusAtProviderScheduling>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sparkConfig")]
    #[builder(default)]
    pub spark_config: Option<JobStatusAtProviderSparkConfig>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "sparksqlConfig")]
    #[builder(default)]
    pub sparksql_config: Option<JobStatusAtProviderSparksqlConfig>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub status: Option<Vec<JobStatusAtProviderStatus>>,
    /// The combination of labels configured directly on the resource and default labels configured on the provider.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "terraformLabels")]
    #[builder(default)]
    pub terraform_labels: Option<HashMap<String, String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobStatusAtProviderHadoopConfig {
    /// HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "archiveUris")]
    #[builder(default)]
    pub archive_uris: Option<Vec<String>>,
    /// The arguments to pass to the driver. Do not include arguments, such as -libjars or -Dfoo=bar, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub args: Option<Vec<String>>,
    /// HCFS URIs of files to be copied to the working directory of Hadoop drivers and distributed tasks. Useful for naively parallel tasks.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fileUris")]
    #[builder(default)]
    pub file_uris: Option<Vec<String>>,
    /// HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jarFileUris")]
    #[builder(default)]
    pub jar_file_uris: Option<Vec<String>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "loggingConfig")]
    #[builder(default)]
    pub logging_config: Option<JobStatusAtProviderHadoopConfigLoggingConfig>,
    /// The name of the driver's main class. The jar file containing the class must be in the default CLASSPATH or specified in jar_file_uris. Conflicts with main_jar_file_uri
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mainClass")]
    #[builder(default)]
    pub main_class: Option<String>,
    /// The HCFS URI of the jar file containing the main class. Examples: 'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar' 'hdfs:/tmp/test-samples/custom-wordcount.jar' 'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'. Conflicts with main_class
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mainJarFileUri")]
    #[builder(default)]
    pub main_jar_file_uri: Option<String>,
    /// A mapping of property names to values, used to configure Hadoop. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site and classes in user code..
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub properties: Option<HashMap<String, String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobStatusAtProviderHadoopConfigLoggingConfig {
    /// The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "driverLogLevels")]
    #[builder(default)]
    pub driver_log_levels: Option<HashMap<String, String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobStatusAtProviderHiveConfig {
    /// Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries. Defaults to false.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "continueOnFailure")]
    #[builder(default)]
    pub continue_on_failure: Option<bool>,
    /// HCFS URIs of jar files to add to the CLASSPATH of the Hive server and Hadoop MapReduce (MR) tasks. Can contain Hive SerDes and UDFs.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jarFileUris")]
    #[builder(default)]
    pub jar_file_uris: Option<Vec<String>>,
    /// A mapping of property names and values, used to configure Hive. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/hive/conf/hive-site.xml, and classes in user code..
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub properties: Option<HashMap<String, String>>,
    /// HCFS URI of file containing Hive script to execute as the job.
    /// Conflicts with query_list
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryFileUri")]
    #[builder(default)]
    pub query_file_uri: Option<String>,
    /// The list of Hive queries or statements to execute as part of the job.
    /// Conflicts with query_file_uri
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryList")]
    #[builder(default)]
    pub query_list: Option<Vec<String>>,
    /// Mapping of query variable names to values (equivalent to the Hive command: SET name="value";).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "scriptVariables")]
    #[builder(default)]
    pub script_variables: Option<HashMap<String, String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobStatusAtProviderPigConfig {
    /// Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries. Defaults to false.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "continueOnFailure")]
    #[builder(default)]
    pub continue_on_failure: Option<bool>,
    /// HCFS URIs of jar files to add to the CLASSPATH of the Pig Client and Hadoop MapReduce (MR) tasks. Can contain Pig UDFs.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jarFileUris")]
    #[builder(default)]
    pub jar_file_uris: Option<Vec<String>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "loggingConfig")]
    #[builder(default)]
    pub logging_config: Option<JobStatusAtProviderPigConfigLoggingConfig>,
    /// A mapping of property names to values, used to configure Pig. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/pig/conf/pig.properties, and classes in user code.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub properties: Option<HashMap<String, String>>,
    /// HCFS URI of file containing Hive script to execute as the job.
    /// Conflicts with query_list
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryFileUri")]
    #[builder(default)]
    pub query_file_uri: Option<String>,
    /// The list of Hive queries or statements to execute as part of the job.
    /// Conflicts with query_file_uri
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryList")]
    #[builder(default)]
    pub query_list: Option<Vec<String>>,
    /// Mapping of query variable names to values (equivalent to the Pig command: name=[value]).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "scriptVariables")]
    #[builder(default)]
    pub script_variables: Option<HashMap<String, String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobStatusAtProviderPigConfigLoggingConfig {
    /// The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "driverLogLevels")]
    #[builder(default)]
    pub driver_log_levels: Option<HashMap<String, String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobStatusAtProviderPlacement {
    /// The name of the cluster where the job
    /// will be submitted.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clusterName")]
    #[builder(default)]
    pub cluster_name: Option<String>,
    /// A cluster UUID generated by the Cloud Dataproc service when the job is submitted.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clusterUuid")]
    #[builder(default)]
    pub cluster_uuid: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobStatusAtProviderPrestoConfig {
    /// Presto client tags to attach to this query.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "clientTags")]
    #[builder(default)]
    pub client_tags: Option<Vec<String>>,
    /// Whether to continue executing queries if a query fails. Setting to true can be useful when executing independent parallel queries. Defaults to false.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "continueOnFailure")]
    #[builder(default)]
    pub continue_on_failure: Option<bool>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "loggingConfig")]
    #[builder(default)]
    pub logging_config: Option<JobStatusAtProviderPrestoConfigLoggingConfig>,
    /// The format in which query output will be displayed. See the Presto documentation for supported output formats.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "outputFormat")]
    #[builder(default)]
    pub output_format: Option<String>,
    /// A mapping of property names to values. Used to set Presto session properties Equivalent to using the --session flag in the Presto CLI.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub properties: Option<HashMap<String, String>>,
    /// The HCFS URI of the script that contains SQL queries.
    /// Conflicts with query_list
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryFileUri")]
    #[builder(default)]
    pub query_file_uri: Option<String>,
    /// The list of SQL queries or statements to execute as part of the job.
    /// Conflicts with query_file_uri
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryList")]
    #[builder(default)]
    pub query_list: Option<Vec<String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobStatusAtProviderPrestoConfigLoggingConfig {
    /// The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "driverLogLevels")]
    #[builder(default)]
    pub driver_log_levels: Option<HashMap<String, String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobStatusAtProviderPysparkConfig {
    /// HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "archiveUris")]
    #[builder(default)]
    pub archive_uris: Option<Vec<String>>,
    /// The arguments to pass to the driver.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub args: Option<Vec<String>>,
    /// HCFS URIs of files to be copied to the working directory of Python drivers and distributed tasks. Useful for naively parallel tasks.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fileUris")]
    #[builder(default)]
    pub file_uris: Option<Vec<String>>,
    /// HCFS URIs of jar files to add to the CLASSPATHs of the Python driver and tasks.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jarFileUris")]
    #[builder(default)]
    pub jar_file_uris: Option<Vec<String>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "loggingConfig")]
    #[builder(default)]
    pub logging_config: Option<JobStatusAtProviderPysparkConfigLoggingConfig>,
    /// The HCFS URI of the main Python file to use as the driver. Must be a .py file.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mainPythonFileUri")]
    #[builder(default)]
    pub main_python_file_uri: Option<String>,
    /// A mapping of property names to values, used to configure PySpark. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub properties: Option<HashMap<String, String>>,
    /// HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "pythonFileUris")]
    #[builder(default)]
    pub python_file_uris: Option<Vec<String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobStatusAtProviderPysparkConfigLoggingConfig {
    /// The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "driverLogLevels")]
    #[builder(default)]
    pub driver_log_levels: Option<HashMap<String, String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobStatusAtProviderReference {
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jobId")]
    #[builder(default)]
    pub job_id: Option<String>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobStatusAtProviderScheduling {
    /// Maximum number of times per hour a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxFailuresPerHour")]
    #[builder(default)]
    pub max_failures_per_hour: Option<f64>,
    /// Maximum number of times in total a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "maxFailuresTotal")]
    #[builder(default)]
    pub max_failures_total: Option<f64>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobStatusAtProviderSparkConfig {
    /// HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "archiveUris")]
    #[builder(default)]
    pub archive_uris: Option<Vec<String>>,
    /// The arguments to pass to the driver.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub args: Option<Vec<String>>,
    /// HCFS URIs of files to be copied to the working directory of Spark drivers and distributed tasks. Useful for naively parallel tasks.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "fileUris")]
    #[builder(default)]
    pub file_uris: Option<Vec<String>>,
    /// HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jarFileUris")]
    #[builder(default)]
    pub jar_file_uris: Option<Vec<String>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "loggingConfig")]
    #[builder(default)]
    pub logging_config: Option<JobStatusAtProviderSparkConfigLoggingConfig>,
    /// The class containing the main method of the driver. Must be in a
    /// provided jar or jar that is already on the classpath. Conflicts with main_jar_file_uri
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mainClass")]
    #[builder(default)]
    pub main_class: Option<String>,
    /// The HCFS URI of jar file containing
    /// the driver jar. Conflicts with main_class
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "mainJarFileUri")]
    #[builder(default)]
    pub main_jar_file_uri: Option<String>,
    /// A mapping of property names to values, used to configure Spark. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub properties: Option<HashMap<String, String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobStatusAtProviderSparkConfigLoggingConfig {
    /// The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "driverLogLevels")]
    #[builder(default)]
    pub driver_log_levels: Option<HashMap<String, String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobStatusAtProviderSparksqlConfig {
    /// HCFS URIs of jar files to be added to the Spark CLASSPATH.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "jarFileUris")]
    #[builder(default)]
    pub jar_file_uris: Option<Vec<String>>,
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "loggingConfig")]
    #[builder(default)]
    pub logging_config: Option<JobStatusAtProviderSparksqlConfigLoggingConfig>,
    /// A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Cloud Dataproc API may be overwritten.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub properties: Option<HashMap<String, String>>,
    /// The HCFS URI of the script that contains SQL queries.
    /// Conflicts with query_list
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryFileUri")]
    #[builder(default)]
    pub query_file_uri: Option<String>,
    /// The list of SQL queries or statements to execute as part of the job.
    /// Conflicts with query_file_uri
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "queryList")]
    #[builder(default)]
    pub query_list: Option<Vec<String>>,
    /// Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "scriptVariables")]
    #[builder(default)]
    pub script_variables: Option<HashMap<String, String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobStatusAtProviderSparksqlConfigLoggingConfig {
    /// The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "driverLogLevels")]
    #[builder(default)]
    pub driver_log_levels: Option<HashMap<String, String>>,
}

#[derive(Serialize, Deserialize, Clone, Debug, TypedBuilder, JsonSchema)]
#[builder(field_defaults(setter(strip_option(ignore_invalid))))]
pub struct JobStatusAtProviderStatus {
    /// Optional job state details, such as an error description if the state is ERROR.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub details: Option<String>,
    /// A state message specifying the overall job state.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub state: Option<String>,
    /// The time when this state was entered.
    #[serde(default, skip_serializing_if = "Option::is_none", rename = "stateStartTime")]
    #[builder(default)]
    pub state_start_time: Option<String>,
    /// Additional state information, which includes status reported by the agent.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[builder(default)]
    pub substate: Option<String>,
}

